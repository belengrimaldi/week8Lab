
Date and Time: Wednesday, March 24, 2021 3:56:00 PM EDT
Job Number: 139771502
Documents (28)
1. A COSMIC Approach To Nanoscale Science
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
2. Artificial Intelligence May Help Achieve UN's Sustainable Development Goals
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
3. Study Claims Reparations For Slavery Could Have Reduced COVID-19 Infections And Deaths In US
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
4. Global Ice Loss Increases At Record Rate
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
5. Forecasting Coastal Water Quality
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
6. Climate Change To Alter Position Of Earth's Tropical Rain Belt
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
7. Big Data To Analyze The Mystery Of Beethoven's Metronome
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
8. Understanding How Birds Respond To Extreme Weather Can Inform Conservation Efforts
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
9. The Danger Of Weaponizing Trade For The Environment – Analysis
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
10. Forecasting Urbanization
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
11. Irish And UK Research Helps To Unravel Secrets Behind Game Of Thrones
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
12. Artificial Intelligence Can Predict Students' Educational Outcomes Based On Tweets
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
13. Knowing The Model You Can Trust: The Key To Better Decision-Making
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
14. Spain Using Mobile Phone Data To Study Efficacy Of Lockdown On Spread Of COVID-19
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
15. Climate Signals Detected In Global Weather
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
16. Experts To Create Predictive Tool To Tackle Hate Crime In Los Angeles
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
17. Police Stop Fewer Black Drivers At Night When 'Veil Of Darkness' Obscures Their Race
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
18. Model Beats Wall Street Analysts In Forecasting Business Financials
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
19. March Madness Bracket Analysis Shows Picking Final Four First Leads To Better Brackets
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
20. Influencing Electoral Outcomes: The Ugly Face Of Facebook - Analysis
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
21. How Climate Change Affects Crops In India
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
22. Facebook, Cambridge Analytica And Surveillance Capitalism - OpEd
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
23. Surface Clean-Up Technology Won't Solve Ocean Plastic Problem
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
24. More Evidence Of Causal Link Between Air Pollution And Early Death
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
25. There's No End In Sight To The Zombie Economy – OpEd
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
26. Limits To Strategic Foresight: Try Wisdom Of The Crowds – Analysis
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
27. Baseball Illustrates Economics With Each Game – OpEd
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon
28. More Than A Lifetime Away: World Faces 100-Year Wait For Gender Parity
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: Oregon




A COSMIC Approach To Nanoscale Science
Eurasia Review
March 6, 2021 Saturday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1887 words
Body


COSMIC, a multipurpose X-ray instrument at Lawrence Berkeley National Laboratory's (Berkeley Lab's) Advanced Light Source (ALS), has made headway in the scientific community since its launch less than 2 years ago, with groundbreaking contributions in fields ranging from batteries to biominerals.
COSMIC is the brightest X-ray beamline at the ALS, a synchrotron that generates intense light - from infrared to X-rays - and delivers it to dozens of beamlines to carry out a range of simultaneous science experiments. COSMIC's name is derived from coherent scattering and microscopy, which are two overarching X-ray techniques it is designed to carry out.
Its capabilities include world-leading soft X-ray microscopy resolution below 10 nanometers (billionths of a meter), extreme chemical sensitivity, ultrafast scanning speed as well as the ability to measure nanoscale chemical changes in samples in real time, and to facilitate the exploration of samples with a combination of X-ray and electron microscopy. Soft X-rays represent a low range in X-ray energies, while hard X-rays are higher in energy. Each type can address a different range of experiments.
COSMIC is setting the stage for a long-term project to upgrade the decades-old ALS. The effort, known as the ALS Upgrade (ALS-U), will replace most of the existing accelerator components with state-of-the-art technology, ensuring capabilities that will enable world-leading soft X-ray science for years to come. The upgrade will also further enhance COSMIC's ability to capture nanoscale details in the structure and chemistry of a broad range of samples.
The expected 100-fold increase in X-ray brightness that ALS-U will deliver will provide a similar increase in imaging speed at COSMIC, and a more than threefold improvement in imaging resolution, enabling microscopy with single-nanometer resolution. Further, the technologies being developed now at COSMIC will be deployed at other beamlines at the upgraded ALS, making possible microscopy with higher X-ray energies for many more experiments. The instrument is one of many highly specialized resources available to scientists from around the world for free through a peer-reviewed proposal process.
A journal article,published Dec. 16, 2020, inScience Advances, highlights some of COSMIC's existing capabilities and those that are on the way. The paper offers examples of 8-nanometer resolution achieved in imaging magnetic nanoparticles, the high-resolution chemical mapping of a battery cathode material during heating, and the high-resolution imaging of a frozen-hydrated yeast cell at COSMIC. (A cathode is one type of battery electrode, a component through which current flows.) These results serve as demonstration cases, revealing critical information about the structure and inner workings of these materials and opening the door for further insights across many fields of science.
Another journal article,published Jan. 19, 2021), inProceedings of the National Academy of Sciences, demonstrated the first-ever use of X-ray linear dichroic ptychography, a specialized high-resolution imaging technique available at COSMIC, to map the orientations of a crystal known as aragonite that is present in coral skeletons at 35-nanometer resolution. The technique shows promise for mapping other biomineral samples at high resolution and in 3D, which will provide new insights into their unique attributes and how to mimic and control them. Some biominerals have inspired humanmade materials and nanomaterials due to their strength, resilience, and other desirable properties.
"We use this user-friendly, unique platform for materials characterization to demonstrate world-leading spatial resolution, in conjunction with operando and cryogenic microscopy," said David Shapiro, the paper's lead author and the lead scientist for COSMIC's microscopy experiments. He also leads the ALS Microscopy Program. "Operando" describes the ability to measure changes in samples as they are occurring.
"There's no other instrument that has these capabilities co-located for X-ray microscopy at this resolution," Shapiro said. COSMIC can provide new clues to the nanoscale inner workings of materials, even as they actively function, that will lead to a deeper understanding and better designs - for batteries, catalysts, or biological materials. Equipping COSMIC with such a diversity of capabilities required an equally broad collaboration across scientific disciplines, he noted.
COSMIC contributors included members of Berkeley Lab's CAMERA (Center for Advanced Mathematics for Energy Research Applications) team, which includes computer scientists, software engineers, applied mathematicians, and others; information technology experts; detector specialists; engineers; scientists at the Molecular Foundry's National Center for Electron Microscopy; ALS scientists; and outside collaborators from the National Science Foundation's STROBE Science and Technology Center and Stanford University.
Several advanced technologies developed by different groups were integrated into this one instrument. Key to the demonstrations at COSMIC reported in the paper is the implementation of X-ray ptychography, which is a computer-aided image reconstruction technique that can exceed the resolution of conventional techniques by up to about 10 times.
With traditional methods, spatial resolution -the ability to distinguish tiny features in samples - is limited by the quality of the X-ray optics and their ability to focus the X-ray beam into a tiny spot. But conventional X-ray optics, which are the instruments used to manipulate X-ray light to see samples more clearly, are difficult to make, inefficient, and have short focal lengths.
Instead of relying on imperfect optics, ptychography records a large number of physically overlapping diffraction patterns - which are images produced as X-ray light scatters from the sample - each offering a small piece of the full picture. Rather than being limited by optics quality, the ptychography technique is limited by the brightness of the X-ray source - precisely the parameter that ALS-U is expected to improve a hundredfold. To capture and process the enormous amount of data and reconstruct the final image requires data processing facilities, computer algorithms, and specialized fast pixel detectors like those developed at Berkeley Lab.
"X-ray ptychography is a detector-enabled technique - first deployed with hard (high-energy) X-rays using hybrid pixel detectors, and then at the ALS with the FastCCD we developed," said Peter Denes, the ALS detector program lead who worked with lead engineer John Joseph on the implementation at COSMIC. "Much of the COSMIC technology benefited from the Laboratory Directed Research and Development (LDRD) Program, as did the FastCCD, which translated tools for cosmology into COSMIC observations." Berkeley Lab's LDRD Program supports innovative research activities that keep the Lab at the forefront of science and technology.
Ptychography utilizes a sequence of scattering patterns, produced as X-ray light scatters from a sample. These scattering patterns are analyzed by a computer running high-performance algorithms, which convert them into a high-resolution image.
In the Dec. 16, 2020, paper, researchers highlighted how ptychographic images made it possible to see the high-resolution chemical distribution in microscopic particles of a lithium iron phosphate battery cathode material (Li0.5FePo4). The ptychographic images showed nanoscale chemical features in the interior of the particles that were not visible using the conventional form of the imaging technique, called spectromicroscopy.
In a separate demonstration of ptychography at COSMIC, researchers noted chemical changes in a collection of LixFePO4 nanoparticles when subjected to heating.
Ptychography is also a source of COSMIC's heavy data demands. The beamline can produce several terabytes of data per day, or enough to fill a few laptop computers. The intensive computations required for COSMIC's imaging necessitate a dedicated cluster of GPUs (graphical processing units), which are specialized computer processors.
The ALS Upgrade will further drive its data demands up to an expected 100 terabytes per day, Shapiro noted. Plans are already being discussed for using more resources at Berkeley Lab's National Energy Research Scientific Computing Center (NERSC)to accommodate this pending ramp-up in data.
COSMIC is a stellar example of Berkeley Lab's Superfacility Project, which is designed to link light sources like the ALS and cutting-edge instrumentation including microscopes and telescopes with data and high-performance computing resources in real time, said Bjoern Enders, a data science workflows architect in NERSC's Data Science Engagement Group.
"We love data and computing challenges from instruments like COSMIC that venture beyond facility borders," Enders said. "We are working toward a future where it will be as easy as a button click to use NERSC's resources from a beamline." The addition of the new Perlmutter supercomputer at NERSC, he added, "will be an ideal partner for COSMIC in team science."
COSMIC started up in commissioning mode in March 2017, and opened to general scientific experiments about 2 years ago. Since this time, instrument staff have launched the operando capabilities that measure active chemical processes, for example, and rolled out linear and circular dichroic microscopy and tomography capabilities that further extend COSMIC's range of imaging experiments.
Its coherent scattering branch is now undergoing testing and is not yet available to external users. Work is also in progress to correlate its X-ray microscopy results with electron microscopy results for active processes, and to further develop its cryogenic capabilities, which will allow biological samples and other soft materials to be protected from damage by the ultrabright X-ray beam while they are being imaged. The combination of X-ray and electron microscopy can provide a powerful tool for gathering detailed chemical and structural information on samples, as demonstrated in an experiment involving COSMIC that was highlighted in the journalScience Advances.
Shapiro noted that there are plans to introduce a new experimental station to the beamline, timed with ALS-U, to accommodate more experiments.
One secret to COSMIC's success is that the instrument is designed for compatibility with standard sample-handling components. Shapiro said this user-friendly approach "has been really important for us," and makes it easier for researchers from academia and industry to design COSMIC-compatible experiments. "Users can just show up and plug (the samples) in. It increases our reach, scientifically," he said.
While COSMIC is loaded with features, it isn't bulky, and Shapiro described it as "streamlined in size and cost." He said he hopes it will be a model for future beamlines, both at ALS-U and at other synchrotron facilities.
"I think what is really attractive about it is that it is a very compact instrument. It is high-performance and very stable," he said. "It is very manageable and not very expensive. In that sense it should be very attractive for synchrotrons."
The article A COSMIC Approach To Nanoscale Science appeared first on Eurasia Review.

Load-Date: March 5, 2021


End of Document




Artificial Intelligence May Help Achieve UN's Sustainable Development Goals
Eurasia Review
February 20, 2021 Saturday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 805 words
Body


Scientists from the Andalusian Research Institute in Data Science and Computational Intelligence, or DaSCI (University of Granada), together with the private company Ferrovial and the Spanish Royal Academy of Engineering, highlight the need for unified, accessible, and open data in developing projects to address many of the challenges of the UN's Sustainable Development Goals
Scientists from the Andalusian Research Institute in Data Science and Computational Intelligence, or DaSCI (University of Granada), together with the private company Ferrovial and the Spanish Royal Academy of Engineering (RAI), have conducted a study to analyse how engineering and technological solutions strongly linked to artificial intelligence (AI) can positively contribute to the 17 Sustainable Development Goals (SDGs) set by the United Nations (UN).
To protect the planet and ensure prosperity for all people, the UN established these 17 SDGs as part of its 2030 Agenda, which constitute a paradigm shift for companies and governments in the design of new business models and public policies based on sustainability. Governments, the private sector, and civil society all play an important role in achieving the goals.
The project, entitled "Engineering as a Facilitator of SDGs: Artificial intelligence and disruptive digital technologies", began in March 2020, focusing specifically on the study of AI and digital technologies and how these might be applied to further progress toward the 17 SDGs. The research is organised into three facets that broadly correspond to (i) an introduction to AI and digital technologies, (ii) analysis of their application to the SDGs, and (iii) recommendations for action that can help develop projects and support the achievement of associated goals. As part of this research, the authors reviewed the specialised scientific literature, including over a thousand bibliographical references relevant to the 169 targets that have been set to achieve the SDGs.
This work makes an important contribution to our understanding of the analytical capacity of engineering--under the umbrella of AI and digitalisation in support of the SDGs--and to addressing the challenges faced by the world economy and society in the 21st Century. It also provides insights into the three dimensions that characterise sustainability: the economic (including the different areas of life and economic and technological development); the social (including social development and equality); and the environmental (including resources and the environment).
One of the conclusions of this work is that data constitute the common element on which AI and digital technologies are based. Here, it is important to highlight the need for data that are unified, accessible, and open, as this supports the development of projects designed to address many of the global challenges. Governments and companies must converge toward this objective by generating and sharing data that allow them to successfully take on projects and design solutions to address the SDGs.
The researchers point out that it is imperative to strengthen the links between science and engineering, industry and governments, to reinforce dialogue and expand the different avenues toward achieving high-quality data.
Global targets
"The SDGs set targets to be achieved at the global level, but not all countries and regions of the world are currently in the same position in this race to achieve them. So the application of AI and digital technologies must obviously be adapted to the situation of each country and targeted at the most pressing SDGs," the authors explain.
Digital technologies are advancing at a rapid pace, which means it is important to look for alternative ways to measure the achievement of the SDGs--ways that are adapted to this accelerated pace of progress and the emergence of new digital paradigms. This is especially important given the current world scenario caused by the COVID-19 pandemic, which has undoubtedly had a profound impact on all dimensions of the SDGs, far beyond the strictly health-related aspect.
AI and digital technologies are fundamental tools for travelling the path we have to navigate during this decade, as we carry heavy moral and ethical responsibility toward today's world. Working toward the 17 SDGs is both a great opportunity and a major challenge.
A new book about the study, featuring original illustrations by Pablo García-Moral, is now available, which explores the latest thinking on this issue from those involved in the project. It was written by a team of 16 authors, coordinated by Rosana Montes (UGR), Francisco Herrera (UGR and RAI), Javier Pérez de Vargas (RAI), and Rosario Marchena (Ferrovial).
The article Artificial Intelligence May Help Achieve UN's Sustainable Development Goals appeared first on Eurasia Review.

Load-Date: February 19, 2021


End of Document




Study Claims Reparations For Slavery Could Have Reduced COVID-19 Infections And Deaths In US
Eurasia Review
February 12, 2021 Friday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 2526 words
Body


New study suggests monetary reparations for Black descendants of people enslaved in the United States could have cut SARS-CoV-2 transmission and COVID-19 rates both among Black individuals and the population at large.
Researchers modeled the impact of structural racism on viral transmission and disease impact in the state of Louisiana.
The higher burden of SARS-CoV-2 infection among Black people also amplified the virus's spread in the wider population.
Reparations could have reduced SARS-CoV-2 transmission in the overall population by as much as 68 percent.
Compared with white people, Black individuals in the United States are more likely to be infected with SARS-CoV-2, more likely to end up in the hospital with COVID-19, and more likely to die from the disease.
Civil rights activists have long called for monetary reparations to the Black descendants of Africans enslaved in the United States as a financial, moral, and ethical form of restitution for the injustices of slavery.
Now, a study led by Harvard Medical School researchers suggests reparations could also have surprising public health benefits for Black individuals and the entire nation.
To estimate the impact of structural inequities between Black and white individuals, the researchers set out to capture the effect of reparation payments on the Black-white wealth gap in the state of Louisiana.
Their analysis, published online inSocial Science & Medicine, suggests that if reparations had been made before the COVID-19 pandemic, transmission of SARS-CoV-2 in the state's overall population could have been reduced by anywhere from 31 percent to 68 percent.
The work was done in collaboration with the Lancet Commission on Reparations and Redistributive Justice.
"While there are compelling moral and historical arguments for racial-injustice interventions such as reparations, our study demonstrates that repairing the damage caused by the legacy of slavery and Jim Crow racism would have enormous benefits to the entire population of the United States," said study senior author Eugene Richardson, assistant professor of global health and social medicine in the Blavatnik Institute at Harvard Medical School.
The disproportionate effects of COVID-19 on racial minorities--Black individuals in particular--have been well documented. Black people get COVID-19 at a rate nearly one and a half times higher than that of white people, are hospitalized at a rate nearly four times higher, and are three times as likely to die from the disease, according to the latest estimates from the U.S. Centers for Disease Control.
The greater disease burden among Black people has caused tremendous loss of life and unspeakable suffering across these already vulnerable and disadvantaged communities. Notably, these effects have also spilled over and are driving transmission rates of the virus in the overall population, the study authors said.
Addressing the structural inequalities at the roots of this disparity through monetary reparations would not only radically decrease the impact of COVID-19 among the people who received reparations, the authors said, but would reduce the overall toll of the disease on a broader scale, benefiting the entire population.
The findings, the researchers said, powerfully underscores the truly global nature of the pandemic and the notion that a society is only as strong as its most vulnerable members.
"If we extrapolate these results to the entire United States, we can imagine that tens or hundreds of thousands of lives would have been spared, and the entire nation would have been saved much of the hardship it has endured in the last year," said Richardson, who is also the chair of the Lancet Commission on Reparations and Redistributive Justice.
For their analysis, the researchers paired sophisticated data analytics and computational tools with commonly used epidemiologic modeling methods to calculate the impact of structural racism on infection rates among Black and white populations in Louisiana. They chose Louisiana as an exemplar of the impacts of structural racism in the U.S. because it was one of the few states that reported infection rates by race in the early stages of the pandemic. For a control group, the researchers chose the relatively egalitarian population of South Korea.
The researchers noted that although modeling is used to understand many factors in the spread of an infectious disease, such as differences in infection risk based on whether passengers on a train sit with windows open or closed or individual variations in mask-wearing habits, it has rarely been used to capture the effects of social factors that can create vast disparities between populations, such as those seen between Blacks and whites in the U.S.
Richardson's recent book Epidemic Illusions explores the ways conventional epidemiology is constrained from proposing solutions that address the root causes of health disparities derived from the combined weight of centuries of racism, imperialism, neoliberal politics, and economic exploitation. One of the goals of the paper is to challenge the narrow ways people who work in medicine and public health measure and think about problems and solutions and to broaden the public imagination, thus opening new conversations about what challenges and opportunities are worth considering in global health and social science, Richardson said.
The study examined the initial period of the outbreak, before infection control measures were implemented, so any differences in infection rates between populations at that time would have been driven mainly by differences in the social structures, the researchers said.
For example, Louisiana has a population heavily segregated by race, with Black people having higher levels of overcrowded housing and working jobs that are more likely to expose them to SARS-CoV-2 than white people. In comparison, South Korea has a more homogenous population with far less segregation.
To probe how such structural inequities impact transmission of SARS-CoV-2, the researchers examined infection rates over time for the first two months of the epidemic in each location. During the initial phase of the outbreak in Louisiana, each infected person spread the virus to1.3 to 2.5 more people than an infected individual during the same phase of the outbreak in South Korea, the analysis showed. The study also showed it took Louisiana more than twice as long to bring the early wave of the epidemic under control as South Korea.
Next, the researchers used next-generation matrices to gauge how overcrowding, segregation, and the wealth gap between Blacks and whites in Louisiana could have driven higher infection rates and how monetary reparations would affect viral transmission.
The model showed that greater equity between Blacks and whites might have reduced infection transmission rates by anywhere from 31 percent to 68 percent for every person in the state.
This research comes at a time when many Americans are already thinking about the larger societal costs of structural racism, the researchers said. They noted, for example, that the nationwide movement to protest police brutality against Black people has been fueled by many of the inequitable outcomes exemplified so painfully by the coronavirus pandemic in the U.S.
"This moment has made it possible for a lot of people who had no reason to think about these inequalities to be very aware of them," said study co-author and Lancet reparations commissioner Kirsten Mullen, who was a member of concept development team for the National Museum of African American History and Culture.
Anti-racism in action
Richardson said that the research was designed to explore how reparations payments might have altered the trajectory of the coronavirus pandemic in the U.S. and how a different response to the disease could have helped mitigate the disparities fueled by social conditions that are vestiges of slavery. Such conditions, Richardson noted, include ongoing discrimination and structural racism in the form of redlining, overcrowding, over-incarceration, and the heightened use of lethal force in policing experienced by Black people.
Richardson said that historian and anti-racist scholar Ibram X. Kendi's description of the differences between racism and anti-racism were helpful in designing the study. According to Kendi, a racist policy is any policy that produces or sustains inequality or promotes the power of one racial group over another, whereas an anti-racist policy is any measure that produces or sustains equity between racial groups.
Richardson said that one important goal of the project was to attempt to harness the power of mathematical modeling for an anti-racist response to the coronavirus and beyond.
"When you look at a formula for transmissibility, it looks like an objective calculation," he said. "But where is lethal policing in that formula?"
Richardson noted that it was important to call attention to the systemic and structural elements of racism that can get lost in simplified models of disease.
What are reparations?
Mullen and study co-author William Darity, who recently published a book on reparations and have written in the press about the case for using reparation payments to fight COVID-19, defined reparations as a program of acknowledgement, redress, and closure for a grievous injustice. In this case, Mullen said, the atrocities are associated with periods of enslavement, legal segregation and white terrorism during the Jim Crow era, and racial strife and violence of the post-Civil Rights Act era, including ongoing inequities in the form of over-policing, police executions of unarmed Black people, ongoing discrimination in regard to incarceration, access to housing, and, possibly most important, the Black-white gulf in wealth.
Successful reparations programs include three elements: admission of culpability on behalf of the perpetrators of the atrocity; redress, in the form of an act of restitution; and closure, wherein the victims agree that the debt is paid and no further claims are to be made unless new harms are inflicted.
In this case, Mullen said, reparations would take the form of financial restitution for living Black individuals who can show that they are descended from at least one ancestor who was enslaved in the U.S. and that they self-identified as Black on a legal document at some point during the 12 years prior.
The financial restitution is designed to help close the Black-white wealth gap. Darity noted that it is important to distinguish wealth from income. Wealth is how much you own, and income is how much you earn. Greater wealth translates to greater stability for individuals and families across time. Greater wealth is also more strongly associated with greater well-being than greater income, Darity said, and disparities in wealth manifest as health disparities.
"Wealth is more strongly associated with familial or individual well-being," said Darity, who is the Samuel DuBois Cook Distinguished Professor of Public Policy at Duke University and a Lancet reparations commissioner. He noted that, according to the Federal Reserve Board 2016 Survey of Consumer Finances, the average Black household had a net worth $800,000 lower than the average white household, and that Black people, who represent 13 percent of the U.S. population, only own 3 percent of the nation's wealth.
"This dramatically restricts the ability of Black Americans to survive and thrive," Darity said.
To assess the effect of reparation payments on the trajectory of the pandemic, the researchers based their calculations on a model that would pay $250,000 per person or $800,000 per household to descendants of enslaved individuals--one of several proposed reparation models.
Every transmission is a social transmission
"Every transmission has a social cause," said study co-author and Lancet reparations commissioner James Jones, associate professor of Earth System Science and a senior fellow at the Woods Institute for the Environment at Stanford University.
For a brief moment when AIDS was in the spotlight during the late 80s and early 90s, people interested in social behavior became interested in mathematical modeling of disease, Jones said. While that interest largely waned, the COVID-19 crisis has highlighted the need to think about social science, inequality, social structure, behavior patterns, and behavior change, as well as how they fit together with how we understand and respond to epidemics, Jones said.
Even the simplest model must account for a rudimentary social structure, Jones said. At its most basic, this can be represented with a generalized estimate of how likely an infected person is to come into contact with a susceptible person. He explained that this number, R0 or "R-naught," is the average number of people an infected individual transmits the virus to. When R0 is less than one, no epidemic is possible because the number of people infected decreases. When R0 is greater than 1 an epidemic is possible. R0 also determines the total number of people who could potentially become infected or how many people would need to be vaccinated to end the epidemic. It can also be used to calculate the so-called endemic equilibrium--which determines whether a disease will continue to exist within a population, simmering constantly in the background or bubbling up seasonally, like influenza.
"That's the theory of infectious disease control in a single parameter," Jones said.
That seeming simplicity can make it hard to focus on the complex ways that infectious diseases move through the real world, the researchers said.
"It's important to highlight that R0 is not simply a function of the pathogen," Jones said. "It's a function of the society." Social and environmental factors like mobility, segregation, and the nature of the built environment help determine rates of infection, he said.
This is one important reason that diseases don't hit all people the same. Global R0 is an average of very different R0s for different groups of people. Some groups are more likely to interact only with members of their own group, some groups are more likely to come in contact with infected people, and some are more susceptible to the disease for other reasons, Jones said.
In this case, the researchers used mathematical models to help understand the differences in R0 for Black people and white people in Louisiana and to help think about how things would change if racism were less prevalent in America.
Absent those interventions, the researchers noted that Black Americans remain at an elevated and inequitable risk of becoming infected and dying during the COVID-19 pandemic and that this inequity will continue to fuel the pandemic for all Americans.
"Increasing equality would have huge benefits on infection rates for everyone," said co-author Momin Malik, who was a data science postdoctoral fellow at the Berkman Klein Center for Internet & Society at Harvard University at the time the study was conducted.
The article Study Claims Reparations For Slavery Could Have Reduced COVID-19 Infections And Deaths In US appeared first on Eurasia Review.

Load-Date: February 12, 2021


End of Document




Global Ice Loss Increases At Record Rate
Eurasia Review
January 26, 2021 Tuesday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 862 words
Body


The rate at which ice is disappearing across the planet is speeding up, according to new research.
And the findings also reveal that the Earth lost 28 trillion tonnes of ice between 1994 and 2017 - equivalent to a sheet of ice 100 metres thick covering the whole of the UK.
The figures have been published by a research team which is the first to carry out a survey of global ice loss using satellite data.
The team, led by the University of Leeds, found that the rate of ice loss from the Earth has increased markedly within the past three decades, from 0.8 trillion tons per year in the 1990s to 1.3 trillion tons per year by 2017.
Ice melt across the globe raises sea levels, increases the risk of flooding to coastal communities, and threatens to wipe out natural habitats which wildlife depend on.
The findings of the research team, which includes the University of Edinburgh, University College London and data science specialists Earthwave, are published in European Geosciences Union's journalThe Cryosphere.
The research, funded by UK Natural Environment Research Council, shows that overall, there has been a 65 % increase in the rate of ice loss over the 23-year survey. This has been mainly driven by steep rises in losses from the polar ice sheets in Antarctica and Greenland.
Lead author Dr Thomas Slater, a Research Fellow at Leeds' Centre for Polar Observation and Modelling , said: "Although every region we studied lost ice, losses from the Antarctic and Greenland ice sheets have accelerated the most.
"The ice sheets are now following the worst-case climate warming scenarios set out by the Intergovernmental Panel on Climate Change. Sea-level rise on this scale will have very serious impacts on coastal communities this century."
Dr Slater said the study was the first of its kind to examine all the ice that is disappearing on Earth, using satellite observations .
He added: "Over the past three decades there's been a huge international effort to understand what's happening to individual components in Earth's ice system, revolutionised by satellites which allow us to routinely monitor the vast and inhospitable regions where ice can be found.
"Our study is the first to combine these efforts and look at all the ice that is being lost from the entire planet."
The increase in ice loss has been triggered by warming of the atmosphere and oceans, which have warmed by 0.26°C and 0.12°C per decade since the 1980, respectively. The majority of all ice loss was driven by atmospheric melting (68 %), with the remaining losses (32%) being driven by oceanic melting.
The survey covers 215,000 mountain glaciers spread around the planet, the polar ice sheets in Greenland and Antarctica, the ice shelves floating around Antarctica, and sea ice drifting in the Arctic and Southern Oceans.
Rising atmospheric temperatures have been the main driver of the decline in Arctic sea ice and mountain glaciers across the globe, while rising ocean temperatures have increased the melting of the Antarctic ice sheet. For the Greenland ice sheet and Antarctic ice shelves, ice losses have been triggered by a combination of rising ocean and atmospheric temperatures.
During the survey period, every category lost ice, but the biggest losses were from Arctic Sea ice (7.6 trillion tons) and Antarctic ice shelves (6.5 trillion tons), both of which float on the polar oceans.
Dr Isobel Lawrence, a Research Fellow at Leeds' Centre for Polar Observation and Modelling, said: "Sea ice loss doesn't contribute directly to sea level rise but it does have an indirect influence. One of the key roles of Arctic sea ice is to reflect solar radiation back into space which helps keep the Arctic cool.
"As the sea ice shrinks, more solar energy is being absorbed by the oceans and atmosphere, causing the Arctic to warm faster than anywhere else on the planet.
"Not only is this speeding up sea ice melt, it's also exacerbating the melting of glaciers and ice sheets which causes sea levels to rise."
Half of all losses were from ice on land - including 6.1 trillion tons from mountain glaciers, 3.8 trillion tons from the Greenland ice sheet, and 2.5 trillion tons from the Antarctic ice sheet. These losses have raised global sea levels by 35 millimetres.
It is estimated that for every centimetre of sea level rise, approximately a million people are in danger of being displaced from low-lying homelands.
Despite storing only 1 % of the Earth's total ice volume, glaciers have contributed to almost a quarter of the global ice losses over the study period, with all glacier regions around the world losing ice.
Report co-author and PhD researcher Inès Otosaka, also from Leeds' Centre for Polar Observation and Modelling, said: "As well as contributing to global mean sea level rise, mountain glaciers are also critical as a freshwater resource for local communities.
"The retreat of glaciers around the world is therefore of crucial importance at both local and global scales."
Just over half (58 %) of the ice loss was from the northern hemisphere, and the remainder (42 %) was from the southern hemisphere.
The article Global Ice Loss Increases At Record Rate appeared first on Eurasia Review.

Load-Date: January 26, 2021


End of Document




Forecasting Coastal Water Quality
Eurasia Review
January 24, 2021 Sunday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 806 words
Body


Less than two days of water quality sampling at local beaches may be all that's needed to reduce illnesses among millions of beachgoers every year due to contaminated water, according to new Stanford research.
The study, published inEnvironmental Science & Technology, presents a modeling framework that dependably predicts water quality at beaches after only a day or two of frequent water sampling. The approach, tested in California, could be used to keep tabs on otherwise unmonitored coastal areas, which is key to protecting the well-being of beachgoers and thriving ocean economies worldwide.
"This work combines knowledge of microbiology, coastal processes and data science to produce a tool to effectively manage one of our most precious resources and protect human health," said senior author Alexandria Boehm, a Stanford professor of civil and environmental engineering.
Measuring concentrations of fecal indicator bacteria (FIB) - which denote the presence of fecal matter and can lead to unsafe water conditions - at beaches ensures the health and safety of the public. While all ocean water contains some degree of pathogens, such as bacteria or viruses, they're typically diluted to harmless concentrations. However, changes in rainfall, water temperature, wind, runoff, boating waste, storm sewer overflow, proximity to waste treatment plants, animals and waterfowl can lead to an influx of water contamination. Exposure to these contaminants can cause many ailments, including respiratory diseases and gastrointestinal illnesses, along with skin, eye and ear infections to swimmers.
Protecting coastal waters and the people that use them remains essential for much of California's 840 miles of coastline. Over 150 million people swim, surf, dive and play at one of the state's 450 beaches annually, generating over $10 billion in revenue. According to the California State Water Resources Control Board, health agencies across 17 counties, publicly owned sewage treatment plants, environmental groups and several citizen-science groups perform water sampling across the state. However, not all waters are routinely checked due to accessibility issues, budget resource constraints or the season, despite their use by the public.
Another obstacle to safeguarding public health lies in the lag time between sampling and results - up to two-days - leading beach managers to make decisions reflecting past water quality conditions. When monitored waters contain high levels of bacteria and pose a health risk, beach managers post warning signs or close beaches. The delay in current testing methods could unknowingly expose swimmers to unhealthy waters.
To overcome these limitations, the researchers combined water sampling and environmental data with machine learning methods to accurately forecast water quality. While predictive water quality models aren't new, they have generally required historical data spanning several years to be developed.
The team used water samples collected at 10-minute intervals over a relatively brief timeframe of one to two days at beaches in Santa Cruz, Monterey and Huntington Beach. Among the three sites, 244 samples were measured for FIB concentrations and marked as above or below the acceptable level deemed safe by the state. The researchers then collected meteorological data such as air temperature, solar radiation and wind speed along with oceanographic data including tide level, wave heights and water temperature (all factors influencing FIB concentrations) over the same timeframe.
Using the high-frequency water quality data and machine learning methods, they trained computer models to accurately predict FIB concentrations at all three beaches. The researchers found hourly water sampling for 24 hours straight - capturing an entire tidal and solar cycle - proved enough for reliable results. Feeding the framework meteorological and tidal data from longer time periods resulted in future water quality predictions that were dependable for at least an entire season.
"These results are really empowering for communities who want to know what's going on with water quality at their beach," Searcy said. "With some resources to get started and a day of sampling, these communities could collect the data needed to initiate their own water quality modeling systems."
The framework code, which is publicly accessible, could also be developed for accurate predictions of other contaminants such as harmful algae, metals and nutrients known to wreak havoc on local waters. The researchers point out that more analysis is needed to better determine the exact timeframe these models remain accurate and note that continually assessing and retraining the models remains a best practice for accurate predictions.
The article Forecasting Coastal Water Quality appeared first on Eurasia Review.

Load-Date: January 24, 2021


End of Document




Climate Change To Alter Position Of Earth's Tropical Rain Belt
Eurasia Review
January 19, 2021 Tuesday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 651 words
Body


Future climate change will cause a regionally uneven shifting of the tropical rain belt - a narrow band of heavy precipitation near the equator - according to researchers at the University of California, Irvine and other institutions. This development may threaten food security for billions of people.
In a study published in Nature Climate Change, the interdisciplinary team of environmental engineers, Earth system scientists and data science experts stressed that not all parts of the tropics will be affected equally. For instance, the rain belt will move north in parts of the Eastern Hemisphere but will move south in areas in the Western Hemisphere.
According to the study, a northward shift of the tropical rain belt over the eastern Africa and the Indian Ocean will result in future increases of drought stress in southeastern Africa and Madagascar, in addition to intensified flooding in southern India. A southward creeping of the rain belt over the eastern Pacific Ocean and Atlantic Ocean will cause greater drought stress in Central America.
"Our work shows that climate change will cause the position of Earth's tropical rain belt to move in opposite directions in two longitudinal sectors that cover almost two thirds of the globe, a process that will have cascading effects on water availability and food production around the world," said lead author Antonios Mamalakis, who recently received a Ph.D. in civil & environmental engineering in the Henry Samueli School of Engineering at UCI and is currently a postdoctoral fellow in the Department of Atmospheric Science at Colorado State University.
The team made the assessment by examining computer simulations from 27 state-of-the-art climate models and measuring the tropical rain belt's response to a future scenario in which greenhouse gas emissions continue to rise through the end of the current century.
Mamalakis said the sweeping shift detected in his work was disguised in previous modelling studies that provided a global average of the influence of climate change on the tropical rain belt. Only by isolating the response in the Eastern and Western Hemisphere zones was his team able to highlight the drastic alterations to come over future decades.
Co-author James Randerson, UCI's Ralph J. & Carol M. Cicerone Chair in Earth System Science, explained that climate change causes the atmosphere to heat up by different amounts over Asia and the North Atlantic Ocean.
"In Asia, projected reductions in aerosol emissions, glacier melting in the Himalayas and loss of snow cover in northern areas brought on by climate change will cause the atmosphere to heat up faster than in other regions," he said. "We know that the rain belt shifts toward this heating, and that its northward movement in the Eastern Hemisphere is consistent with these expected impacts of climate change."
He added that the weakening of the Gulf Stream current and deep-water formation in the North Atlantic is likely to have the opposite effect, causing a southward shift in the tropical rain belt across the Western Hemisphere.
"The complexity of the Earth system is daunting, with dependencies and feedback loops across many processes and scales," said corresponding author Efi Foufoula-Georgiou, UCI Distinguished Professor of Civil & Environmental Engineering and the Henry Samueli Endowed Chair in Engineering. "This study combines the engineering approach of system's thinking with data analytics and climate science to reveal subtle and previously unrecognized manifestations of global warming on regional precipitation dynamics and extremes."
Foufoula-Georgiou said that a next step is to translate those changes to impacts on the ground, in terms of flooding, droughts, infrastructure and ecosystem change to guide adaptation, policy and management.
The article Climate Change To Alter Position Of Earth's Tropical Rain Belt appeared first on Eurasia Review.

Load-Date: January 18, 2021


End of Document




Big Data To Analyze The Mystery Of Beethoven's Metronome
Eurasia Review
January 4, 2021 Monday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 878 words
Body


Data science and physics research at the Universidad Carlos III de Madrid and UNED has analysed a centuries-old controversy over Beethoven's annotations about the tempo (the playing speed) of his works, which is considered to be too fast based on these marks. In this study, published in the PLOS ONE journal, it is noted that this deviation could be explained by the composer reading the metronome incorrectly when using it to measure the beat of his symphonies.
Ludwig van Beethoven (1770-1827) was one of the first composers to start using a metronome, a device patented by Johann Nepomuk Maelzel in 1815. At that time, he started to edit his works with numerical marks with metronome indications. Doubts about the validity of these marks date back to the 19th century and during the 20th century many musicological analyses were carried out, some of which already pointed to the hypothesis that the metronome was broken, an assumption that could never be verified.
In any case, most orchestra conductors have omitted these marks as they consider them to be too fast (Romanticism), whereas since the 1980s, other conductors (Historicism) have used them to play Beethoven. However, music critics and the public described these concerts as frantic and even unpleasant.
Previous scientific research, such as Sture Forsén's study in 2013, has pointed to several defects that may have affected the metronome, causing it to function slower, which would have led the composer from Bonn to choose faster marks than those actually proposed. In order to validate this explanation, researchers from the UC3M and UNED have systematically compared the metronomic marks with contemporary interpretations. This requires physical skills to model the metronome mathematically, analyse data, computing, usability, and, of course, music skills. Overall, they have analysed the tempo and its variations for each movement of 36 symphonies interpreted by 36 different conductors, a total of 169 hours of music.
"Our study has revealed that conductors tend to play slower than Beethoven indicated. Even those who aim to follow his directions to the letter! The tempi indicated by the composer are, in general, too fast, to the point that, collectively, musicians tend to slow them down," says Iñaki Ucar, one of the authors of this research, data scientist at the UC3M's Big Data Institute, and clarinetist. This slowing down follows, on average, a systematic deviation, so it is not random, but conductors tend to play consistently below Beethoven's marks.
"This deviation could be explained by the composer reading the scale of the apparatus in the wrong place, for example, under the weight instead of above. Ultimately, this would be a problem caused by using new technology," says Almudena Martín Castro, the other author of the study, user experience designer and pianist, who carried out this research within the framework of her Bachelor Thesis for her Degree in Physics at UNED.
In this study, researchers have developed a mathematical model for the metronome based on a double pendulum, perfected with three types of corrections which take the amplitude of its oscillation, the friction of its mechanism, the impulse force, and the mass of its rod, an aspect that had not been considered in previous work, into account.
"With the help of this model, we developed a methodology for estimating the original parameters of Beethoven's metronome from photographs that are available and the patent outline," the work explains. In addition to this, they dismantled a modern metronome to measure it and use it to validate both the mathematical model and methodology.
The researchers tried to identify a "break" in the metronome that gave rise to the slow tempi usually followed by musicians. They tried to change the metronome's mass (it may have been damaged and a piece may have fallen off), move it onto the rod, increase the friction (the metronome may have been poorly lubricated) and even testing the assumption that the apparatus may have been misplaced, leaning over the piano while the composer was creating his music.
"None of the hypotheses matched what the data told us, which is a homogeneous slowdown in the tempi on the entire scale. Finally, we considered the fact that the deviation matches the size of the metronome's weight exactly, and we also found the annotation '108 or 120' on the first page of the manuscript for his ninth symphony, which indicates that the composer doubted where he was reading at least once. Suddenly, it all made sense: Beethoven was able to write down a lot of these marks by reading the tempo in the wrong place," they explain.
This methodology could be applied when investigating the work of other classical composers, as they are able to extract the tempo from a musical recording and clean up the data so they can be compared.
"Studying the relationship between the tempo played and marks from other composers would be very interesting, or even looking for the 'correct tempo' for composers who did not leave any metronomic marks. Is it possible that there is an average tempo at which people usually interpret Bach's fugues, for example?" they ask.
The article Big Data To Analyze The Mystery Of Beethoven's Metronome appeared first on Eurasia Review.

Load-Date: January 4, 2021


End of Document




Understanding How Birds Respond To Extreme Weather Can Inform Conservation Efforts
Eurasia Review
August 24, 2020 Monday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 783 words
Body


When it comes to climate change, University of Wisconsin¬-Madison forest and wildlife ecology Professor Ben Zuckerberg says birds are the proverbial canary in the coal mine. They are both responsive and sensitive to changes in the environment, including the extreme weather events associated with a warming planet.
However, not all birds are the same, and not all weather events have the same impact. How do different bird species respond to extreme weather events that occur for different amounts of time, ranging from weekly events like heat waves to seasonal events like drought? And how do traits unique to different species -- for example, how far they migrate or how commonly they occur -- predict their vulnerability to extreme weather?
To answer these questions, ecologists would traditionally observe a small number of bird species at a few sites over a few years, and then draw general conclusions. However, Zuckerberg and UW-Madison postdoctoral researcher Jeremy Cohen, along with Daniel Fink of the Cornell Lab of Ornithology, had more ambitious goals: they looked at 109 species across eastern North America over a 15-year period, and integrated this information with fine-scale satellite temperature and precipitation data.
In a study recently published in the journalGlobal Change Biology, the researchers show that not all birds are equally vulnerable to the effects of extreme weather resulting from climate change. As the planet warms, some species will adapt while others may struggle without conservation measures. The results of this study could help conservationists target their efforts to vulnerable species, as well as locations where extreme weather events are predicted.
The researchers used data from eBird, a global citizen-science initiative where bird enthusiasts submit checklists of bird sightings online. These checklists include which species were seen, how many, the location and time, and other observations.
The researchers compiled more than 830,000 of these checklists and integrated each one with weather data summarized over the week, month and three months before the observation was recorded. They relied on advanced computing to manage this large amount of information.
"The study we did would not have been remotely possible without data science," says Cohen. The emerging field of data science involves the study, development or application of methods that reveal new insights from data.
Zuckerberg points out that the combination of citizen science and data science makes research possible at a scale that was previously unimaginable for ecologists. However, citizen science has its limitations. Researchers have less control over the scientific process, and data quality can vary.
"Someone can go out for five minutes or two hours and submit eBird data. They can submit a checklist for 10 species or 40 species," says Zuckerberg. "We've adopted data science methods for working with large, unstructured data sets."
After controlling for this noisy data, the researchers observed that some species are less sensitive to extreme weather, and populations are not equally exposed to its effects because some geographic areas are warming faster than others.
When it comes to heat waves, Cohen notes, "long-distance migrants were not super affected by really hot periods. They winter in tropical environments and should be tolerant of heat."
However, resident birds and short-distance migrants such as robins and red-winged blackbirds responded negatively to heat waves, with their numbers sometimes declining 10% to 30% over several weeks.
As for drought, commonly occurring species like crows were more resilient than rare birds, particularly if the drought was severe and long-lasting.
"Rarer species have more specialized habitat and food requirements -- this is a general rule in ecology," says Cohen. "More common species usually have more options. If habitat quality declines due to drought, a generalist can go somewhere else."
Cohen says this is the first large-scale study, spanning half a continent, to look at how birds respond immediately after weather events. Because of the scope of the project, conservationists can better understand how many different bird species are likely to be affected by climate change, and mitigate some of the negative effects.
"If birds are truly winged sentinels of climate change, the greater likelihood of drought, flooding and extreme temperature conditions like heat waves will have significant consequences," says Zuckerberg. "We need to think about how we help species adapt to climate extremes."
The article Understanding How Birds Respond To Extreme Weather Can Inform Conservation Efforts appeared first on Eurasia Review.

Load-Date: August 25, 2020


End of Document




The Danger Of Weaponizing Trade For The Environment – Analysis
Eurasia Review
November 30, 2020 Monday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 984 words
Body


By Ken Heydon
Pressure around the world is growing to apply penalty tariffs on imports from perceived environmental free riders. But such policies are a threat to trade and are unlikely to help the environment. Fortunately, there are better policy alternatives to deal with trade and environment linkages, including tackling fossil fuel subsidies.
Prominent economists such as William Nordhaus and Thomas Piketty are advocating the imposition of carbon border taxes on imports from polluting countries. These calls are founded on the fear that levies on carbon-intensive production simply push production to countries where it is not taxed. Those imposing such border taxes might also claim legitimacy under General Agreement on Tariffs and Trade (GATT) Article XX which allows measures necessary to protect human, animal or plant life.
There is howeverno evidence of growthof widespread pollution havens. The International Energy Agency reports that by 2019 global energy-related carbon dioxide (CO2) emissions had flattened, withstrong renewables growthin China and India. China, Japan and South Korea have each recently set target dates for zero net carbon emissions.
Trade sanctions carry the risk of protectionist capture and of being a brake on the very economic development needed to fund the transition to cleaner energy and, now, to tackle theeconomic disruption from COVID-19and the associated acceleration of digitisation.
Carbon border adjustments— tax levied on imports from countries without carbon pricing mechanisms — are becoming an integral part of EU trade policy, and US president-elect Joe Biden has also expressed support for them.
These sanctions may take the form of unilateral action by Europe and the United States or be applied through EU and US preferential trade agreements (PTAs) in the Asia Pacific and elsewhere. The EU–Japan agreement, for example, contains commitments that the European Union could invoke to promote a more aggressive approach to trade and the environment, including that parties shall cooperate to promote the contribution of trade to the transition to low greenhouse emissions (Article 16.4). Similar risks are inherent in negotiation of the Australia–EU PTA.
As for the United States, Joe Biden has made it clear that any consideration of US re-engagement in the Trans-Pacific Partnership will depend on stronger commitments being made on the environment and labour. Such commitments could involve trade penalties.
Advocacy of these measures should be rebutted at every opportunity, but it is not enough just to say 'no' to trade sanctions. The energy transition is not assured and more needs to be done. Despite their movement in the right direction, China, Japan and South Korea still fund the majority of new coal-fired power plants. Fortunately, there are other trade-related measures that can be taken to serve environmental ends and which, importantly, involve reducing rather than increasing distortions to trade.
Two such measures have been on the World Trade Organization's (WTO) agenda for years but are proving frustratingly difficult to advance: attempts to reduce fishing subsidies and negotiations to liberalise trade in environmental goods and services. While this work should be maintained and accelerated — within a hopefully revitalised WTO — two other measures might yield more immediate results.
The first is action in the WTO to reduce fossil fuel subsidies. The elimination of fossil fuel subsidies would, according to the International Monetary Fund, reduce global CO2 emissions by up to 23 per cent. Some WTO disputes have targeted government support for renewable energy, giving grounds to also target policies supporting fossil fuel-based energy.
Two implementation challenges would need tackling. First, the link betweendomesticsubsidies and trade needs to be demonstrated. This can be done by invoking the Anti-Dumping Agreement to determine that energy-subsidised exports constitute exporting at less than normal value and so are open to retaliation. Second, to avoid social disruption in fossil-fuel-dependent developing countries, mitigating development assistance policies need to be implemented, coordinated by a body such as the G20.
Global fossil fuel subsidy reductions could spur reform in Australia. Support to fossil fuel consumption has increased significantly in Australia over the past decade, with revenue forgone equivalent to over 40 per cent of the energy-related tax take, a high share byOECD standards.
A second necessary area of action — again with implications for Australia — is to ensure that the ongoing US–China tech war and ill-advised pursuit of decoupling does not bring further collateral damage to vital cooperation with China on renewable energy. Australia's export of education services in electrical engineering to Chinese (and other) students has supported the development of solar PV panel manufacturing plants in China for export to Australia and the rest of the world. Trade openness is thus vital to the energy transition.
Looking ahead, as countries move to 'smart energy' policies that depend on digital grids, Australia's expertise in information and communications technology, and data science will become an increasingly valuable tradable service. Trade has a direct role to play in the pursuit of environmental goals as a facilitator, not a weapon, that can benefit all countries involved.
*About the author: Ken Heydon is a visiting fellow at the London School of Economics. He is formerly an Australian trade official, Deputy Director-General of the Office of National Assessments and senior member of the OECD secretariat. His latest book is The Political Economy of International Trade: Putting Commerce in Context (Polity, 2019).
Source: This article was published by East Asia Forum
The article The Danger Of Weaponizing Trade For The Environment - Analysis appeared first on Eurasia Review.

Load-Date: November 30, 2020


End of Document




Forecasting Urbanization
Eurasia Review
May 9, 2020 Saturday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1071 words
Body


University of Delaware data scientist Jing Gao is fascinated by the ways that cities and towns grow over time. This concept is known as urbanization.
Take Chicago, Los Angeles and New York. All of these are cities, but they each grow differently, in terms of how the city's land areas expand. The same is true globally, from New Delhi, India, to Paris, France.
Gao, an assistant professor of geography and spatial sciences in UD's College of Earth, Ocean and Environment, and collaborator Brian O'Neill, a UD alumnus and professor from the University of Denver, have created a new global simulation model to predict how urban land will change over the next 100 years under different social and economic conditions.
The research leverages data science and machine learning to provide a first long-term look at how urbanization will unfold - decade by decade.
The researchers describe their simulations in a paper published in the journal Nature Communications.
Data science helps long-term forecasting
According to Gao, until recently it has been difficult to generate long-term, global forecasts of urban expansion. This is because while urbanization is a global trend, the way cities develop (buildings, roads, people, economics) can change over time. Additionally, this development can vary widely country to country, and even within different parts of the same country.
To understand how this change occurs, Gao and O'Neill used data science to analyze 15 global data sets depicting various aspects of urbanization, including a newly available global map series showing urban land change over the past 40 years based on satellite images of Earth. The global maps are accurate to within approximately 125 feet (38 meters) and provide a uniquely detailed look at past urban development that was not previously possible with this degree of specificity.
"Mining historical data revealed that there are three different urbanization styles: urbanized, steadily urbanizing and rapidly urbanizing," Gao said. "And countries evolve from rapidly urbanizing to steadily urbanizing to urbanized over time."
It should come as no surprise that the United States and most western European countries are already urbanized. India and China, which previously experienced rapid development, have now transitioned to steadily urbanizing. Rapidly urbanizing countries at present include many countries in Africa.
And here's the data science part. Understanding these broad styles is not enough to capture - globally - how urbanization is playing out on the ground at a local scale.
To do this, the researchers divided the world into 375 small regions and ran a unique model for each region simultaneously, then pieced results from all models together to develop a global map. This information can shed light on how our cities may change and reveal potential impacts of urbanization that can inform local to global urban planners and policymakers.
The research team's projections show that the total amount of urban areas on Earth can grow anywhere from 1.8 to 5.9-fold by 2100. On average, if past urbanization trends continue, the world will build approximately 618,000 square miles (1.6 million square kilometers) of new urban areas globally over the century. This is an area roughly 4.5 times the size of Germany, or, more than 225 million football fields.
How this urban expansion occurs, however, largely depends on societal trends in the years to come. This includes trends in economic growth, population change and lifestyle habits, and what level of consideration is given to how our habits affect the environment.
For both developed and developing countries, for example, countries in Europe and Southeast Asia, urban expansion is expected to roughly triple if society favors materialistic and fossil-fuel driven development instead of adopting a sustainability mindset.
In the U.S., the least urban expansion occurs if people are focused on sustainability, such as green development and environmental awareness. In this case, urban land is expected to grow by 1.3 times by 2100. But if people favor highly materialistic development over the same timeframe, with high consumption of fossil fuels and a material-driven society, sprawl-like urban expansion is expected, with close to four times the amount of expansion the U.S. had at the beginning of the century.
The U.S. already is among the countries with the largest amount of developed land, so four-fold growth in urban expansion is a lot.
"This is where our projections can inform policy and planning," said Gao. "These projections can help researchers and analysts understand how large-scale changes that occur over a long time period, such as climate change, may affect local urban areas."
Most individuals do not realize how changes to the landscape, such as buildings and roads, may affect their lives. In Delaware, for example, second homes being built near the coast often come at the cost of agricultural farmland. While these developments may increase an area's economic prosperity, they can have other unintended consequences, such as increased potential exposure to coastal flooding and sea level rise.
And, no matter what socio-economic scenario was selected, the simulations show that most countries will become urbanized by the end of the century.
One interesting finding from the work is that although prevailing thought is that urbanization is primarily happening in the developing world, Gao said this may not be the case.
"If you look at the data collected over the past 40 years, the absolute amount of new urban land construction in the developed world is comparable to the developing world," she said. "However, the changes seem faster in the developing world because there currently is much less developed land there, so the rate of change appears greater."
This begs the question: as developing countries in Africa continue to grow, will they ever catch up to or surpass developed countries like the United States in terms of urbanized land?
"According to today's definition, Africa is expected to become urbanized by 2100," said Gao. "But even if it continues developing at a very fast rate relative to the rest of the world throughout the century, it won't catch up to developed countries like the U.S. because the difference at the outset is large and the developed world still keeps expanding its urban area."
The post Forecasting Urbanization appeared first on Eurasia Review.

Load-Date: May 10, 2020


End of Document




Irish And UK Research Helps To Unravel Secrets Behind Game Of Thrones
Eurasia Review
November 10, 2020 Tuesday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 679 words
Body


A researcher at University of Limerick in Ireland has played a key role in examining some of the secrets behind Game of Thrones.
What are the secrets behind one of the most successful fantasy series of all time? How has a story as complex as the one in George R.R. Martin's novels enthralled the world and how does it compare to other narratives?
Researchers from five universities across the UK and Ireland - including UL's Dr Padraig MacCarron - came together to unravel 'A Song of Ice and Fire', the books on which the TV series is based.
In a paper that has just been published by theProceedings of the National Academy of Sciences of the USA, the team of physicists, mathematicians and psychologists from Coventry, Warwick, Limerick, Cambridge and Oxford universities used data science and network theory to analyse the acclaimed book series by George R.R. Martin.
The study shows the way the interactions between the characters are arranged is similar to how humans maintain relationships and interact in the real world. Moreover, although important characters are famously killed off at random as the story is told, the underlying chronology is not at all so unpredictable, the research shows.
The team found that, despite over 2,000 named characters in 'A Song of Ice and Fire' and over 41,000 interactions between them, at chapter-by-chapter level these numbers average out to match what we can handle in real life. Even the most predominant characters - those who tell the story - average out to have only 150 others to keep track of. This is the same number that the average human brain has evolved to deal with.
While matching mathematical motifs might have been expected to lead to a rather narrow script, George R. R. Martin keeps the tale bubbling by making deaths appear random as the story unfolds. But, as the team show, when the chronological sequence is reconstructed the deaths are not random at all: rather, they reflect how common events are spread out for non-violent human activities in the real world.
"These books are known for unexpected twists, often in terms of the death of a major character, it is interesting to see how the author arranges the chapters in an order that makes this appear even more random than it would be if told chronologically," explained Dr Padraig MacCarron, a postdoctoral researcher at the Centre for Social Issues Research and Mathematics Applications Consortium for Science and Industry (MACSI) at UL.
"Social networks of the most connected characters, while seemingly extensive, mirrored the typical range of social networks that humans maintain. Furthermore, characters' social networks did not extend beyond the cognitive limit of social connections that humans are able to sustain.
"Although the time intervals between significant deaths in relation to the story's timeline may appear random, they are not told in chronological order. Re-arranging them in order of which they occur, they follow a pattern more commonly observed in reality," added Dr MacCarron.
'Game of Thrones' has invited all sorts of comparison to history and myth and the marriage of science and humanities in this paper opens new avenues to comparative literary studies. It shows, for example, that it is more akin to the Icelandic sagas than to mythological stories such as the Tain Bo Cuailnge or Beowulf. The trick in Game of Thrones, it seems, is to mix realism and unpredictability in a cognitively engaging manner.
"People largely make sense of the world through narratives, but we have no scientific understanding of what makes complex narratives relatable and comprehensible. The ideas underpinning this paper are steps towards answering this question," explained Professor Colm Connaughton, from the University of Warwick.
Fellow researcher Professor Robin Dunbar, from the University of Oxford, observed: "This study offers convincing evidence that good writers work very carefully within the psychological limits of the reader."
The article Irish And UK Research Helps To Unravel Secrets Behind Game Of Thrones appeared first on Eurasia Review.

Load-Date: November 11, 2020


End of Document




Artificial Intelligence Can Predict Students' Educational Outcomes Based On Tweets
Eurasia Review
October 24, 2020 Saturday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 2100 words
Body


Ivan Smirnov, Leading Research Fellow of the Laboratory of Computational Social Sciences at the Institute of Education of HSE University, has created a computer model that can distinguish high academic achievers from lower ones based on their social media posts. The prediction model uses a mathematical textual analysis that registers users' vocabulary (its range and the semantic fields from which concepts are taken), characters and symbols, post length, and word length.
Every word has its own rating (a kind of IQ). Scientific and cultural topics, English words, and words and posts that are longer in length rank highly and serve as indicators of good academic performance. An abundance of emojis, words or whole phrases written in capital letters, and vocabulary related to horoscopes, driving, and military service indicate lower grades in school. At the same time, posts can be quite short--even tweets are quite informative. The study was supported by a grant from the Russian Science Foundation (RSF), and an article detailing the study's results was published in EPJ Data Science.
Smirnov's study used a representative sample of data from HSE University's longitudinal cohort panel study, 'Educational and Career Trajectories' (TrEC). The study traces the career paths of 4,400 students in 42 Russian regions from high schools participating in PISA (the Programme for International Students Assessment). The study data also includes data about the students' VK accounts (3,483 of the student participants consented to provide this information).
'Since this kind of data, in combination with digital traces, is difficult to obtain, it is almost never used,' Smirnov says. Meanwhile, this kind of dataset allows you to develop a reliable model that can be applied to other settings. And the results can be extrapolated to all other students--high school students and middle school students.
Posts from publicly viewable VK pages were used as a training sample--this included a total of 130,575 posts from 2,468 subjects who took the PISA test in 2012. The test allowed the researcher to assess a student's academic aptitude as well as their ability to apply their knowledge in practice. The study included only publicly visible VK posts from consenting participants.
When developing and testing the model from the PISA test, only students' reading scores were used an indicator of academic aptitude, although there are three tests in total: reading, mathematics, and science. PISA defines reading literacy as 'understanding, using, reflecting on and engaging with written texts in order to achieve one's goals, to develop one's knowledge and potential, and to participate in society.' The exam has six proficiency levels. Students who score a 2 are considered to meet only the basic, minimum level, while those who score a 5 or 6 are considered to be strong students.
In the study, unsupervised machine learning with word vector representations was performed on VK post corpus (totaling 1.9 billion words, with 2.5 million unique words). It was combined with a simpler supervised machine learning model that was trained in individual positions and taught to predict PISA scores.
'We represented each post as a 300-dimensional vector by averaging over vector representations of all its constituent words,' Smirnov writes. 'These post representations were used to train a linear regression model to predict the PISA scores of the posts' authors.'
By 'predict', the researcher does not refer to future forecasting, but rather the correlation between the calculated results and the real scores students earned on the PISA exam, as well as their USE scores (which are publicly available online in aggregated form--i.e., average scores per school). In the preliminary phase, the model learned how to predict the PISA data. In the final model, the calculations were checked against the USE results of high school graduates and university entrants.
The final model was supposed to be able to reliably recognize whether a strong student or a weak student had written a particular social media post, or in other words, differentiate the subjects according to their academic performance. After the training period, the model was able to distinguish posts written by students who scored highly or poorly on PISA (levels 5-6 and levels 0-1) with an accuracy of 93.7%. As for the comparability of PISA and the USE, although these two tests differ, studies have shown that students' scores for the two tests strongly correlate with each other.
'The model was trained using PISA data, and we looked at the correlation between the predicted and the real PISA scores (which are available in the TrEC study),' Smirnov explains. 'With the USE things gets more complicated: since the model does not know anything about the unified exams, it predicted the PISA scores as before. But if we assume that the USE and PISA measure the same thing -- academic performance -- then the higher the predicted PISA results are, the higher the USE results should be.' And the fact that the model learned to predict one thing and can predict another is quite interesting in itself, Smirnov notes.
However, this also needed to be verified, so the model was then applied to 914 Russian high schools (located in St. Petersburg, Samara and Tomsk; this set included almost 39,000 users who created 1.1 million posts) and one hundred of Russia's largest universities (115,800 people; 6.5 million posts) to measure the academic performance of students at these institutions.
It turned out that 'predicted academic performance is closely related to USE scores,' says Smirnov. 'The correlation coefficient is between 0.49 and 0.6. And in the case of universities, when the predicted academic performance and USE scores of applicants were compared (the information is available in HSE's ongoing University Admissions Quality Monitoring study), then the results also demonstrated a strong connection. The correlation coefficient is 0.83, which is significantly higher than for high schools, because there is more data.'
But can the model be applied to other social media sites? 'I checked what would happen if, instead of posts on VK, we gave the model tweets written by the same users,' Smirnov says. 'It turned out that the quality of the model does not significantly decrease.' But since a sufficient number of twitter accounts were available only for the university dataset (2,836), the analysis was performed only on this set.
It is important that the model worked successfully on datasets of different social media sites, such as VK and Twitter, thereby proving that is can be effective in different contexts. This means that it can be applied widely. In addition, the model can be used to predict very different characteristics, from student academic performance to income or depression.
Smirnov's study used a representative sample of data from HSE University's longitudinal cohort panel study, 'Educational and Career Trajectories' (TrEC). The study traces the career paths of 4,400 students in 42 Russian regions from high schools participating in PISA (the Programme for International Students Assessment). The study data also includes data about the students' VK accounts (3,483 of the student participants consented to provide this information).
'Since this kind of data, in combination with digital traces, is difficult to obtain, it is almost never used,' Smirnov says. Meanwhile, this kind of dataset allows you to develop a reliable model that can be applied to other settings. And the results can be extrapolated to all other students--high school students and middle school students.
Posts from publicly viewable VK pages were used as a training sample--this included a total of 130,575 posts from 2,468 subjects who took the PISA test in 2012. The test allowed the researcher to assess a student's academic aptitude as well as their ability to apply their knowledge in practice. The study included only publicly visible VK posts from consenting participants.
It is important that the scores on the standardized PISA and USE tests were used as an academic aptitude metric. This gives a more objective picture than assessment mechanisms that are school-specific (such as grades).
When developing and testing the model from the PISA test, only students' reading scores were used an indicator of academic aptitude, although there are three tests in total: reading, mathematics, and science. PISA defines reading literacy as 'understanding, using, reflecting on and engaging with written texts in order to achieve one's goals, to develop one's knowledge and potential, and to participate in society.' The exam has six proficiency levels. Students who score a 2 are considered to meet only the basic, minimum level, while those who score a 5 or 6 are considered to be strong students.
In the study, unsupervised machine learning with word vector representations was performed on VK post corpus (totaling 1.9 billion words, with 2.5 million unique words). It was combined with a simpler supervised machine learning model that was trained in individual positions and taught to predict PISA scores.
Word vector representations, or word embedding, is a numeric vector of a fixed size that describes some features of a word or their sequence. Embedding is often used for automated word processing. In Smirnov's research, the fastText system was used since it is particularly conducive to working with Russian-language text.
'We represented each post as a 300-dimensional vector by averaging over vector representations of all its constituent words,' Smirnov writes. 'These post representations were used to train a linear regression model to predict the PISA scores of the posts' authors.'
By 'predict', the researcher does not refer to future forecasting, but rather the correlation between the calculated results and the real scores students earned on the PISA exam, as well as their USE scores (which are publicly available online in aggregated form--i.e., average scores per school). In the preliminary phase, the model learned how to predict the PISA data. In the final model, the calculations were checked against the USE results of high school graduates and university entrants.
Results
First, Smirnov highlighted the general textual features of posts in relation to the academic performance of their authors (Fig. 1). The use of capitalized words (-0.08), emojis (-0.06), and exclamations (-0.04) were found to be negatively correlated with academic performance. The use of the Latin characters, average post and word length, vocabulary size, and entropy of users' texts on the other hand, were found to positively correlate with academic performance (from 0.07 to 0.16, respectively).
It was also confirmed that students with different levels of academic performance have different vocabulary ranges. Smirnov explored the resulting model by selecting 400 words with the highest and lowest scores that appear at least 5 times in the training corpus. Thematic clusters were identified and visualized (Fig. 2).
The clusters with the highest scores (in orange) include:
English words (above, saying, yours, must);Words related to literature (Bradbury, Fahrenheit, Orwell, Huxley, Faulkner, Nabokov, Brodsky, Camus, Mann);Concepts related to reading (read, publish, book, volume);Terms and names related to physics (Universe, quantum, theory, Einstein, Newton, Hawking);Words related to thought processes (thinking, memorizing).
Clusters with low scores (in green) include misspelled words, names of popular computer games, concepts related to military service (army, oath, etc.), horoscope terms (Aries, Sagittarius), and words related to driving and car accidents (collision, traffic police, wheels, tuning).
Smirnov calculated the coefficients for all 2.5 million words of the vector model and made them available for further study. Interestingly, even words that are rarely found in a training dataset can predict academic performance. For example, even if the name 'Newt' (as in the Harry Potter character, Newt Scamander) never appears in the training dataset, the model might assign a higher rating to posts that contain it. This will happen if the model learns that words from novel series are used by high-achieving students, and, through unsupervised learning, 'intuit' that that the name 'Newt' belongs to this category (that is, the word is closely situated to other concepts from Harry Potter in the vector space).
The article Artificial Intelligence Can Predict Students' Educational Outcomes Based On Tweets appeared first on Eurasia Review.

Load-Date: October 24, 2020


End of Document




Knowing The Model You Can Trust: The Key To Better Decision-Making
Eurasia Review
October 25, 2020 Sunday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 367 words
Body


As much of Europe is engulfed by a second wave of Covid-19, and track and trace struggles to meet demand, modelling support tools are being increasingly used by policymakers to make key decisions. Most notably, models have been used to predict the Covid-19 R0 rate - the average rate of secondary infections from a single infection, which has formed the basis for many lockdown decisions across the UK.
Models can represent the most effective tool for identifying interventions that can balance the risks of widespread infection and help assess socio-economic disruption until an effective treatment is established. However, not all models are equal, and differences in model predictions during the Covid-19 pandemic have caused confusion and suspicion.
A recent paper 'Three questions to ask before using model outputs for decision support' published inNature Communicationsaims to help decision makers choose the best available model for the problem at hand. The paper proposes three screening questions that can help critically evaluate models with respect to their purpose, organisation, and evidence, and enable more secure use of models for key decisions by policy makers.
One of the authors of the paper, Dr Alice Johnston, Lecturer in Environmental Data Science at Cranfield University, said: "From Covid-19 to the stock market, models are increasingly used by policymakers to support their decisions.
"However, different models are based on different assumptions and so can produce conflicting results, even when they represent the same system. Models used early on in the Covid-19 pandemic were a prime example of this, which led to confusion over which models to trust to support the decision-making process. This really highlights the need for clear communication of a model's context, so that policymakers have confidence in which models to trust.
"We propose that before engaging with a model, policymakers ask themselves three screening questions focusing on the model's purpose, organisation and evidence base, with the aim of bringing greater clarity to the decision-making process."
The article Knowing The Model You Can Trust: The Key To Better Decision-Making appeared first on Eurasia Review.

Load-Date: October 26, 2020


End of Document




Spain Using Mobile Phone Data To Study Efficacy Of Lockdown On Spread Of COVID-19
Eurasia Review
April 14, 2020 Tuesday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 855 words
Body


A new CSIC [National Scientific Research Council] project uses computer science and data science techniques to observe how the lockdown measures taken to halt the spread of the disease COVID-19 are proving effective. The results will be key to improving social distancing strategies taken in future outbreaks of this disease and of others.
To carry out this research, a multi-disciplinary team with experts in computer science, demographics, physics and movement studies are analyzing anonymous and high resolution big data obtained from telephone operators and map servers. These data explain how mobility patterns and social contact have changed since the start of the lockdown.
How to lift the lockdown and when
Once all the data is gathered, the team simulates different scenarios and strategies for social distancing and helps with decision-making. The results are key both for deciding whether a stricter lockdown should be activated and to plan for the safe and effective lifting of the lockdown.
"We hope that the results serve to better understand the effects of the lockdown on the spread of the disease, but also help in decision-making related to the lifting of the measures, to see whether or not it is better to end the lockdown gradually," said Frederic Bartumeus.
"To achieve this goal the project includes several phases that are being carried out in parallel," said José Javier Ramasco. "Firstly, mobility is characterized, which is being coordinated by the IFISC based on the contribution from various data platforms: information, for example, from online social media and mobility patterns captured from mobile phone records. In this latter case, the data are collected by the operators and companies that are taking part in the project, which provide the research team with aggregated travel flows between different areas" specifies the researcher.
In no case is individual information accessed.
A second aspect is the change in conduct of people due to the perception of risk. The CEAB and IEGD are carrying out surveys and implementing mobile phone applications to quantify these changes, trying to estimate the adherence to personal protection measures by people and the changes in the amount and quality of personal contact.
"This information is crucial for understanding the contagion process," said  José Javier Ramasco.
Lastly, all these data form part of the computational models being developed by the IFISC and IFCA to study the different scenarios to exit the crisis.
"The lockdown has been widespread and relatively sudden, but to avoid new outbreaks it is necessary to use simulators capable of assessing scenarios with different rhythms to return to normality, both by sector and by geographic area," José Javier Ramasco said.
Epidemiology in the future
The project uses artificial intelligence tools and data science, and integrates big data in real time on human mobility, geo-localized surveys and computational models. This is a new way of undertaking epidemiology which combines computational epidemiology, digital demography and human mobility models.
"The study will take into account such important aspects as the spatial distribution of the population, their age structure, and the distribution and characteristics of social health centers (hospitals, local health centers, and care homes for the elderly). We can see how the contention measures have changed the mobility and conduct of people," said José Javier Ramasco.
The information and models to be developed during this research study will be made available to the public for their future use following an open data model under FAIR (Findable, Accessible, Interoperable, Reusable) principles.
A second long-term goal is to establish the basis for a computational epidemiology network in Spain, as in other countries, and a series of interoperable analytical tools based on epidemiological theories, data science and artificial intelligence, to report decisions to be taken in future situations of epidemiological crisis which, as the scientists say, is something that "has already happened on several occasions since 2009 and is likely to be recurrent in our globalized and interconnected world".
The project, pre-financed by the CSIC, thanks to the donation received from AENA, is coordinated by the scientists José Javier Ramasco, from the Institute of Complex Physics System (Spanish acronym: IFISC, a joint CSIC and University of Balearic Islands centre) and Frederic Bartumeus, from the Blanes Advanced Studies Centre (Spanish acronym: CEAB-CSIC) and the CREAF [Centre for Ecological Research and Forestry Applications]. It also involves the participation of teams from the Institute for Economy, Geography and Demography (Spanish acronym: IEGD-CSIC), from the Institute for Physics of Cantabria (Spanish acronym: IFCA-CSIC), from the National Biotechnology Centre (Spanish acronym: CNB-CSIC), as well as scientists from Pompeu Fabra University and the National Epidemiology Centre- Carlos III Health Institute (ISCIII).
The post Spain Using Mobile Phone Data To Study Efficacy Of Lockdown On Spread Of COVID-19 appeared first on Eurasia Review.

Load-Date: April 14, 2020


End of Document




Climate Signals Detected In Global Weather
Eurasia Review
January 2, 2020 Thursday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 765 words
Body


In October this year, weather researchers in Utah measured the lowest temperature ever recorded in the month of October in the US (excluding Alaska): -37.1°C. The previous low-temperature record for October was -35°C, and people wondered what had happened to climate change.
Until now, climate researchers have responded that climate is not the same thing as weather. Climate is what we expect in the long term, whereas weather is what we get in the short term - and since local weather conditions are highly variable, it can be very cold in one location for a short time despite long-term global warming. In short, the variability of local weather masks long-term trends in global climate.
A paradigm shift
Now, however, a group led by ETH professor Reto Knutti has conducted a new analysis of temperature measurements and models. The scientists concluded that the weather-is-not-climate paradigm is no longer applicable in that form. According to the researchers, the climate signal - that is, the long-term warming trend - can actually be discerned in daily weather data, such as surface air temperature and humidity, provided that global spatial patterns are taken into account.
In plain English, this means that - despite global warming - there may well be a record low temperature in October in the US. If it is simultaneously warmer than average in other regions, however, this deviation is almost completely eliminated. "Uncovering the climate change signal in daily weather conditions calls for a global perspective, not a regional one," says Sebastian Sippel, a postdoc working in Knutti's research group and lead author of a study recently published inNature Climate Change.
Statistical learning techniques extract climate change signature
In order to detect the climate signal in daily weather records, Sippel and his colleagues used statistical learning techniques to combine simulations with climate models and data from measuring stations. Statistical learning techniques can extract a "fingerprint" of climate change from the combination of temperatures of various regions and the ratio of expected warming and variability. By systematically evaluating the model simulations, they can identify the climate fingerprint in the global measurement data on any single day since spring 2012.
A comparison of the variability of local and global daily mean temperatures shows why the global perspective is important. Whereas locally measured daily mean temperatures can fluctuate widely (even after the seasonal cycle is removed), global daily mean values show a very narrow range.
If the distribution of global daily mean values from 1951 to 1980 are then compared with those from 2009 to 2018, the two distributions (bell curves) barely overlap. The climate signal is thus prominent in the global values but obscured in the local values, since the distribution of daily mean values overlaps quite considerably in the two periods.
Application to the hydrological cycle
The findings could have broad implications for climate science. "Weather at the global level carries important information about climate," says Knutti. "This information could, for example, be used for further studies that quantify changes in the probability of extreme weather events, such as regional cold spells. These studies are based on model calculations, and our approach could then provide a global context of the climate change fingerprint in observations made during regional cold spells of this kind. This gives rise to new opportunities for the communication of regional weather events against the backdrop of global warming."
The study stems from a collaboration between ETH researchers and the Swiss Data Science Center (SDSC), which ETH Zurich operates jointly with its sister university EPFL. "The current study underlines how useful data science methods are in clarifying environmental questions, and the SDSC is of great use in this," says Knutti.
Data science methods not only allow researchers to demonstrate the strength of the human "fingerprint", they also show where in the world climate change is particularly clear and recognisable at an early stage. This is very important in the hydrological cycle, where there are very large natural fluctuations from day to day and year to year.
"In future, we should therefore be able to pick out human-induced patterns and trends in other more complex measurement parameters, such as precipitation, that are hard to detect using traditional statistics," says the ETH professor.
The post Climate Signals Detected In Global Weather appeared first on Eurasia Review.

Load-Date: January 2, 2020


End of Document




Experts To Create Predictive Tool To Tackle Hate Crime In Los Angeles
Eurasia Review
September 22, 2016 Thursday


Copyright 2016 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 672 words
Body


Cardiff University team awarded over $800,000 by the US Department of Justice to develop real-time predictions of hate crime using Twitter
Experts from Cardiff University are developing a statistical tool that uses social media to make real-time predictions of where hate crimes may occur.
The team, from the University's Social Data Science Lab, will be using Los Angeles County as a test bed for their study, thanks to over $800,000 in funding from the US Department of Justice.
It is the first time that social media has been used in the United States to create predictive policing models of hate crime.
Over the next three years, the team will be closely scrutinizing data taken from Twitter and cross-referencing this with reported hate crimes in Los Angeles to develop markers, or signatures, which could indicate if, and where, a hate crime is likely to take place at a certain point in time, and then enable police officers to intervene.
The term hate crime is used to describe a prejudice-motivated crime, often violent, which occurs when a perpetrator targets a victim because of his or her affiliation to a social group, such as their sex, ethnicity, disability or religion.
According to the US Bureau of Justice Statistics (BJS), in 2012 an estimated 293,800 nonfatal violent and property hate crime victimizations occurred in the United States.
UK official data shows that there were 52,528 hate crimes recorded by the police in England and Wales in 2014/15, an increase of 18 per cent compared with 2013/14.
Previous research from the Social Data Science Lab has already shown that Twitter data can be used to identify hot spots, such as certain states or cities, where hate speech has occurred but where hate crime has not been reported. One example is an area when recent immigrants may be unlikely to report crime due to fear of deportation.
Professor Matt Williams, from the University's School of Social Science, said, "Developing a better understanding of hateful sentiments online and their relationship with crime on the streets could push law enforcement to better identify, report and address hate crimes that are occurring offline.
"The insights provided by our work will help US localities to design policies to address specific hate crime issues unique to their jurisdiction and allow service providers to tailor their services to the needs of victims, especially if those victims are members of an emerging category of hate crime targets."
The Los Angeles Police Department has a history of incorporating progressive and forward-thinking methods into their policing, having previously used mathematical models to predict other areas of crime, which have been shown to successfully lower crime rates.
The huge volumes of data that social media now generates has provided researchers with large swathes of information that can be used to identify emerging patterns and trends in a number of areas across society, including crime.
Dr Pete Burnap, from the University's School of Computer Science and Informatics, said, "This is the first study in the United States to use social media data in predictive policing models of hate crime. Predictive policing is a proactive law enforcement model that has become more common partially due to the advent of advanced analytics such as data mining and machine-learning methods.
"New analytic approaches and the ability to process very large data sets have increased the accuracy of predictive models over traditional crime analysis methods and this project will evaluate if police departments can leverage these new data and techniques to reduce hate crimes."
Cardiff University's Social Data Science Lab forms part of the Data Innovation Research Institute and has been involved in research grants amounting to more than £6 million. The Social Data Science Lab brings together social, computer, political, health, statistical and mathematical scientists to study the methodological, theoretical, empirical and technical dimensions of New Forms of Data in social and policy contexts.

Load-Date: September 22, 2016


End of Document




Police Stop Fewer Black Drivers At Night When 'Veil Of Darkness' Obscures Their Race
Eurasia Review
May 6, 2020 Wednesday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 883 words
Body


The largest-ever study of alleged racial profiling during traffic stops has found that blacks, who are pulled over more frequently than whites by day, are much less likely to be stopped after sunset, when "a veil of darkness" masks their race.
That is one of several examples of systematic bias that emerged from a five-year study that analyzed 95 million traffic stop records, filed by officers with 21 state patrol agencies and 35 municipal police forces from 2011 to 2018.
The Stanford-led study also found that when drivers were pulled over, officers searched the cars of blacks and Hispanics more often than whites. The researchers also examined a subset of data from Washington and Colorado, two states that legalized marijuana, and found that while this change resulted in fewer searches overall, and thus fewer searches of blacks and Hispanics, minorities were still more likely than whites to have their cars searched after a pull-over.
"Our results indicate that police stops and search decisions suffer from persistent racial bias, and point to the value of policy interventions to mitigate these disparities," the researchers write in Nature Human Behaviour.
The paper culminates a five-year collaboration between Stanford's Cheryl Phillips, a journalism lecturer whose graduate students obtained the raw data through public records requests, and Sharad Goel, a professor of management science and engineering whose computer science team organized and analyzed the data.
Goel and his collaborators, which included Ravi Shroff, a professor of applied statistics at New York University, spent years culling through the data, eliminating records that were incomplete or from the wrong time periods, to create the 95 million-record database that was the basis for their analysis. "There is no way to overstate the difficulty of that task," Goel said.
Creating that database enabled the team to find the statistical evidence that a "veil of darkness" partially immunized blacks against traffic stops. That term and idea has been around since 2006 when it was used in a study that compared the race of 8,000 drivers in Oakland, California, who were stopped at any time of day or night over a six month period. But the findings from that study were inconclusive because the sample was too small to prove a link between the darkness of the sky and the race of the stopped drivers.
The Stanford team decided to repeat the analysis using the much larger dataset that they had gathered. First, they narrowed the range of variables they had to analyze by choosing a specific time of day - around 7 p.m. - when the probable causes for a stop were more or less constant. Next, they took advantage of the fact that, in the months before and after daylight saving time each year, the sky gets a little darker or lighter, day by day. Because they had such a massive database, the researchers were able to find 113,000 traffic stops, from all of the locations in their database, that occurred on those days, before or after clocks sprang forward or fell back, when the sky was growing darker or lighter at around 7 p.m. local time.
This dataset provided a statistically valid sample with two important variables - the race of the driver being stopped, and the darkness of the sky at around 7 p.m. The analysis left no doubt that the darker it got, the less likely it became that a black driver would be stopped. The reverse was true when the sky was lighter.
More than any single finding, the collaboration's most lasting impact may be from the Stanford Open Policing Project, which the researchers started to make their data available to investigative and data-savvy reporters, and to hold workshops to help reporters learn how to use the data to do local stories.
For example, the researchers helped reporters at the Seattle-based non-profit news organization, Investigate West, understand the patterns in the data for stories showing bias in police searches of Native Americans. That reporting prompted the Washington State Patrol to review its practices and boost officer training. Similarly, the researchers helped reporters at the Los Angeles Times analyze data that showed how police searched minority drivers far more often than whites. It resulted in a story that was part of a larger investigative series that prompted changes in Los Angeles Police Department practices.
"All told we've trained about 200 journalists, which is one of the unique things about this project," Phillips said.
Goel and Phillips plan to continue collaborating through a project called Big Local News that will explore how data science can shed light on public issues, such as civil asset forfeitures - instances in which law enforcement is authorized to seize and sell property associated with a crime. Gathering and analyzing records of when and where such seizures occur, to whom, and how such property is disposed will help shed light on how this practice is being used. Big Local News is also working on collaborative efforts to standardize information from police disciplinary cases.
"These projects demonstrate the power of combining data science with journalism to tell important stories," Goel said.
The post Police Stop Fewer Black Drivers At Night When 'Veil Of Darkness' Obscures Their Race appeared first on Eurasia Review.

Load-Date: May 6, 2020


End of Document




Model Beats Wall Street Analysts In Forecasting Business Financials
Eurasia Review
December 20, 2019 Friday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1237 words
Body


Knowing a company's true sales can help determine its value. Investors, for instance, often employ financial analysts to predict a company's upcoming earnings using various public data, computational tools, and their own intuition. Now MIT researchers have developed an automated model that significantly outperforms humans in predicting business sales using very limited, "noisy" data.
In finance, there's growing interest in using imprecise but frequently generated consumer data - called "alternative data" - to help predict a company's earnings for trading and investment purposes. Alternative data can comprise credit card purchases, location data from smartphones, or even satellite images showing how many cars are parked in a retailer's lot. Combining alternative data with more traditional but infrequent ground-truth financial data - such as quarterly earnings, press releases, and stock prices - can paint a clearer picture of a company's financial health on even a daily or weekly basis.
But, so far, it's been very difficult to get accurate, frequent estimates using alternative data. In a paper published this week in the Proceedings of ACM Sigmetrics Conference, the researchers describe a model for forecasting financials that uses only anonymized weekly credit card transactions and three-month earning reports.
Tasked with predicting quarterly earnings of more than 30 companies, the model outperformed the combined estimates of expert Wall Street analysts on 57 percent of predictions. Notably, the analysts had access to any available private or public data and other machine-learning models, while the researchers' model used a very small dataset of the two data types.
"Alternative data are these weird, proxy signals to help track the underlying financials of a company," says first author Michael Fleder, a postdoc in the Laboratory for Information and Decision Systems (LIDS). "We asked, 'Can you combine these noisy signals with quarterly numbers to estimate the true financials of a company at high frequencies?' Turns out the answer is yes."
The model could give an edge to investors, traders, or companies looking to frequently compare their sales with competitors. Beyond finance, the model could help social and political scientists, for example, to study aggregated, anonymous data on public behavior. "It'll be useful for anyone who wants to figure out what people are doing," Fleder says.
Joining Fleder on the paper is EECS Professor Devavrat Shah, who is the director of MIT's Statistics and Data Science Center, a member of the Laboratory for Information and Decision Systems, a principal investigator for the MIT Institute for Foundations of Data Science, and an adjunct professor at the Tata Institute of Fundamental Research.
Tackling the "small data" problem
For better or worse, a lot of consumer data is up for sale. Retailers, for instance, can buy credit card transactions or location data to see how many people are shopping at a competitor. Advertisers can use the data to see how their advertisements are impacting sales. But getting those answers still primarily relies on humans. No machine-learning model has been able to adequately crunch the numbers.
Counterintuitively, the problem is actually lack of data. Each financial input, such as a quarterly report or weekly credit card total, is only one number. Quarterly reports over two years total only eight data points. Credit card data for, say, every week over the same period is only roughly another 100 "noisy" data points, meaning they contain potentially uninterpretable information.
"We have a 'small data' problem," Fleder says. "You only get a tiny slice of what people are spending and you have to extrapolate and infer what's really going on from that fraction of data."
For their work, the researchers obtained consumer credit card transactions - at typically weekly and biweekly intervals - and quarterly reports for 34 retailers from 2015 to 2018 from a hedge fund. Across all companies, they gathered 306 quarters-worth of data in total.
Computing daily sales is fairly simple in concept. The model assumes a company's daily sales remain similar, only slightly decreasing or increasing from one day to the next. Mathematically, that means sales values for consecutive days are multiplied by some constant value plus some statistical noise value - which captures some of the inherent randomness in a company's sales. Tomorrow's sales, for instance, equal today's sales multiplied by, say, 0.998 or 1.01, plus the estimated number for noise.
If given accurate model parameters for the daily constant and noise level, a standard inference algorithm can calculate that equation to output an accurate forecast of daily sales. But the trick is calculating those parameters.
Untangling the numbers
That's where quarterly reports and probability techniques come in handy. In a simple world, a quarterly report could be divided by, say, 90 days to calculate the daily sales (implying sales are roughly constant day-to-day). In reality, sales vary from day to day. Also, including alternative data to help understand how sales vary over a quarter complicates matters: Apart from being noisy, purchased credit card data always consist of some indeterminate fraction of the total sales. All that makes it very difficult to know how exactly the credit card totals factor into the overall sales estimate.
"That requires a bit of untangling the numbers," Fleder says. "If we observe 1 percent of a company's weekly sales through credit card transactions, how do we know it's 1 percent? And, if the credit card data is noisy, how do you know how noisy it is? We don't have access to the ground truth for daily or weekly sales totals. But the quarterly aggregates help us reason about those totals."
To do so, the researchers use a variation of the standard inference algorithm, called Kalman filtering or Belief Propagation, which has been used in various technologies from space shuttles to smartphone GPS. Kalman filtering uses data measurements observed over time, containing noise inaccuracies, to generate a probability distribution for unknown variables over a designated timeframe. In the researchers' work, that means estimating the possible sales of a single day.
To train the model, the technique first breaks down quarterly sales into a set number of measured days, say 90 - allowing sales to vary day-to-day. Then, it matches the observed, noisy credit card data to unknown daily sales. Using the quarterly numbers and some extrapolation, it estimates the fraction of total sales the credit card data likely represents. Then, it calculates each day's fraction of observed sales, noise level, and an error estimate for how well it made its predictions.
The inference algorithm plugs all those values into the formula to predict daily sales totals. Then, it can sum those totals to get weekly, monthly, or quarterly numbers. Across all 34 companies, the model beat a consensus benchmark - which combines estimates of Wall Street analysts - on 57.2 percent of 306 quarterly predictions.
Next, the researchers are designing the model to analyze a combination of credit card transactions and other alternative data, such as location information. "This isn't all we can do. This is just a natural starting point," Fleder says.
The post Model Beats Wall Street Analysts In Forecasting Business Financials appeared first on Eurasia Review.

Load-Date: December 20, 2019


End of Document




March Madness Bracket Analysis Shows Picking Final Four First Leads To Better Brackets
Eurasia Review
March 5, 2020 Thursday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 619 words
Body


Data science researchers at the University of Illinois have some March Madness advice based on new research: Pick top-seeded teams as the Final Four in your March Madness bracket and work backward and forward from there. If you are going to submit multiple brackets-as you can in the ESPN, CBS Sports and Yahoo Challenges-starting with the Final Four is still a good strategy, but make sure you also diversify your brackets as much as possible.
A paper describing the research behind this advice is published in the American Statistical Association's (ASA)Journal of Quantitative Analysis in Sports(JQAS) by Sheldon H. Jacobson (computer science faculty), Ian Ludden (computer science graduate student), Arash Khatibi (former graduate student) and Douglas M. King (industrial and enterprise systems engineering faculty).
"If you can only pick one bracket, then leaning heavily on the top seeds makes sense," said Jacobson. "However, all bracket challenges allow you to submit multiple entries. A person does not need all of their brackets to score well; just one will do." Jacobson's research on basketball brackets over the past decade has focused entirely on seeds, not teams, making his body of seed-centered work distinct.
Given there are 2^63 possible brackets, which is more than 9 quintillion (9 x 10^18) combinations, picking a bracket with all 63 games correct is highly unlikely, even if you can submit multiple brackets. So, Jacobson suggests focusing on your Final Four teams first, and then building backward and forward from those games. "Once you pick a set of Final Four teams, 12 additional game outcomes become fixed, effectively reducing the number of games that you must pick," Jacobson said. "Our research suggests that anything that can be done to reduce the uncertainty in your picks, while simultaneously expanding the diversity of your pool, will give you a step up in having a good scoring bracket amongst your set of brackets." More information can be found on Jacobson's Bracket Odds website athttp://bracketodds.cs.illinois.edu/pool.html.
"For the 2016 through 2019 tournaments, our models produce many brackets that would have placed in the top 100 of the ESPN bracket challenge." Ludden said. "Our models that start by picking the Elite Eight or Final Four teams perform especially well, perhaps because they balance the two main risks: incorrect picks in the first two rounds, which may propagate through the tournament, and incorrect teams in the later rounds, where each game is worth more points."
Jacobson's seed-centered research has been integrated into the Bracket Odds website. Launched in 2012, the website-labeled as a University of Illinois STEM Learning Laboratory-draws together graduate and undergraduate students to apply data science methods to the tournament. The site has attracted more than 650,000 hits since its inception, providing insights and information for those interested in the mathematics of March Madness.
The website offers a smorgasbord of data analytics for people following the tournament. For example, one of the website calculators gives the probability of all number-one seeds reaching the Final Four to be 0.0155, or around once every 64 tournaments. Meanwhile, the probability of a Final Four comprised of only No. 16 seeds-the lowest-seeded teams in the tournament-is so small that it has a frequency of happening once every 13 trillion tournaments. (For perspective, if an entire tournament was played once every second, the lowest-seeded teams would only meet in the Final Four approximately once every 433,000 years.)
The post March Madness Bracket Analysis Shows Picking Final Four First Leads To Better Brackets appeared first on Eurasia Review.

Load-Date: March 5, 2020


End of Document




Influencing Electoral Outcomes: The Ugly Face Of Facebook - Analysis
Eurasia Review
March 27, 2018 Tuesday


Copyright 2018 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1574 words
Body


By Munish Sharma*
Free and fair elections are the backbone of a democratic system of governance, and they are often celebrated as the "festival of democracy". Election campaigns of political parties and candidates employ a wide variety of strategies and tactics to influence voters. The digital era has added a whole new flavour, be it the eye-catching colossal digital campaigns or instances of foreign governments interfering in the electoral process. Last year, the Presidential elections in both the US and France were controversial due to hacking incidents and data leaks at the campaigns of the Democratic National Committee (DNC) and En Marche, respectively. In general, cyber means of intervention appear to be becoming an inevitable part of the electoral process. The Cambridge Analytica incident proves that India is no exception to this trend. During the next general elections in 2019, the Election Commission of India has an uphill task to thwart both external interference and the abuse of social media platforms to influence voter behaviour.
While there is a long history of external interference in elections both through covert and overt means, digital platforms add a new dimension. News and online content over digital platforms can spread at lightning speed, without paying heed to the credibility or authenticity of the source. Moreover, social media platforms generate vast amounts of data related to the socio-economic conditions, purchasing behaviour, interests, hobbies, and political inclinations or orientations of the users. These details are captured and treasured for commercial purposes. Business analytics feed on this data to generate business intelligence and derive monetary benefits for informed decision making. Present day electoral campaigns are also data driven and they are well-funded to let the campaigners harness data for their own political advantage. Data analytics tools can harvest data from user profiles and sift through the trove to support research, augment targeted campaigns and help political parties in assessing and evaluating their performance. These have been quite effective in targeting swing voters and behaviour forecasting.
As the popularity of social media platforms hits new heights, Facebook and Twitter in particular have been under the scanner of both intelligence agencies and election watchdogs. With close to 2.2 billion active users (by the end of 2017), Facebook alone sits on a stockpile of data which could be used to drive election campaigns towards any preferred outcome. Data in itself is worthless, but data science and the corresponding analytical tools turn it into a goldmine for both businesses and political strategists in the digital age.
Cambridge Analytica, the London-based political consultancy firm presently under the scanner, has an eight-year-old association with Indian elections. It undertook an in-depth electorate analysis for the Bihar Assembly Election in 2010 and, as per the case study details on its website, "the client (political party) achieved a landslide victory, with over 90 percent of total seats targeted by Cambridge Analytica being won."1 This was carried out through Ovleno Business Intelligence, which is an Indian affiliate of Cambridge Analytica's parent firm Strategic Communications Laboratories. The firm had hit media headlines for its association with Donald Trump's election campaign, which it has referred to as "A Full-Scale Data-Driven Digital Campaign". Bringing together the expertise of data scientists, researchers, strategists and content writers in three integrated teams (research, data science, and digital marketing), Cambridge Analytica's campaign helped Trump win the elections.2 The above case studies, mentioned in the Cambridge Analytica website, are prime examples of the vital role data science has begun to play especially in devising techniques to change voter behaviour in the targeted population or audience.
Facebook has played a central role in this entire episode. In a statement, Facebook has accepted that in 2015 a research app for psychologists with the name "thisisyourdigitallife", developed by a psychology professor at Cambridge University, was used for commercial purposes by Cambridge Analytica and other firms in violation of its platform policies. The app, meant for personality prediction, had around 270,000 downloads. Users revealed content related to their likes, preferences, and their own social circles according to their privacy settings.3 The access to Facebook content, in technical terms, was legitimate and through proper channels but the information was passed on to third parties likes Cambridge Analytica and Eunoia Technologies, which exploited it for commercial gains. However, Cambridge Analytica has outright denied allegations of using Facebook data as part of the services rendered to the Trump presidential campaign and while working on the Brexit referendum in the UK.4
As of January 2018, with 250 million users, India is the largest user-base for Facebook. It is also an important tool for the government to take forward its flagship programmes to the wider populace. Facebook is one of the top contenders for partnering with the government's societal development and digital inclusion plans. The Election Commission of India had also partnered with Facebook in 2017, launching a nationwide voter registration campaign.5 Indian users, paying little regard to the privacy terms and condition of social media platforms, uninhibitedly share images, pictures and other content, and are extremely vulnerable to the tools, techniques and campaigns devised for influencing both commercial and political behaviour. Against this backdrop, the government's concerns have been raised by Cambridge Analytica's alleged mining of data from the profiles of 50 million US Facebook users without their consent.6 If such an incident were to occur in India, it would constitute a serious violation of the IT Act. Not just in India, Cambridge Analytica is also at loggerheads with the Electoral Commission in the UK over its alleged role in the BREXIT vote and in Europe for violating EU privacy laws in collusion with Facebook.
Although Facebook has tendered an assurance of data security on its platform for the upcoming elections in India (2019) and Brazil (October 2018), the incident has caused severe damage to its reputation even as a development partner for governments in digital inclusion or other societal benefits plans. As the stakes in elections go up, political parties are unlikely to shy away from leveraging the technical expertise of data analytical firms like Cambridge Analytica fed with expansive data sets harvested from prominent social media platforms.
Data is being extensively harvested and harnessed for commercial purposes, targeted marketing campaigns and to influence consumer choices. It is ethically and legally controversial when information derived without the consent of the users or through dubious means is leveraged to influence political choices. Flourishing in the void of effective legal and regulatory regimes, such incidents seriously undermine the trust of people in the democratic process. To an extent, users understanding the perils of sharing unwanted details or content on social media platforms and aware of their privacy settings is pertinent for containing such instances of abuse. For India, as a functioning democracy, the Cambridge Analytica episode highlights the need to expedite the process of developing a data protection framework and probably amend the IT Act in accordance with the changing realities of cyberspace. The earlier this is realised, the better it would be for the healthy functioning of our democratic systems and processes.
Views expressed are of the author and do not necessarily reflect the views of the IDSA or of the Government of India.
About the author:

*Munish Sharma is Consultant at the Institute for Defence Studies and Analyses, New Delhi.
Source:

This article was published by IDSA.
Notes:
1. "Case Studies - India", Cambridge Analytica Political, available at https://ca-political.com /casestudies/casestudyindia, accessed 23 March, 2018.
2. "Case Studies – Donald J. Trump for President", Cambridge Analytica Political, available at            https://ca-political.com /casestudies, accessed 23 March, 2018.
3. Paul Grewal, "Suspending Cambridge Analytica and SCL Group from Facebook", Facebook Newsroom, March 16, 2018, available at            https://newsroom.fb.com /news/2018/03/suspending-cambridge-analytica/, accessed 25 March, 2018.
4. Cambridge Analytica, "Cambridge Analytica responds to false allegations in the media", available at            https://ca-commercial.com /news/cambridge-analytica-responds-false-allegations-media, accessed 25 March, 2018.
5. "Election Commission of India partners with Facebook to launch first nationwide voter registration reminder", Facebook, June 27, 2017, available at            https://www.facebook.com /notes/facebook/election-commission-of-india-partners-with-facebook-to-launch-first-nationwide-v/1672618862758040/, accessed 23 March, 2018.
6. Prashant Jha, "In eye of Facebook 'data breach' storm, Cambridge Analytica in talks with Cong, BJP for 2019", Hindustan Times, March 19, 2018, available at            https://www.hindustantimes.com /india-news/suspended-by-facebook-over-data-breach-cambridge-analytica-in-talks-with-congress-bjp-for-2019/story-g7J9rodV24lgCK7Xqu5eaO.html, accessed 23 March, 2018.

Load-Date: March 27, 2018


End of Document




How Climate Change Affects Crops In India
Eurasia Review
June 18, 2019 Tuesday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 602 words
Body


Kyle Davis is an environmental data scientist whose research seeks to
increase food supplies in developing countries. He combines techniques 
from environmental science and data science to understand patterns in 
the global food system and develop strategies that make food-supply 
chains more nutritious and sustainable.
Since joining the Data Science Institute as a postdoctoral fellow in  September 2018, Davis has co-authored four papers, all of which detail  how developing countries can sustainably improve their crop production.  For his latest study, he focuses on India, home to 1.3 billion people,  where he led a team that studied the effects of climate on five major  crops: finger millet, maize, pearl millet, sorghum and rice.
These crops  make up the vast majority of grain production during the  June-to-September monsoon season - India's main growing period - with  rice contributing three-quarters of the grain supply for the season.  Taken together, the five grains are essential for meeting India's  nutritional needs.
And in a paper published in Environmental Research Letters,  Davis found that the yields from grains such as millet, sorghum, and  maize are more resilient to extreme weather; their yields vary  significantly less due to year-to-year changes in climate and generally  experience smaller declines during droughts. But yields from rice,  India's main crop, experience larger declines during extreme weather  conditions.
"By relying more and more on a single crop - rice - India's  food supply is potentially vulnerable to the effects of varying  climate," said Davis, the lead author on the paper, "Sensitivity of  Grain Yields to Historical Climate Sensitivity in India," which has four  co-authors, all of whom collaborated on the research.
"Expanding the area planted with these four alternative grains can 
reduce variations in Indian grain production caused by extreme climate, 
especially in the many places where their yields are comparable to 
rice," Davis added. "Doing so will mean that the food supply for the 
country's massive and growing population is less in jeopardy during 
times of drought or extreme weather."
Temperatures and rainfall amounts in India vary from year to year 
and influence the amount of crops that farmers can produce. And with 
episodes of extreme climate such as droughts and storms becoming more 
frequent, it's essential to find ways to protect India's crop production
from these shocks, according to Davis.
The authors combined historical data on crop yields, temperature,  and rainfall. Data on the yields of each crop came from state  agricultural ministries across India and covered 46 years (1966-2011)  and 593 of India's 707 districts.
The authors also used modelled data on  temperature (from the University of East Anglia's Climate Research  Unit) and precipitation (derived from a network of rain gauges  maintained by the Indian Meteorological Department). Using these climate  variables as predictors of yield, they then employed a linear mixed  effects modelling approach - similar to a multiple regression - to  estimate whether there was a significant relationship between  year-to-year variations in climate and crop yields.
"This study shows that diversifying the crops that a country grows 
can be an effective way to adapt its food-production systems to the 
growing influence of climate change," said Davis. "And it adds to the 
evidence that increasing the production of alternative grains in India 
can offer benefits for improving nutrition, for saving water, and for 
reducing energy demand and greenhouse gas emissions from agriculture."

Load-Date: June 18, 2019


End of Document




Facebook, Cambridge Analytica And Surveillance Capitalism - OpEd
Eurasia Review
March 23, 2018 Friday


Copyright 2018 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1429 words
Body


Whether it creeps into politics, marketing, or simple profiling, the nature of surveillance as totality has been affirmed by certain events this decade.  The Edward Snowden disclosures of 2013 demonstrated the complicity and collusion between Silicon Valley and the technological stewards of the national security state.
It took the election of Donald J. Trump in 2016 to move the issue of social media profiling, sharing and targeting of information, to another level.  Not only could companies such as Facebook monetise their user base; those details could, in turn, be plundered, mined and exploited for political purpose.
As a social phenomenon, Facebook could not help but become a juggernaut inimical to the private sphere it has so comprehensively colonised.  "Facebook in particular," claimed WikiLeaks' Julian Assange in May 2011, "is the most appalling spy machine that has ever been invented." It furnished "the world's most comprehensive database about people, their relationships, their names, their addresses, their locations, their communications with each other, and their relatives, all sitting within the United States, all accessible to US intelligence."
Now, the unsurprising role played by Cambridge Analytica with its Facebook accessory to politicise and monetise data reveals the tenuous ground notions of privacy rest upon.  Outrage and uproar has been registered, much of it to do with a simple fact: data was used to manipulate, massage and deliver a result to Trump – or so goes the presumption.  An instructive lesson here would be to run the counter-factual: had Hillary Clinton won, would this seething discontent be quite so enthusiastic?
Be that as it may, the spoliations of Cambridge Analytica are embedded in a broader undertaking: the evisceration of privacy, and the generation of user profiles gathered through modern humanity's most remarkable surveillance machine.  The clincher here is the link with Facebook, though the company insists that it "received data from a contractor, which we deleted after Facebook told us the contractor had breached their terms of service."
Both Facebook and Cambridge Analytica have attempted to isolate and distance that particular contractor, a certain Aleksandr Kogan, the Cambridge University researcher whose personality quiz app "thisisyourdigitallife" farmed the personal data of some 50 million users who were then micro-targeted for reasons of political advertising.
The sinister genius behind this was the ballooning from the initial downloads – some 270,000 people – who exchanged personal data on their friends including their "likes" for personality predictions.  A broader data set of profiles were thereby created and quarried.
Kogan claims to have been approached by Cambridge Analytica, rather than the other way around, regarding "terms of usage of Facebook data".  He was also reassured that the scheme was legal, being "commercial" in nature and typical of the way "tens of thousands of apps" were using social media data.  But it took Cambridge Analytica's whistleblower, Christopher Wylie, to reveal that data obtained via Kogan's app was, in fact, used for micro-targeting the US electorate in breach of privacy protocols.
Mark Zuckerberg's response has entailed vigorous hand washing.  In 2015, he claims that Facebook had learned that Cambridge Analytica shared data from Kogan's app.  "It is against our policies for developers to share data without other people's consent, so we immediately banned Kogan's app from our platform". Certifications were duly provided that such data had been deleted, though the crew at Facebook evidently took these at unverified face value.  Not so, as matters transpired, leading to the claim that trust had not only been breached between Facebook, Kogan and Cambridge Analytica, but with the users themselves.
Facebook, for its part, has been modestly contrite.  "We have a responsibility to protect your data," went Zuckerberg in a statement, "and if we can't then we don't deserve to serve you."  His posted statement attempts to water down the fuss.  Data protections – most of them, at least – were already being put in place. He described the limitations placed on the accessing of user information by data apps connected to Facebook friends.
The networked sphere, as it is termed in with jargon-heavy fondness by some academics, has seen the accumulation of data all set and readied for the "information civilisation".  Google's chief economist Hal Varian has been singled out for special interest, keen on what he terms, in truly benign fashion, "computer-mediated transactions".  These entail "data extraction and analysis," various "new contractual forms" arising from "better monitoring", "personalisation and customisation" and "continuous experiments".
Such are the vagaries of the information age. As a user of such freely provided services, users are before a naked confessional, conceding and surrendering identities to third parties with Faustian ease.  This surrender has its invidious by products, supplying intelligence and security services accessible data.
Cambridge Analytica, for its part, sets itself up as an apotheosis of the information civilisation, a benevolent, professionally driven information hitman. "Data drives all we do," it boldly states to potential clients.  "Cambridge Analytica uses data to change audience behaviour."
This sounds rather different to the company's stance on Saturday, when it claimed that, "Advertising is not coercive; people are smarter than that."  With cold show insistence, it insisted that, "This isn't a spy movie."
Two services are provided suggesting that people are not, in the minds of its bewitchers, that intelligent: the arm of data-driven marketing designed to "improve your brand's marketing effectiveness by changing consumer behaviour" and that of "data-driven campaigns" where "greater influence" is attained through "knowing your electorate better".
On the latter, it is boastful, claiming to have supported over 100 campaigns across five continents. "Within the United States alone, we have played a pivotal role in winning presidential races as well as congressional and state elections."
CA has donned its combat fatigues to battle critics.  Its Board of Directors has suspended CEO Alexander Nix, claiming that "recent comments secretly recorded by Channel 4 and other allegations do not represent the values or operations of the firm and his suspension reflects the seriousness with which we view this violation."
The comments in question, caught in an undercover video, show Nix offering a range of services to the Channel 4 undercover reporter: Ukrainian sex workers posing as "honey-traps" a video evidencing corruption that might be uploaded to the Internet; and operations with former spies. "We can set up fake IDs and Web sites, we can be students doing research projects attached to a university; we can be tourists."
The company has also attempted to debunk a set of what it sees as flourishing myths.  It has not, for instance, been uncooperative with the UK's data regulator, the Information Commissioner's Office, having engaged it since February 2017.  It rejects notions that it peddles fake news. "Fake news is a serious concern for all of us in the marketing industry."  (Nix's cavalier advertising to prospective clients suggests otherwise.)
In other respects, Cambridge Analytica also rejected using Facebook data in its political models, despite having obtained that same data.  "We ran a standard political data science program with the same kind of political preference models used by other presidential campaigns."  Nor did it use personality profiles for the 2016 US Presidential election. Having only hopped on board in June, "we focused on the core elements of a core political data science program."
The company's weasel wording has certainly been extensive.  Nix has done much to meander, dodge and contradict.  On the one hand, he would like to take credit for the company's product – the swaying of a US election.  But in doing so, it did not use "psychographic" profiles.
Surveillance capitalism is the rope which binds the actors of this latest drama in the annals of privacy's demise.  There are discussions that political data mining designed to manipulate and sway elections be considered in the same way political donations are.  But in the US, where money and political information are oft confused as matters of freedom, movement on this will be slow.  The likes of Cambridge Analytica and similar information mercenaries will continue thriving.

Load-Date: March 23, 2018


End of Document




Surface Clean-Up Technology Won't Solve Ocean Plastic Problem
Eurasia Review
August 4, 2020 Tuesday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 717 words
Body


Clean-up devices that collect waste from the ocean surface won't solve the plastic pollution problem, a new study shows.
Researchers compared estimates of current and future plastic waste with the ability of floating clean-up devices to collect it - and found the impact of such devices was "very modest". However, river barriers could be more effective and - though they have no impact on plastic already in the oceans - they could reduce pollution "significantly" if used in tandem with surface clean-up technology.
The study - by the University of Exeter, the Leibniz Centre for Tropical Marine Research, the Leibniz Institute for Zoo and Wildlife Research, Jacobs University and Making Oceans Plastic Free - focusses on floating plastic, as sunk waste is difficult or impossible to remove depending on size and location.
The authors estimate that the amount of plastic reaching the ocean will peak in 2029, and surface plastic will hit more than 860,000 metric tonnes - more than double the current estimated 399,000 - by 2052 (when previous research suggested the rate of plastic pollution may finally reach zero).
"The important message of this paper is that we can't keep polluting the oceans and hoping that technology will tidy up the mess," said Dr Jesse F. Abrams, of the Global Systems Institute and the Institute for Data Science and Artificial Intelligence, both at the University of Exeter.
"Even if we could collect all the plastic in the oceans - which we can't - it's really difficult to recycle, especially if plastic fragments have floated for a long time and been degraded or bio-fouled.
"The other major solutions are to bury or burn it - but burying could contaminate the ground and burning leads to extra CO2 emissions to the atmosphere."
Private initiatives proposing to collect plastic from oceans and rivers have gained widespread attention recently.
One such scheme, called the Ocean Cleanup, aims to clean the "Pacific garbage patch" in the next 20 years using 600m floating barriers to collect plastic for recycling or incineration on land.
The new study analysed the impact of deploying 200 such devices, running without downtime for 130 years - from 2020 to 2150.
In this scenario, global floating plastic debris would be reduced by 44,900 metric tonnes - just over 5% of the estimated global total by the end of that period.
"The projected impact of both single and multiple clean up devices is very modest compared to the amount of plastic that is constantly entering the ocean," said Dr Sönke Hohn, of Leibniz Centre for Tropical Marine Research.
"These devices are also relatively expensive to make and maintain per unit of plastic removed."
As most plastic enters the oceans via rivers, the authors say a "complete halt" of such pollution entering the ocean using river barriers - especially in key polluting rivers - could prevent most of the pollution they otherwise predict over the next three decades.
However, due to the importance of large rivers for global shipping, such barriers are unlikely to be installed on a large scale.
Given the difficulty of recycling and the negative impacts of burying or burning plastic, the study says reducing disposal and increasing recycling rates are essential to tackle ocean pollution. "Plastic is an extremely versatile material with a wide range of consumer and industrial applications, but we need to look for more sustainable alternatives and rethink the way we produce, consume and dispose of plastic," said Professor Agostino Merico, of Leibniz Centre for Tropical Marine Research and Jacobs University.
Dr Roger Spranz, an author of the study, is a co-founder of non-profit organisation Making Oceans Plastic Free.
"We have developed expertise in changing behaviour to break plastic habits and stop plastic pollution at its source," Dr Spranz said.
"We are registered in Germany but the focus of our activities and collaborations is in Indonesia, the second-largest source of marine plastic pollution.
"Working with local partners, the implementation of our Tasini campaign in Indonesia has to date helped to prevent an estimated 20 million plastic bags and 50,000 plastic bottles from ending up in coastal areas and the ocean."
The article Surface Clean-Up Technology Won't Solve Ocean Plastic Problem appeared first on Eurasia Review.

Load-Date: August 5, 2020


End of Document




More Evidence Of Causal Link Between Air Pollution And Early Death
Eurasia Review
June 29, 2020 Monday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 579 words
Body


Strengthening U.S. air quality standards for fine particulate pollution to be in compliance with current World Health Association (WHO) guidelines could save more than 140,000 lives over the course of a decade, according to a new study from Harvard T.H. Chan School of Public Health.
The study, published June 26, 2020 inSciences Advances, provides the most comprehensive evidence to date of the causal link between long-term exposure to fine particulate (PM2.5) air pollution and premature death, according to the authors.
"Our new study included the largest-ever dataset of older Americans and used multiple analytical methods, including statistical methods for causal inference, to show that current U.S. standards for PM2.5 concentrations are not protective enough and should be lowered to ensure that vulnerable populations, such as the elderly, are safe," said doctoral student Xiao Wu, a co-author of the study.
The new research builds on a 2017 study that showed that long-term exposure to PM2.5 pollution and ozone, even at levels below current U.S. air quality standards, increases the risk of premature death among the elderly in the U.S.
For the new study, researchers looked at 16 years' worth of data from 68.5 million Medicare enrollees--97% of Americans over the age of 65--adjusting for factors such as body mass index, smoking, ethnicity, income, and education. They matched participants' zip codes with air pollution data gathered from locations across the U.S. In estimating daily levels of PM2.5 air pollution for each zip code, the researchers also took into account satellite data, land-use information, weather variables, and other factors. They used two traditional statistical approaches as well as three state-of-the-art approaches aimed at teasing out cause and effect.
Results were consistent across all five different types of analyses, offering what authors called "the most robust and reproducible evidence to date" on the causal link between exposure to PM2.5 and mortality among Medicare enrollees--even at levels below the current U.S. air quality standard of 12 ?g/m3 (12 micrograms per cubic meter) per year.
The authors found that an annual decrease of 10 ?g/m3 in PM2.5 pollution would lead to a 6%-7% decrease in mortality risk. Based on that finding, they estimated that if the U.S. lowered its annual PM2.5 standard to 10 ?g/m3--the WHO annual guideline--143,257 lives would be saved in one decade.
The authors included additional analyses focused on causation, which address criticisms that traditional analytical methods are not sufficient to inform revisions of national air quality standards. The new analyses enabled the researchers, in effect, to mimic a randomized study--considered the gold standard in assessing causality--thereby strengthening the finding of a link between air pollution and early death.
"The Environmental Protection Agency has proposed retaining current national air quality standards. But, as our new analysis shows, the current standards aren't protective enough, and strengthening them could save thousands of lives. With the public comment period for the EPA proposal ending on June 29, we hope our results can inform policymakers' decisions about potentially updating the standards," said co-author Francesca Dominici, Clarence James Gamble Professor of Biostatistics, Population, and Data Science.
The article More Evidence Of Causal Link Between Air Pollution And Early Death appeared first on Eurasia Review.

Load-Date: June 29, 2020


End of Document




There's No End In Sight To The Zombie Economy – OpEd
Eurasia Review
June 2, 2020 Tuesday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 988 words
Body


By Andrew Moran*
The United States was waiting for the zombie apocalypse. The country was given a coronapocalypse instead. But could the two events merge and provide the nation with a dangerous economic trend? Corporate America's worst-kept secret had been the swelling number of zombies kept on life support and hidden away during the boom phase of the business cycle. Now that the coronavirus pandemic has exposed the fault lines underneath theeconomy, the zombification may accelerate due to a toxic concoction of Federal Reserve stimulus and congressional relief. Will zombies leave their graves, searching for freshly created US dollars and feasting on the carcass of the ailing marketplace?
The Walking Dead
Azombiecompany is a business that requires perpetual bailouts to keep its doors open, or it is a deeply indebted firm that can only repay the interest on its debt. The zombification has been eating away at Japan and China, and now it is gradually infecting the US economy. What's worse is that American zombie businesses employ about 2 million people, according to new Arbor Data Science figures.
Workers employed by zombie companies are found in many different industries. Arbor's research found that the top five sectors by employee headcount were:
Industrial conglomerates: 233,000Hardware, storage, and peripherals: 193,000Energy equipment and services: 185,000Hotels, restaurants, and leisure: 153,000Software: 142,000
But could the fragility of the market cause a huge number of workers to file for unemployment benefits? It might seem counterintuitive, but it has become a lot easier for these businesses to be resuscitated.
One institution has been supplying these walkers with brain nourishment: the Federal Reserve.
Feeding the Undead
Zombies have been finding it easier to borrow for a combination of reasons. The first is that interest rates are historically low, so it can be less difficult to repay the interest on the debt and use the cost savings to keep the lights on. The second is that the US central bank has taken unprecedented action by acquiringcorporate debtthrough the secondary exchange-traded fund (ETF) market.
Put simply, if it were not for accommodative monetary policy, these firms would have otherwise shut down by now. Once again, the Eccles Building is refusing to allow the invisible hand to rein in the excess for fear a liquidity crisis, credit crunch, and every other fancy way of saying we are in a dilly of a pickle.
But it is the entire market that is taking advantage of a desperate Fed. US companies are borrowing at the fastest pace in history year-to-date, issuing more than $1 trillion in new bonds, according to Bank of America Global. This is about double the number from 2019 during the same period. This eyebrow-raising number highlights two important facts in this environment: Companies are borrowing more cheaply than anybody would have anticipated last year, and investors are being paid so little to fund operations in an uncertain market.
MarketWatch alluded to AutoNation as a struggling company that recently borrowed $500 million from the bond market. The national chain of car dealerships posted a $232.3 million net loss in the first quarter, but it still attracted investor interest due to its 4.4 percent yield. Under present conditions, traders know that their investment is insured, because the central bank could just swoop in and rescue a troubled asset to avoid a powder keg from going off.
And it is not just Fed chair Jerome Powell going on a bond-buying spree. There is still obviously a demand for all kinds of bonds—investment grade and junk status—and this is fueling the rise of moribund companies that are gorging on debt. The undead can roam the streets for several more years if this is thede factomonetary policy.
Zombification
In Tokyo and Beijing, the typical walking dead are the banks. In the postcoronavirus economy, it is evident that the next generation of zombies will be cruise lines, retailers, and airlines. In a truly free market, these industries would eliminate a large number of companies, but because of cheap money and an accommodative Fed, that is not going to happen. Many of these businesses are also benefiting from creditors waiving or loosening previous debts, allowing some of the world's largest organizations to reach noteworthy agreements with lenders.
This is terrible news for Main Street, because now capital is being misallocated and transferred to unproductive enterprises. Businesses—large or small—with high growth prospects may not see their visions realized, because the money is going to prop up the Marriotts and Vail Resorts of the world.
With the death of small businesses potentially nigh, the entrepreneurial spirit may wither away before it even has a chance to be zombified, because it cannot access as much liquidity as the big boys.
Money Printer Go Brrr
The ramifications of these whatever-it-takes and money printer go brrr policies will only be felt in a few years, when the Fed chooses to tighten up and remove the training wheels. We have seen what happens when the Fed scales back its quantitative easing efforts: triple-digit losses on the stock market and ballooning debt-servicing payments. If this ever happens, a tidal wave of debt defaults and bankruptcies will swallow the US. Should the central bank refrain from embarking upon a prescription of tightening, a new type of economy will be born: the anti-productivity economy, comparable to Japan in the 1990s and China today. You could even make a movie out of it: Big Trouble in Little Tokyo: Dawn of the American Dead.
About the author: Andrew Moran is the Economics Correspondent at LibertyNation.com and is the author of The War on Cash. You can find more of his work at AndrewMoran.net.
Source: This article was originally published by LibertyNation.
The post There's No End In Sight To The Zombie Economy - OpEd appeared first on Eurasia Review.

Load-Date: June 2, 2020


End of Document




Limits To Strategic Foresight: Try Wisdom Of The Crowds – Analysis
Eurasia Review
May 21, 2020 Thursday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1292 words
Body


Even for a nation with Singapore's foresight capability, the full-range of COVID-19's consequences could not have been foreseen. Would not now be the ideal time to revisit key tenets of Singapore's foresight enterprise – and by implication our national security framework – and perhaps limiting the impact of future strategic surprise?
By Shashi Jayakumar and Adrian W J Kuah*
Could Singapore, with its strategic foresight capability, have predicted COVID-19? It is reasonable to ask this, since Singapore is admired globally for its strategic foresight capability. Two milestones stand out in the development of this capability.
The first occurred in the mid-1980s, when the Government experimented with using long-term scenarios in defence planning and not long after rolled it out for broader use across the different ministries. This led to the practice of "National Scenarios" that forms, at least partly, the basis for long-term planning. The second was a major review which in 2004 led to the Strategic Framework for Singapore's National Security.
Swan, Elephant, or Something Else?
Deputy Prime Minister and Coordinating Minister for Security and Defence Dr Tony Tan's 2005Ministerial Statementon the Framework observed that "the most important recommendation" was to "set up a new Coordinating Structure in the centre of Government…in the Prime Minister's Office."
Dr Tan, in highlighting the importance of investing in "imaginative solutions to intractable problems", also announced the establishment of a Risk Assessment and Horizon Scanning capability intended "to help us anticipate and deal with shocks to our system."
The Risk Assessment and Horizon Scanning Programme Office was set up within NSCS, alongside the other strategic foresight apparatus of the Government such as the Centre for Strategic Futures in the Prime Minister's Office.
Returning to the question posed above: was COVID-19 inherently unpredictable? Was it a "black swan" that would have defied the best foresight capability? Or was COVID-19 a "black elephant", the sort of catastrophic problem identified in advance but ultimately ignored, sometimes wilfully?
But maybe there is a third possibility ? that catastrophic surprise is context specific: it is not simply that it is unforeseen and unforeseeable, but seems that way because of the limits imposed by one's perspective and experiences.
Limits to Whole-of-Government Approach?
Notwithstanding the lapse when it came to assessing the magnitude of risk to foreign worker dormitories, agencies have not been unprepared. They have for years been anticipating, and preparing for, disruptions to food security (through source diversification) and infectious diseases outbreaks (the opening of the National Centre for Infectious Diseases in 2019 being a case in point).
Still, COVID-19 should give us pause: there is the opportunity to use this crisis, and do deep thinking on the our strategic foresight enterprise, and cognitive diversity within.
A horizon scanning system, for example, combines data science and data analytics with the craft and intuition of the analyst, the scenario planner or the futurist, so that we can mitigate the failure of imagination and the lack of communication that allows people to feign ignorance and thereafter act surprised.
A well-designed horizon scanning system is underpinned by Ashby's Law of Requisite Variety, encapsulated in the aphorism, "it takes complexity to defeat complexity." To better able to intuit catastrophic surprises, it helps if we look in different directions, have different mental models, and are constantly and courageously challenging each other's interpretations of what is going on.
So, could a horizon scanning system have foreseen COVID-19? Alas, probably not, and certainly not in its painful detail. But COVID-19 has shown that the best whole-of-government efforts must be enhanced. To be sure, centralising strategic foresight and horizon scanning capability within the Government, albeit with frequent consultations with outsiders, certainly makes for greater efficiency.
Wisdom of the Crowds: Look Beyond Government
But it also creates a single point of failure. To prevent this, we need to inject the requisite diversity of viewpoints and expertise necessary to detect and tackle emerging threats. Needless to say, this more distributed approach will have to trade off efficiency for robustness (and messiness).
If complexity is required to defeat complexity, then the diversity of expertise, knowledge and insights that can help Singapore deal with these emerging threats may well come as much from outside of Government – from the usual quarters such as universities and think tanks, as well as unexpected ones such as NGOs and loving critics – as from within.
In his 2004 "The Wisdom of Crowds", James Surowiecki suggested that, under certain conditions, a sufficiently diverse and independent group could produce more accurate forecasts and guesses than so-called experts.
To mitigate that "single point of failure", the strategic foresight enterprise should be less of a "Government-hub and different spokes" model, and more of a decentralised network with diverse and independent nodes simultaneously complementing and countering each other in making sense of the world. This idea has been put into practice by various corporations and government agencies, in the form of hackathons, crowdsourcing for ideas, and prediction markets.
There is a practical consideration. No matter how enlightened a government or organisation is, there is a limit to inconvenient albeit useful insights it is willing to accept, much less embrace, especially if surfaced internally. Moreover, an internal group established to counter groupthink can itself on occasion be susceptible to groupthink.
By building capabilities outside of the Government or drawing on pre-existing ones, we can shift the cognitive odds in our favour even though the foresight enterprise remains a fallible one.
National Security: Time for Review
A distributed, extra-Government horizon scanning process provides a platform for multiple interpretations of ambiguous weak signals to occur. Incompatible interpretations of the same signal, far from causing consternation, are instructive: it tells you that it does not conform to any previous pattern, and we should pay it even more attention, instead of trying to force a convergence of views.
This is where culture comes into play: we should further develop a culture that takes on board potentially useful albeit unpalatable insights surfaced by outsiders.
We are now at a point where national security and strategic foresight need to encompass and embrace that emerging range of uncomfortable novelties: not just pandemics, but food security, climate change, and grey zone operations such as disinformation. Some 15 years ago, Singapore's national security enterprise placed resilience and whole-of-government coordination front and centre, and augmented its foresight capabilities with horizon scanning.
Given how the world has changed because of COVID-19 (though not only because of it), another review may be in order, one that expands the national security discourse from one that is whole-of-government to whole-of-society, in order to tap the wisdom of the crowds.
*Shashi Jayakumar is Head, Centre of Excellence for National Security (CENS) and Executive Coordinator for Future Issues and Technology (FIT) cluster at the S. Rajaratnam School of International Studies (RSIS), Nanyang Technological University (NTU), Singapore. Adrian W J Kuah is Director, Futures Office at the National University of Singapore. This is part of an RSIS Series.
The post Limits To Strategic Foresight: Try Wisdom Of The Crowds - Analysis appeared first on Eurasia Review.

Load-Date: May 21, 2020


End of Document




Baseball Illustrates Economics With Each Game – OpEd
Eurasia Review
December 28, 2019 Saturday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1417 words
Body


By Andrew Moran*
Legendarybaseballmanager Tommy Lasorda wrote, "No matter how good you are, you're going to lose one-third of your games. No matter how bad you are you're going to win one-third of your games. It's the other third that makes the difference." Baseball is a game of failure; you are going to groundout, flyout, and strikeout more than you are going to get a base hit or a walk. Are you batting .289? You are having a good season.
Baseball is a unique sport because nothing else can replicate it. The thinking man's game is also a great mechanism to learn abouteconomics. Let's explore some of the various economic principles that Major League Baseball (MLB) teaches us.
Creative Destruction
In economics, creative destruction, also known as Schumpeter's gale, is the process of ending conventional practices to make room for innovation. You will find creative destruction in every industry, whether it is transportation or finance. It is even ubiquitous in The Show.
Baseball is in a constant state of evolution. The greatest sport on the planet routinely innovates thanks to the ingenuity and creativity of players, managers, and front offices. From the way pitchers throw from the stretch to the way the defense is positioned on the diamond, ballclubs are always searching for a competitive edge, and the only way to achieve this is by employing unique strategies and tactics.
The best example of creative destruction in the MLB is the infield shift.Although the defensive shift has been around for a century, this realignment oddity has become the norm in the last decade (thanks, Joe Maddon!). You will see third basemen playing on the other side of second base to prevent left-handed batters from placing the ball into the gap between fielders. Also, thanks to advanced analytics, teams are further maneuvering players to spots where the hitters – left- and right-handed – are more likely to place their balls.
You are starting to witness, on occasion, the outfield doing the shift. It has become so prevalent due to its efficacy that there has been a discussion about banning the measure. Until then, the shift will be used and refined by franchises across the league.It might not be as exciting as seeing a bullet into the gap in shallow right-field, but the shift is effective.
Subjective Value
Thesubjective theoryof value is the concept that a good's or a service's worth is not innate but rather determined by consumers based on how much they want or need the object. This economic principle can be found in every component of professional baseball. Everyone is participating in it, too: the owners, the players, and even the fans.
Many people will say that it is ridiculous to pay one athlete $324 million over nine years. But that is exactly what the New York Yankees did when they signed Gerrit Cole. The Boston Red Sox thought it was worth signing David Price to a $217 million contract. The Los Angeles Angels opened the checkbook and inked an aging Albert Pujols to a 10-year, $240 million deal.
This is a lot of money to spend on just one guy, especially when you have nine players on the field and 25 men on the main roster. Owners who sign these players have a couple of things in mind when handing out such lucrative contracts, including marketing dollars and championships. If your team has a superstar, then the idea is that fans are more likely to see the team. If that player helps the club win a championship, then paying a single person $27 million a year was worth the cost.
Fans make similar decisions but on a much smaller scale. When they visit Fenway Park (Red Sox) or Citi Field (New York Mets), they are paying $30, $50, or $200 for tickets, proving that they value the tickets more than the cash. Plus, these fans will pay premium prices for hot dogs and beer, showing, once again, that they prefer to consume ballpark food and drinks than to keep their hard-earned money.
To non-baseball fans, It might seem like a waste of money and time. However, baseball aficionados might feel the same way aboutStar Warsnerd culture and grown men buying fake lightsabers and pretending to be Luke Skywalker at Comic-Con. It is all about subjective value.
Self-Regulating Markets
You may have heard about the Houston Astros' sign-stealing controversy from the 2017 season and postseason. If you have not, let's just say that it consisted of overhead cameras, trashcan banging, and whistling. While many teams and players suspected foul play on the part of the 2017 World Series champions, nothing was ever proven and little national attention was generated. The MLB has initiated an exhaustive investigation and is expected to release the results of the probe in early 2020.
Sign-stealing is common in baseball, just not on the alleged level of the Astros. A runner on second base, for example, can try to determine what the pitcher is going to throw and then convey to the hitter what type of pitch he can expect. A pitcher might have a tell, revealing to batters what he is going to toss from the mound. Teams are, rightfully, so paranoid about sign-stealing that they have taken matters into their own hands.
If you watch a random MLB broadcast, you might notice the back catcher delivering multiple signs tothe pitcher. The purpose of this act is to ensure that the opposing squad cannot quickly and accurately determine the signal if the coming pitch is high cheese with some hair on it or salad (consult your nearest Ecktionary for definitions).
Rather than wait for rule changes or bans, some teams are attempting to eliminate sign-stealing with complex signs and unique communication. Should clubs fail to correctly decipher what signs are being shown, theywillinevitably shut down operations and try to win games by just being the better team through tactics and athleticism. This is how markets regulate themselves without any state interventions.
Labor Negotiations
How did you land your job? Chances are, you worked on an arrangement with the hiring manager that satisfied both parties.Liberty Nationrecently reported on the $800 million in contracts that agent Scott Boras helped negotiate during the 2019 winter meetings in San Diego, CA. For all the insults thrown his way, he is an effective agent who gets the job done by generating top dollar for his clients. Ask Cole, ask Anthony Rendon, and ask Stephen Strasburg.
In baseball, there is no salary cap, meaning teams can spend any amount they wish on a player of their choosing. Depending on the state of the market and the teams' needs, elite athletes usually have the advantage. They typically demand a few things: dollar-amount, length of the contract, and average annual value (AAV). A team may offer a five-year deal worth $143 million, but a player may request a six-year deal valued at $146 million.
To get what they want, athletes will have the best people around them negotiating and ironing out the terms and conditions of a lucrative contract. General managers may attempt to sweeten the pot by throwing in incentives: If a player finishes in the top 10 of MVP voting each season, then he will earn an extra $2 million a year – or a pitcher may need three consecutive seasons of 200 or more innings to receive a $4 million bonus and a club option.
Sometimes it works and sometimes it doesn't. The point is all about labor relations and how employers and employees agree on a working partnership.
As American As Apple Pie
Today, every team's front office is filled with employees holding degrees in economics, mathematics, or data science. Baseball was always a game of strategy, but now this has morphed into intricate and obscure measurements, such as xFIP, BABIP, and REW – all Sabermetrics that would trigger headaches for non-baseball fans and non-STEM majors. Key performance indicators are integral to the sport, but baseball is also a fantastic physical display of the various laws and principles of economics. In a world where this discipline is portrayed with great disdain, baseball can breathe some life into the science.
*About the author: Economics Correspondent at LibertyNation.com. Andrew has written extensively on economics, business, and political subjects for the last decade. He also writes about economics at Economic Collapse News and commodities at EarnForex.com. He is the author of "The War on Cash." You can learn more at AndrewMoran.net.
Source: This article was published by Liberty Nation
The post Baseball Illustrates Economics With Each Game - OpEd appeared first on Eurasia Review.

Load-Date: December 27, 2019


End of Document




More Than A Lifetime Away: World Faces 100-Year Wait For Gender Parity
Eurasia Review
December 17, 2019 Tuesday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1993 words
Body


The time it will take to close the gender gap narrowed to 99.5 years in 2019. While an improvement on 2018 – when the gap was calculated to take 108 years to close – it still means parity between men and women across health, education, work and politics will take more than a lifetime to achieve. This is the finding of the World Economic Forum's Global Gender Gap Report 2020, published Monday.
According to the report, this year's improvement can largely be ascribed to a significant increase in the number of women in politics. The political gender gap will take 95 years to close, compared to 107 years last year. Worldwide in 2019, women now hold 25.2% of parliamentary lower-house seats and 21.2% of ministerial positions, compared to 24.1% and 19% respectively last year.
Politics, however, remains the area where least progress has been made to date. With Educational Attainment and Health and Survival much closer to parity on 96.1% and 95.7% respectively, the other major battlefield is economic participation. Here, the gap widened in 2019 to 57.8% closed from 58.1% closed in 2018. Looking simply at the progress that has been made since 2006 when the World Economic Forum first began measuring the gender gap, this economic gender gap will take 257 years to close, compared to 202 years last year.
Economic Gap Widening
The report attributes the economic gender gap to a number of factors. These include stubbornly low levels of women in managerial or leadership positions, wage stagnation, labour force participation and income. Women have been hit by a triple whammy: first, they are more highly represented in many of the roles that have been hit hardest by automation, for example, retail and white-collar clerical roles.
Second, not enough women are entering those professions – often but not exclusively technology-driven – where wage growth has been the most pronounced. As a result, women in work too often find themselves in middle-low wage categories that have been stagnant since the financial crisis 10 years ago.
Third, perennial factors such as lack of care infrastructure and lack of access to capital strongly limit women's workforce opportunities. Women spend at least twice as much time on care and voluntary work in every country where data is available, and lack of access to capital prevents women from pursuing entrepreneurial activity, another key driver of income.
"Supporting gender parity is critical to ensuring strong, cohesive and resilient societies around the world. For business, too, diversity will be an essential element to demonstrate that stakeholder capitalism is the guiding principle. This is why the World Economic Forum is working with business and government stakeholders to accelerate efforts to close the gender gap," said Klaus Schwab, Founder and Executive Chairman of the World Economic Forum.
Could the "Role Model Effect" close the gender gap?
One positive development is the possibility that a "role model effect" may be starting to have an impact in terms of leadership and possibly also wages. For example, in eight of the top 10 countries this year, high political empowerment corresponds with high numbers of women in senior roles. Comparing changes in political empowerment from 2006 to 2019 shows that improvements in political representation occurred simultaneously with improvements in women in senior roles in the labour market.
While this is a correlation, not a causation, in OECD countries, where women have been in leadership roles for relatively longer and social norms started to change earlier, role model effects could contribute to shaping labour market outcomes.
Gender Inequality in the Jobs of the Future
Possibly the greatest challenge preventing the economic gender gap from closing is women's under-representation in emerging roles. New analysis conducted in partnership with LinkedIn shows that women are, on average, heavily under-represented in most emerging professions. This gap is most pronounced across our "cloud computing" job cluster where only 12% of all professionals are women. The situation is hardly better in "engineering" (15%) and "Data and AI" (26%), however women do outnumber men in two fast-growing job clusters, "content production" and "people and culture".
According to our data, this reality presents leaders intent on addressing the gender gap in the future with two key challenges. The first and most obvious challenge is that more must be done to equip women with the skills to perform the most in-demand jobs. Indeed, there is an economic cost of not doing so as skills shortages in these professions hold back economic growth.
The second is possibly more complex. According to our data, even where women have the relevant in-demand skillset they are not always equally represented. In data science, for example, 31% of those with the relevant skillset are women even though only 25% of roles are held by women. Likewise, there is no gender gap in terms of skills when it comes to digital specialists, however only 41% of these jobs are performed by women.
These facts point to three key strategies that must be followed to hardwire gender equality into future workforces: to ensure women are equipped in the first place – either through skilling or reskilling – with disruptive technical skills; to follow-up by enhancing diverse hiring; and to create inclusive work cultures.
"Insights from LinkedIn's Economic Graph can help policymakers, business leaders, and educators understand and prepare for how women will be represented in the future workforce. Our data shows that meaningful action is needed to build the systems and talent pipelines required to close the gender gap in tech and ensure women have an equal role in building the future," said Allen Blue, Co-Founder and Vice-President, Product Strategy, LinkedIn.
What the Forum is Doing to Close the Gender Gap
The World Economic Forum's Platform for Shaping the Future of the New Economy and Society aims to close economic gender gaps through both in-country and global industry work. Through Closing the Gender Gap Accelerators, the Forum drives change by setting up action coalitions between relevant ministries and the largest employers in the country to increase female labour force participation, the number of women in leadership positions, closing wage gaps and preparing women for jobs of the future. Additionally, the global business commitment on Hardwiring Gender Parity in the Future of Work mobilizes businesses to commit to hiring 50% women for their five highest growth roles between now and 2022. Finally, the Forum has committed to at least double the current percentage of women participants at the Annual Meeting in Davos-Klosters, Switzerland, by 2030.
"To get to parity in the next decade instead of the next two centuries, we will need to mobilize resources, focus leadership attention and commit to targets across the public and private sectors. Business-as-usual will not close the gender gap – we must take action to achieve the virtuous cycle that parity creates in economies and societies," said Saadia Zahidi, Head of the Centre for the New Economy and Society and Member of the Managing Board, World Economic Forum.
The Global Gender Gap in 2020
Nordic countries continue to lead the way to gender parity. Iceland (87.7%) remains the world's most gender-equal country, followed by Norway (2nd, 84.2%), Finland (3rd, 83.2%) and Sweden (4th, 82.0%). Other economies in the top 10 include Nicaragua (5th, 80.4%), New Zealand (6th, 79.9%), Ireland (7th, 79.8%), Spain (8th, 79.5%), Rwanda (9th, 79.1%) and Germany (10th, 78.7%).
Among the countries that improve the most this year are Spain in Western Europe, Ethiopia in Africa, Mexico in Latin America, and Georgia in Eastern Europe and Central Asia. These countries all improved their positions in the ranking by more than 20 places, largely driven by improvements in the political empowerment dimension.
Western Europe is the best performing region for the 14th consecutive year. With an average score of 76.7% (out of 100), the region has now closed 77% of its gender gap, further improving from last edition. At the current pace, it will take 54 years to close the gap in Western Europe. The region is home to the four most gender-equal countries in the world, namely in order Iceland (87.7%), Norway (84.2%) and Finland (83.2%) and Sweden (82.0%), and one country (Spain, 8th) is among the most improved countries this year.
The North America region regroups the United States (72.4%, 53rd) and Canada (77.2%, 19th). Both countries' performances are stalling, especially in terms of economic participation and opportunity. At this rate it will take 151 years to close the gap.
The Eastern Europe and Central Asia region has closed 71.5% of its gender gap so far with a slight improvement since last year. To date the time to fully close its overall gender gap is estimated to be 107 years. The region has fully closed its educational gap and has improved women's political empowerment which however remains only closed at 15%. 21 of the 26 countries in this region have closed at least 70% and the top-ranked country, Latvia, 11th has closed 78.5% of its gap.
The Latin America and the Caribbean region has closed 72.1% of its gender gap so far, progressing 1 percentage points since last year. At this rate it will take 59 years to close the gender gap. The most noticeable improvement is on the Political empowerment dimension where the region closes its gap by 5 percentage points. Led my Nicaragua that has closed 80.4% of its gap (5th), 15 of the 24 countries covered by the report have improved their overall scores. Among the most improved countries, Mexico reduced its gender gap by 3.4 points on a year-over-year basis.
The Sub-Saharan Africa region has closed 68.0% of its gender gap so far. This result is a significant progress since last edition which leads to revise down the number of years it will take to close the gender gap, which is now estimated at 95 years. The region is home of one of the top-ten countries overall Rwanda (9th) while another 21 countries have improved their performances since last year, including Ethiopia (82nd) one of the best improved this year globally.
The East Asia and Pacific Region has closed 69% of the overall gender gap. If the region maintains the same rate of improvement as the 2006-2019 period, and given the current gap, it will take another 163 years to close the gender gap, the most time of any region. The region has improved on three of the four gender gap dimensions and has been the only region where political empowerment gap has widened (16% closed so far). The best performing country is New Zealand

6th, which has closed 79.9% of its gap. It is followed by the Philippines 16th with 78.1% closed and Lao PDR, 43rd with a score of 73.1%.
South Asia region has closed two thirds of its gender gap. The region's gender gap is the second largest despite a progress of 6 points over the past 14 years. If the rate of progress of the past 15 years was to continue it will take 71 years to close the region's gender gap. However, in contrast with the overall's performance, the region's Economic participation and opportunity gap widens this year. Bangladesh (50th) leads the region, while the second ranked country, Nepal, lags several positions behind (101th)
The Middle East and North Africa (MENA) region obtains the lowest score (61.1%) despite having narrowed its gap by 0.5 points since last year. Assuming the same rate of progress going forward it will take approximately 150 years to close the gender gap in the MENA region. The two most highly ranked countries in the region are Israel

(64th) with a closed gap to date of 71.8% and the United Arab Emirates (120th) with a score of 65.5%. 15 of the 19 countries in this region rank 130th or lower.
The post More Than A Lifetime Away: World Faces 100-Year Wait For Gender Parity appeared first on Eurasia Review.

Load-Date: December 16, 2019


End of Document







| About LexisNexis | Privacy Policy | Terms & Conditions | Copyright © 2021 LexisNexis 






Page 8 of 9A COSMIC Approach To Nanoscale Science


 


 



Page 12 of 13Artificial Intelligence May Help Achieve UN's Sustainable Development Goals


 


 



Page 14 of 15Study Claims Reparations For Slavery Could Have Reduced COVID-19 Infections And Deaths In US


 


 



Page 19 of 20Global Ice Loss Increases At Record Rate


 


 



Page 22 of 23Forecasting Coastal Water Quality


 


 



Page 24 of 25Climate Change To Alter Position Of Earth's Tropical Rain Belt


 


 



Page 26 of 27Big Data To Analyze The Mystery Of Beethoven's Metronome


 


 



Page 28 of 29Understanding How Birds Respond To Extreme Weather Can Inform Conservation Efforts


 


 



Page 30 of 31The Danger Of Weaponizing Trade For The Environment – Analysis


 


 



Page 33 of 34Forecasting Urbanization


 


 



Page 36 of 37Irish And UK Research Helps To Unravel Secrets Behind Game Of Thrones


 


 



Page 38 of 39Artificial Intelligence Can Predict Students' Educational Outcomes Based On Tweets


 


 



Page 42 of 43Knowing The Model You Can Trust: The Key To Better Decision-Making


 


 



Page 44 of 45Spain Using Mobile Phone Data To Study Efficacy Of Lockdown On Spread Of COVID-19


 


 



Page 46 of 47Climate Signals Detected In Global Weather


 


 



Page 48 of 49Experts To Create Predictive Tool To Tackle Hate Crime In Los Angeles


 


 



Page 50 of 51Police Stop Fewer Black Drivers At Night When 'Veil Of Darkness' Obscures Their Race


 


 



Page 52 of 53Model Beats Wall Street Analysts In Forecasting Business Financials


 


 



Page 55 of 56March Madness Bracket Analysis Shows Picking Final Four First Leads To Better Brackets


 


 



Page 57 of 58Influencing Electoral Outcomes: The Ugly Face Of Facebook - Analysis


 


 



Page 60 of 61How Climate Change Affects Crops In India


 


 



Page 62 of 63Facebook, Cambridge Analytica And Surveillance Capitalism - OpEd


 


 



Page 65 of 66Surface Clean-Up Technology Won't Solve Ocean Plastic Problem


 


 



Page 67 of 68More Evidence Of Causal Link Between Air Pollution And Early Death


 


 



Page 69 of 70There's No End In Sight To The Zombie Economy – OpEd


 


 



Page 72 of 73Limits To Strategic Foresight: Try Wisdom Of The Crowds – Analysis


 


 



Page 75 of 76Baseball Illustrates Economics With Each Game – OpEd


 


 



Page 78 of 79More Than A Lifetime Away: World Faces 100-Year Wait For Gender Parity


 


 

