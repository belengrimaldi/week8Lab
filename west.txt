
Date and Time: Wednesday, March 24, 2021 3:55:00 PM EDT
Job Number: 139771397
Documents (72)
1. Can vaccination and infection rates add up to reach COVID herd immunity?
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
2. USC Pharmacy School Receives $5 Million To Improve Prescription Safety
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
3. Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. iHealthBeat;Seizures and Mosquitoes: The Rewards of Working Smarter With Data Science
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
4. LOWER FEES, BETTER TECH?;NEW VOICES ARE VYING FOR SPACE IN THE FOOD-DELIVERY WORLD
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
5. How to erase the digital divide
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
6. CORONAVIRUS IN CALIFORNIA;We don't know the half of it;Did 50% of L.A. residents catch COVID-19? It all depends.
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
7. UCSD Researchers Use Map To Reveal How Resources Are Distributed In The Brain
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
8. CITY BEAT;A tail-wagging tale of man and corgi
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
9. D.A.'s New Investigative Chief Sworn In
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
10. Is it time to kill calculus?
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
11. Is it time to kill calculus?
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
12. Is it time to kill calculus?
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
13. In the West, wildfire smoke accounts for more pollution;Air quality declines again after years of steady gains, study finds
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
14. County Reports 56 Deaths, Hospitalizations Surge As COVID-19 Variant Spreads
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
15. SD County Reports 3,815 new COVID-19 Infections As Hospitalizations Increase
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
16. San Diego Foundation Announced $750,000 in STEM Grants for Minority Students
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
17. Nielsen Sets Timetable for Cross-Platform Media Measurement
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
18. Learn to expertly work with data and earn an in-demand job
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
19. Back to the drawing board, maps
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
20. Garcetti Announces Deputy Mayor to Lead Office of Budget and Innovation
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
21. Asian Americans split on affirmative action at UC;Proposition 16 divides university system's most overrepresented group of students.
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
22. Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. California Healthline;'It's Science, Stupid': A School Subject Emerges as a Hot-Button Political Issue
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
23. Studies Link Viewership of Fox News to Reduced Pandemic Precaution
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
24. NASA Awards Los Angeles $1.3 Million for Air Quality Monitoring Program
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
25. Super Aggregator ScreenHits TV Closes $2 Million in Funding
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
26. There is hard data that shows "Bernie Bros" are a myth
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
27. Biden campaign adds more staff in Texas
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
28. CDC softened school reopening guidelines criticized in fringe group's letter to President Trump
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
29. CDC softened school reopening guidelines criticized in fringe group's letter to President Trump
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
30. Student debt and the end of the liberal arts dream
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
31. Student debt and the end of the liberal arts dream
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
32. L.A Times Endorses Biden
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
33. New Measurement of Home Condition Without Inspections - Pomar Lane Completes Condition Scores for 90,000 Homes
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
34. Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. California Healthline;As Georgia Reopened, Officials Knew of Severe Shortage of PPE for Health Workers
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
35. Andrew Cuomo saw COVID-19's threat to nursing homes - yet he still risked adding to it
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
36. Mathematicians urge peers to stop working on racist "predictive policing" technology
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
37. Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. California Healthline;Coronavirus Tests The Value Of Artificial Intelligence In Medicine
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
38. Cal State LA researchers use data visualization, AI in fight against COVID-19
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
39. Statistic of the decade: The massive deforestation of the Amazon
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
40. Uber's data revealed nearly 6,000 sexual assaults. Does that mean it's not safe?
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
41. QAnon conspiracy theories about the coronavirus pandemic are a public health threat
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
42. UC Health Launches Online Dashboard of COVID-19 Cases,Testing at its Hospitals
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
43. UC Health Launches Online Dashboard of COVID-19 Cases,Testing at its Hospitals
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
44. Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. California Healthline;No Safety Switch: How Lax Oversight Of Electronic Health Records Puts Patients At Risk
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
45. Berkeley Coding Academy Launches 2020 Summer Camp in Machine Learning and A.I.
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
46. In YouTube "edutainment," minimal control for scientific accuracy
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
47. The census goes digital - 3 things to know
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
48. 200 years before Orwell, a German naturalist prophesied surveillance capitalism
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
49. Facebook CEO Mark Zuckerberg made staff recommendations to Pete Buttigieg's presidential campaign
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
50. Film Diversity Helps Drive Box Office Hits, Study Shows
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
51. Roku to Acquire Video Ad Platform dataxu for $150 Million
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
52. CBS and Viacom Chiefs' Memos to Staff Hint at Changes Ahead
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
53. Researchers Identifies Genes that Increase Autism Risk
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
54. You'd be better off lighting your money on fire than giving it to a politician to spend on TV ads
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
55. Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. California Healthline;Walmart Charts New Course By Steering Workers To High-Quality Imaging Centers
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
56. Garcetti Heads to New Jersey to Promote Nonprofit Partnership
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
57. Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. California Healthline;Extreme Temperatures May Pose Risks To Some Mail-Order Meds
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
58. Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. California Healthline;Extreme Temperatures May Pose Risks To Some Mail-Order Meds
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
59. Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. California Healthline;Why The U.S. Remains The World's Most Expensive Market For 'Biologic' Drugs
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
60. UCSD Professor Elected to the National Academy of Medicine
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
61. Cloudvirga Earns a 2018 Benzinga FinTech Award
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
62. Has the Opioid Epidemic Begun to Turn a Corner?
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
63. The Whistle-Blower Behind the Facebook Data Scandal Has a Surprising Fashion Background
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
64. Tribeca: 'Enhanced' Directors Discuss Sports Science's Complicated Future
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
65. Hillary Clinton -- Endless Sore Loser
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
66. No Headline In Original
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
67. Opinion: Take the Dictator Quiz
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
68. Copyright © Advisory Board Company & California Healthcare Foundation 2014, All Rights Reserved. iHealthBeat;Can the U.S. Health Care System Realize the Promise of Digital Health?
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
69. Lauren Zalaznick to Leave NBCUniversal
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
70. No Headline In Original
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
71. Producer Michael Shamberg Wants to "Invent the Future" With BuzzFeed Motion Pictures
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California
72. WME | IMG Launches "Internet of Things" Joint Venture With AGT International
Client/Matter: -None-
Search Terms: "data science"
Search Type: Natural Language 
Narrowed by: 

Content TypeNarrowed byNewsLanguage: English; Location by Publication: California




Can vaccination and infection rates add up to reach COVID herd immunity?
Salon.com
March 21, 2021 Sunday


Copyright 2021 Salon.com, LLC. All Rights Reserved
Length: 1125 words
Highlight: Some experts are skeptical of the 60% threshold for herd immunity


Body


Link to Image
Shot of a group of young people wearing masks Getty Images
It's been a long, dark winter of covid concerns, stoked by high post-holiday case counts and the American death tally exceeding 530,000 lives lost. But with three vaccines - Pfizer-BioNTech, Moderna and Johnson & Johnson - now authorized for emergency use in the United States, there seems to be hope that the pandemic's end may be in sight.
A recent analysis by the Wall Street research firm Fundstrat Global Advisors fueled this idea, suggesting as many as nine states were already reaching the coveted "herd immunity" status as of March 7, signaling that a return to normal was close at hand.
"Presumed 'herd immunity' is 'the combined value of infections + vaccinations as % population > 60%,'" noted a tweet by a CNBC anchor based on a more complete analysis by the firm. That got us thinking: Does this calculation hold up?
First, do public health experts generally consider herd immunity to kick in at 60%? In addition, does current scientific thinking equate protection from the antibodies generated by past covid infections with the same degree of protection as a vaccination?
We decided to find out.
First, a review of herd immunity. Also known as community or population immunity, the term is used to describe the point at which enough people are sufficiently resistant - or have an immune response - to an infectious agent that it has difficulty spreading to others.
In this explainer, we noted that people generally gain immunity either from vaccination or infection. For contagious diseases that have marked modern history - smallpox, polio, diphtheria or rubella - vaccines have been the mechanism through which herd immunity was achieved.
While the United States is getting closer to this point, most health experts caution, it still has ground to cover. Fundstrat's analysis offered a rosier take. Although the site is located behind a paywall, the chart generated buzz on Twitter and in news outlets like the Daily Caller.
Fundstrat relied on a variety of sources - particularly, a data scientist and pandemic modeler named Youyang Gu - to determine what level of immunity a state needs to stamp out covid, said Ken Xuan, the firm's head of data science research. From there, analysts created a chart intended to track the level of covid immunity in each state. They calculated the number by adding the percentage of people estimated to have been infected with the virus to the percentage of people who had received the vaccine.
Xuan, who was quick to note that he is not a public health expert, said he and his team followed Gu's predictions and arrived at 60%, a figure he acknowledges is an assumption.
"The idea would be we don't know if 60% is true," he said. However, if states that have reached this threshold see steep declines in covid cases, "then it's the number to watch."
What About the 60% Marker?
Throughout the pandemic, health experts have tended to set the magic number for herd immunity between 50% and 70% - with most, including Dr. Anthony Fauci, the head of the National Institute of Allergy and Infectious Diseases, leaning toward the higher end of the spectrum.
"I would say 75 to 85% would have to get vaccinated if you want to have that blanket of herd immunity," he told NPR in December.
The experts we consulted were skeptical of the 60% figure, saying the mechanics of the Fundstrat analysis were relatively sound but oversimplified.
Ali Mokdad, chief strategy officer for population health at the University of Washington, said the level of immunity needed to reach this goal can vary due to several factors. "Nobody knows what is herd immunity for covid-19 because it's a new virus," he said.
That said, Mokdad described using 60% as "totally wrong." Data from other communities around the world show covid outbreaks happening at or near that level of immunity, he said. Indeed, the city of Manaus in Brazil saw cases drop for several months, then surge despite three-fourths of their residents already having had the virus.
Josh Michaud, associate director for global health policy at KFF, described the 60% assumption as "off-base."
And some said it wasn't even the main point.
Dr. Jeff Engel, senior adviser for covid at the Council of State and Territorial Epidemiologists, said the question of herd immunity may not even be relevant because, regarding covid, we may never reach it. The novel virus may become endemic, he said, which means it will continue circulating like influenza or the common cold. For him, lowering deaths and hospitalizations is more important.
"The concept of herd immunity means that once we reach the threshold, it's going to go away," Engel said. "That's not the case. That's a false notion."
Natural and Vaccine Immunity - Should They Be Lumped Together?
When asked why the Fundstrat analysis treated the two types of immunity as equivalent, Xuan said it was an assumption.
Here's what current science supports.
Those who receive any of the three vaccines available in the United States enjoy a high level of protection against getting seriously sick and dying from covid - even after one dose of a two-shot series.
In addition, people who were infected and recovered from the virus appear to retain some protection for at least 90 days after testing positive. Immunity may be lower and decline faster among people who developed few to no symptoms.
Practically speaking, two experts said, natural and vaccine-induced immunity work the same way in the body. This lends credibility to Fundstrat's approach.
However, some health experts consider vaccine-induced immunity to be better than the protection generated by the infection because it may be more robust, said Michaud. Researchers are still figuring out whether people who were infected with the virus but experienced mild or no symptoms generated an immune response as strong as those who developed more severe disease.
In fact, the Centers for Disease Control and Prevention cites the unknowns surrounding natural immunity and the risk of getting sick again with covid as reasons for those who had the virus to get a vaccine.
"They haven't been studied well at all yet," said Engel, in reference to asymptomatic people. "And maybe we're going to discover that a large group of them didn't develop really robust immunity."
Both types of viral protection leave room for potential breakthrough infections, Michaud said. Neither offers "perfect immunity," he said. And wild cards remain. How long do both types of immunity last? How do different people's systems respond? How protected will people be from emerging coronavirus variants?
"It's a witches' brew of different factors to consider when you're trying to estimate herd immunity at this point," said Michaud.Link to Image

Load-Date: March 20, 2021


End of Document




USC Pharmacy School Receives $5 Million To Improve Prescription Safety
City News Service
March 11, 2021 Thursday 6:01 PM PST


Copyright 2021 City News Service, Inc. All Rights Reserved
No City News Service material may be republished without the express written permission of the City News Service, Inc.
Length: 418 words
Body


LOS ANGELES (CNS) - The USC School of Pharmacy received a $5 million gift to create a center that will help ensure patients receive the right medication and guidance to reduce their risk of hospitalization, university officials announced today.
The donation from the estate of USC alumna Susie Titus, who died in February 2020,will be used to establish the USC Titus Center for Medication Safety and Population Health.
"The goal of the Titus Center is to improve patient health and safety by ensuring that patients with chronic diseases receive the right medication at the right doses, avoid dangerous drug interactions and understand how to use each medication correctly," said Steven Chen, the school's associate dean for clinical affairs.
Taking the wrong medication or dosage can lead to costly hospital visits or even death, Chen said.
"Over $528 billion of avoidable spending occurs each year in the U.S. due to harm or inadequate results from medication, accounting for the third leading cause of death," he said. "The Titus Center will create opportunities for pharmacists to ensure that all patients, regardless of socioeconomic status, attain optimal results from medication therapy."
Titus, who earned her bachelor's degree in education from USC in 1960 and came from a long line of Trojans and pharmacists, is one of seven relatives to graduate from USC. The Titus family endowed the pharmacy department at the USC School of Pharmacy in 2004.
"We are deeply grateful to Susie Titus for her vision and generosity," said USC School of Pharmacy Dean Vassilios Papadopoulos. "Likewise, we are grateful to the entire Titus family for their longstanding support of the USC School of Pharmacy."
Part of the donation will fund the salary of the Susie Titus Professorship in Medication Safety. That faculty member will have a background in health care data science, machine learning, artificial intelligence and medication safety. The holder of the endowed professorship will encourage USC School of Pharmacy students to explore new technologies and strengthen the mission of the USC Titus Center.
"This support will allow USC students the opportunity to learn the critical components of developing a sustainable advanced pharmacy practice," Papadopoulos said. "Through the USC Titus Center, students will gain exposure to health care data science, machine learning and artificial intelligence in medication safety -- tools to ensure they are prepared for successful careers in a dynamic health care environment."

Load-Date: March 12, 2021


End of Document





 
Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. 
iHealthBeatSeizures and Mosquitoes: The Rewards of Working Smarter With Data Science
 iHealthBeat
October 16, 2015


Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. 


Section: Perspectives
Byline: Ray Guzman
Body

Big Data has been defined as the oil of the 21st century. Just as crude oil must be refined into gasoline to power our cars, the large, complex data sets big data comprise aren't much use until they're honed into actionable insights. Data science deploys a range of tools --- from crowdsourcing to visualization -- to capitalize on the promise of big data.
 
Until recently, the U.S. health care system has been less than immersed in the big data revolution. What drives most providers are issues of compliance or the avoidance of financial penalties, such as those associated with readmission.
 
But the landscape is changing. Instead of looking at a population of diabetics and asking retrospective questions, such as "What percentage required hospitalization? For how long?," big data can help answer specific questions around a given population, diagnosis or risk factor. This intelligence is the fuel that providers need to produce the most effective, personalized intervention.
 
The elegance of data science provides the kind of granular detail needed to answer very specific questions, but it can also be harnessed to tackle some of the world's thorniest health challenges.
 
Two case studies illustrate how data science is used to gain insight into world health issues.
 
Crowdsourcing To Predict Epileptic Seizures
 
The Mayo Clinic threw out a problem to a group of data scientists across the globe. Together with the American Epilepsy Society, it used the wisdom of the crowd to develop computer algorithms for the detection, prediction and prevention of epileptic seizures.
 
The particular needle-in-a-haystack problem was this: Physicians were looking to apply DBS (deep brain stimulation) as a treatment for epilepsy, and they needed highly specific information on the data surrounding pre-seizure brain activity.
 
To start the project, EEG data from both human and canine subjects were provided to approved teams. The goal was to find a variable to identify an algorithm to predict who will have a seizure and who will not.
 
Two hundred teams took up the 60-day challenge. The benchmark for the project was 65% accuracy, and the top two teams hit 95%.
 
The algorithm development process, as designed by one team, involved creating an artificial neural network that acts as a starting point for analysis. The team also used what it had learned from a previous challenge -- locate black holes based on the halo effect of starlight by separating signal from noise. The team has documented the algorithm for Benjamin Brinkmann of the Mayo Systems Electrophysiology Lab so Mayo researchers can use it for further innovation in epilepsy treatment.
 
But Mayo isn't keeping all this information to itself. Perhaps the greatest promise this and other data science competitions hold is that all of the data and algorithms will be shared openly online. Crowdsourcing creates a new approach for medical research that holds promise for the one in 26 people who suffers from epilepsy worldwide, and the one-third of those individuals whose seizures are not sufficiently treated with medication or other therapies.
 
Predicting Outbreaks of West Nile Virus
 
Here's another world health issue that big data addresses. West Nile virus is most commonly spread to humans through infected mosquitoes. From the time it was first reported in the U.S. in 1999 through 2012, treating the West Nile virus cost $ 778 million in expenses and lost productivity. Twenty percent of people who become infected with the virus develop symptoms ranging from a persistent fever to serious neurological complications that can result in death.
 
City public health systems have established comprehensive surveillance and control programs for West Nile since roughly 2004, but very few have developed a process to predict where the virus might occur.
 
Again, mountains of data -- including weather, location, testing and spraying -- are corralled to develop an algorithm to predict locations where West Nile is likely to become an issue. The algorithm predicts when and where different species of mosquitoes will likely test positive for West Nile virus. Families can protect themselves more aggressively and providers can be better prepared.
 
These findings will be presented at a global health conference in Sri Lanka later this year.
 
Refining big data into information we can use has enormous implications. For data scientists tackling health care issues, the opportunities are as big as curing epilepsy and as widespread as the summer's mosquitoes.
 
What is most important is finding ways to affordably and rapidly get answers to questions that allow health care to become a proactive industry rather than reactive.
 
When health care organizations of all sizes are faced with a rising number of challenges, better data to support clinical, financial and operational challenges are a requirement. Providing access to "practical data science" is an important component to ensure the big data tide, combined with data science, lifts quality care delivery and operational performance.


End of Document




LOWER FEES, BETTER TECH?; NEW VOICES ARE VYING FOR SPACE IN THE FOOD-DELIVERY WORLD
Los Angeles Times
March 7, 2021 Sunday
Final Edition


Copyright 2021 Los Angeles Times All Rights Reserved
Section: FOOD; Food Desk; Part FO; Pg. 1
Length: 1031 words
Byline: STEPHANIE BREIJO
Body


Nabeel Alamgir keeps waiting for a cease-and-desist letter. If he gets one, he says, he'll view it as a badge of honor.
In January, the food-tech entrepreneur launched NotGrubhub.org, a map-based website that points customers to restaurants that take food orders directly. It was designed to bypass Grubhub and third-party food delivery apps and platforms that can legally charge up to 20% in commissions or marketing fees from restaurants in Los Angeles.
Any restaurant can add itself to the database, which is free to owners and customers and lists more than 120 businesses nationwide.
"That's just an awareness campaign," says Alamgir, who used to work in food service. His website is one of a number of emerging voices vying for space in the food-delivery world, offering fees that are typically lower than those of major players such as Postmates, Uber Eats and Grubhub.
The former chief marketing officer of Bareburger said he watched as his chain's profits diminished over the years and realized if a national chain was hurting from app commissions and fees, mom-and-pop restaurants could be too.
In 2019 Alamgir cofounded Lunchbox, a platform that charges restaurants a monthly flat fee -- as opposed to sales commissions -- that range from $88 to $490, depending on the service package; most clients, he says, pay $200. The platform hosts and designs apps, websites and ordering pages; maintains the virtual end of cloud kitchens; and creates marketing materials such as Instagram ads. Lunchbox also hires the same delivery drivers used by the big third-party platforms at the same cost of roughly $6 per driver per order and makes marketing technology and materials easier for restaurateurs and chefs to use.
"A very messy system was created because restaurant people are not starting these tech companies -- tech people are," Alamgir says. "Restaurateurs are not tech people; they want to be hospitable and create amazing food, and then we told them, 'You've got to be amazing with tech as well or you're dead. Your business is dead.' "
The new contenders tend to promise restaurants one of two things to remain competitive with larger, more established platforms: flat-rate fees, like Lunchbox, or commission rates that can hover as low as 2%, as well as marketing capabilities to help restaurants stay visible.
Grassroots and more locally focused newcomers also can offer hypertailored curation, privacy and a shift from third-party delivery systems entirely.
The owners of Pasadena-based DiNG -- not to be confused with Ding Menu, a new commission-free restaurant ordering tool -- say they want it to become the Spotify of food platforms by tailoring meal recommendations based on a two-minute quiz.
Former chef and DiNG cofounder Mike Chen said his background in data science helped inform the company's algorithm, which is based on answers to questions such as, "What looks good for a cold winter night?" (Your options might be fish fillet in chile sauce; braised pork belly; or a soup of salted pork with bamboo shoots.)
The Asian-cuisine-focused operation also allows menu ordering, but the format is arranged by dishes or even region, as opposed to separate restaurants, creating a sort of editors' pick of noodles, poached chicken, curries, stews and more from a mix of restaurants largely located in the San Gabriel Valley.
The platform charges a commission of less than 5%. The startup also focuses on user privacy: Hired drivers, also used by third-party apps, pick up food from the restaurants and deliver them to designated DiNG handoff points. From there, the company's own drivers deliver on the last leg of the route to avoid providing home addresses to the major platforms.
DiNG, spurred by the pandemic and still in its nascency, offers daily service within a limited radius. By the end of 2021, Chen hopes it will provide the same daily service to all of Los Angeles County as well as Ventura and Orange counties.
Some innovators never intended to enter the delivery business at all. Jared Jue envisioned MAMA as a restaurant-recommendation site, but as the pandemic began to shutter independent restaurants, the founder felt a need to preserve businesses in need -- many of which are underrepresented in media and run by immigrant families.
With the help of Alice Han, MAMA launched Drive-By Kitchen, which picks up an ever-changing lineup of dishes from multiple restaurants throughout Los Angeles and Orange counties and then delivers them to three pickup locations: the Westside, Koreatown and Alhambra.
The service is not offered on a daily basis; rather, it is scheduled approximately every other week. Customers must order in advance, almost like an event, and "tickets" are limited.
Drive-By Kitchen participants are frequently restaurants that can't afford to join the major delivery apps or aren't tech-savvy, and they keep 100% of the profits. MAMA's only charge to customers? A credit card service charge, along with a small fee for team members and gasoline.
"Businesses were kind of living and dying by the phone waiting for some sort of Doordash or Grubhub or Uber to come through, and I think it was mentally just draining because they weren't getting what they needed," Jue says. "We knew what we wanted to do was focus on placing large orders with the restaurants so that they could have something concrete, in a way."
The format ensures restaurants don't lose money on food costs, while the "combo-meal" format gives diners a new lineup every time.
This year Drive-By Kitchen, which Han oversees, also launched a meal-matching program, where every meal sold gets matched by a charitable partner that buys a second meal -- doubling the restaurants' revenues and donating that second portion to those in need, such as seniors in Chinatown.
"In terms of the big players in the market, we're not trying to compete with them by any means; I think that they're business-first, and our mission is more culturally relevant-first," says Jue. "We're really trying to preserve the dishes, the recipes, the restaurants -- those kinds of things that will actually disappear at the end of this whole thing. Is it sustainable? I hope so. I hope we can change the conversation."

Graphic

 
PHOTO: (no caption)  PHOTOGRAPHER:Kay Scanlon For The Times; Getty 

Load-Date: March 7, 2021


End of Document




How to erase the digital divide
Los Angeles Times
March 1, 2021 Monday
Final Edition


Copyright 2021 Los Angeles Times All Rights Reserved
Section: MAIN NEWS; Opinion Desk; Part A; Pg. 1
Length: 870 words
Byline: Meka Egwuekwe, Meka Egwuekwe is executive director of CodeCrew.
Body


The computer sci-ence industry is filled with talented techies, most of whom are white or Asian. Black professionals make up only 5% of the tech workforce, a segment that increased by only 1 percentage point between 2014 and 2020, according to a recent report. That's unacceptable.
Much is made of the digital divide, but little has been done to eradicate it.
Bias in artificial intelligence, its algorithms and its training data are direct byproducts of the divide. These biases are more dangerous than ever because they can impact decision-making on a massive scale and at breakneck speeds, hindering business in all sectors of the economy.
To help solve this problem, we need to get more underrepresented communities into careers in computing and engineering, especially data sci-ence. More, and different, perspectives can only help lead to better products and services.
At the same time, we can truly advance a Black and brown middle class, and create generational wealth, boosting economic growth and providing an entire new set of industries and opportunities across the nation.
It won't be easy to grow such a culture of tech producers in underrepresented communities, but it is absolutely doable.
The organization I lead, Memphis-based CodeCrew, and others such as the California-based Black Girls Code, are working to build that culture into the heart of Black and brown communities.
The potential economic impact is significant. Computer science graduates of CodeCrew's adult coding boot camp make an annual average starting salary of more than $51,000. Before their training, they made an average of just $15,000 per year.
That boost in income can be life-changing, providing a comfortable middle-class life in Memphis and many other parts of the country.
If we are to truly diversify the tech community we are going to need help -- a lot of it -- from the business and technology communities.
What could they do?
Waive the four-year college requirement for entry-level computer science positions at their companies. That is often a barrier for qualified CodeCrew grads.
Set goals and targets for diversity in workplaces, especially on engineering and leadership teams, and hold themselves publicly accountable by annually publishing how they are doing, as Netflix did recently. In three years, the company doubled the number of Black employees in its workforce and on its leadership team.
At the same time, companies should take an honest look at barriers in their workplaces that prevent them from retaining diverse talent and then eliminate discriminatory or unwelcoming environments and conditions. That could include establishing equitable pay and promotion opportunities, and initiating zero-tolerance policies for harassment, stereotypes or microaggressions.
Allow a small percentage of company time for employees to participate in mentoring programs that target underrepresented groups. WorldQuant Predictive's data scientists are working with CodeCrew students to bridge this gap and bring new skills to a diverse workforce. Google encourages employees to explore tech ideas not directly related to their assigned projects, a policy that has been credited with launching Gmail and Google News.
Why not have that same moonshot mentality with respect to diversity, equity and inclusion goals? Mentoring is a simple but highly effective means to transmit knowledge and skills.
Set up data science/software engineering offices in more regions where there are disproportionate populations of underrepresented groups -- including such places as Memphis and New Orleans.
Partner with historically Black colleges and universities, which have been working on these issues for a long time, despite being under-resourced. Then make sure to include these schools, along with diverse coding boot camps, as sources for talent -- and increase financial support for them.
Lobby the federal government to subsidize or otherwise make broadband internet access affordable in low-income communities, much like food is subsidized. Broadband is no longer a luxury, but a necessity, and should be treated as a utility, like electricity, gas and water.
Doing these things can create a new environment for data science and AI. But that is just a means to an overall greater outcome, which is building businesses by increasing the number of people of color in technology.
In doing so, we could become more internationally competitive with a rising China, advancing India, resurging Russia and unified European Union.
To bridge the digital divide, we need to catch future techies early. Every child should have access to computer science and computational thinking in school.
CodeCrew is working in Tennessee to help secure passage of legislation that requires every high school in the state to offer computer science classes and provide the necessary funding to train teachers to lead these classes.
Of the 500 kids CodeCrew works with each week, more than 90% are Black and Latinx youth. The vast majority of our students are more likely to go on to study computer science. With an assist from businesses and tech companies, overdue change like this could happen on a national level -- and businesses would ultimately be the biggest beneficiaries.

Load-Date: March 1, 2021


End of Document




CORONAVIRUS IN CALIFORNIA; We don't know the half of it; Did 50% of L.A. residents catch COVID-19? It all depends.
Los Angeles Times
March 5, 2021 Friday
Final Edition


Copyright 2021 Los Angeles Times All Rights Reserved
Section: CALIFORNIA; Metro Desk; Part B; Pg. 1
Length: 1106 words
Byline: SANDY BANKS
Body


I've grown accustomed to conflicting views when it comes to the pandemic.
We can gather in the library, but our kids can't go to school. I can finally get my hair done, but a facial is not allowed. You shouldn't wear a mask, you have to wear a mask, you really should be wearing two masks.
I understand the inconsistency. This virus is so new that all of us -- from CDC scientists to supermarket cashiers -- are still trying to navigate a steep learning curve.
And I like to think that nothing surprises me anymore. But then something comes along that shocks me all over again. Last week, it was the news about how many people locally already carry antibodies to the virus.
According to some estimates, as reported in The Times and elsewhere, as many as half of Los Angeles County's 10 million people have already been infected. And that's even though tests for COVID-19 have confirmed fewer than 1.2 million local infections.
The prospect of that many millions of uncounted infections seemed mind-boggling to me. How could more than 3 million people slip through our testing apparatus?
I wanted to know how those numbers were calculated, so I called L.A. County's chief prognosticator, Dr. Roger Lewis. His job is to quantify and model the spread of COVID-19, to help make sure that the county's hospital system is prepared to meet pandemic healthcare needs.
He is not surprised by the high immunity estimates, but he noted that calculations vary. "I've seen different estimates, from 1 in 4 to more than 50%," he said.
The county's official immunity estimate is near the middle of that spread, at approximately 3 in 8 people, or 37.5% of county residents. And that does not take into account the almost 2 million people who have received at least one dose of the vaccine.
Measuring immunity, it turns out, is more sophisticated that just counting positive tests and vaccinations. When the virus first emerged last year, California researchers began "antibody surveillance" -- tests of random people to check for COVID-19 antibodies.
"They found that the fraction of people with antibodies was much larger than the fraction of people who knew they'd been sick or people who'd tested positive," Dr. Lewis said. Many people with evidence of having been infected had never experienced symptoms of the illness; others may not have had access to tests.
Projecting that forward suggests there are now millions of uncounted COVID-19 survivors who were never tested, or at least not at a point when they would register as positive.
That early antibody testing clued researchers in to the phenomenon of asymptomatic infection and surreptitious spread. Further study of virus replication, infection demographics, hospitalizations, deaths and more led them to believe that approximately 40% of COVID-19 carriers will have no symptoms and may not show up in testing statistics.
That information established a baseline for measuring how many people might be immune -- and, by extension, how many others are still vulnerable to the disease.
But immunity estimates are also influenced by what assumptions researchers make and what trajectory they expect.
"If you talk to enough people, you're bound to get a mix of opinions," said Dr. George Rutherford, professor of epidemiology and biostatistics at UC San Francisco. "Some might [estimate] conservatively, and some are more gung-ho," he told me.
In other words, they all use the same basic formula, but the ingredients and measurements might differ a bit.
"We know the natural progression of this disease," explained David Conti, a USC professor of preventive medicine and specialist in data science integration. "We can describe that with a mathematical model."
But in real life "the number of new cases each day bounces around a lot," Conti said. "So as soon as we come up with a model, it's out of date."
The USC researchers' recent model offers a wide range of estimated cumulative infections, concluding that between 3 million and 5.5 million people in Los Angeles County have likely already been infected by COVID-19.
That translates to between 30% and 55% of the county's population. It's the higher number -- painting us as a hotbed of infection but also as a region drawing closer to herd immunity -- that has caught people's attention.
But that doesn't mean that half of the people you know have already been infected with COVID-19.
"Our model is on the level of the entire L.A. County," where infection rates vary drastically by neighborhood, USC researcher Abigail Horn said. "People want to take a number and make a statement about it. But it really is about the local communities ... and how this [pandemic] has amplified healthcare disparities."
That is a message we can't afford to forget. The burden of disease is distributed unevenly -- and a 50% immunity rate looks different depending on where you live. It's a worrying reflection of the dismal job we've done protecting working-class families from infection -- or it's an encouraging prospect, because the remaining pool of prospective virus spreaders is smaller than presumed.
Considering that sort of ambiguous messaging, I couldn't help but wonder how vested we should be in the barrage of statistics that accompanies our journey through COVID-19.
I put that question to Dr. Rutherford, who specializes in the study of infectious diseases. He thinks a break from tracking the stats may be just what people like me need. "When you're listening to all these statistics and opinions," he said, "people may lose the forest for the trees."
The "forest," to him, is the rapid creation of effective COVID-19 vaccines, which he sees as "the greatest miracle of modern molecular biology since the discovery of DNA," he says. "That's the overwhelming good news right now ... the culmination of 70 years of [scientific] advances."
The "trees" are the numbers we're being bombarded with. "There were 64,000 [research] papers published about COVID-19 by the end of October," he said. "There's a lot of information to digest."
We are not going to find the reassurance we seek in the minutiae of statistics. "A vaccine that is 90% effective, is the same as one that is 85%," Dr. Rutherford said. "You're not being cheated out of that 5%." Yet people are obsessing about which vaccine to get.
He reminded me that our health rests not on the numbers, but on the precautions we take. We may not know what number equals herd immunity, but we do know how to protect ourselves, the doctor says:
"Wear a mask, keep your distance from people outside your pod, avoid crowded indoor spaces. Don't go to Costco on a Saturday afternoon. And get vaccinated as soon as your turn comes."

Graphic

 
PHOTO: NURSE Marijorie Tabago administers a COVID-19 test on Mercedes Madrano at an Ontario testing site.  PHOTOGRAPHER:Irfan Khan Los Angeles Times PHOTO: NURSE Celeste Montoya, left, inoculates Winnetka resident Ana Vidales Torres as her daughter Talia, 6, gets a close-up look at a mass vaccination site at Valley Crossroads Seventh-day Adventist Church in Pacoima.  PHOTOGRAPHER:Mel Melcon Los Angeles Times PHOTO: DRIVERS remain in their vehicles while being monitored to ensure they suffer no adverse reaction to the COVID-19 vaccine at the Forum in Inglewood in January.  PHOTOGRAPHER:Al Seib Los Angeles Times 

Load-Date: March 5, 2021


End of Document




UCSD Researchers Use Map To Reveal How Resources Are Distributed In The Brain
City News Service
March 2, 2021 Tuesday 2:23 PM PST


Copyright 2021 City News Service, Inc. All Rights Reserved
No City News Service material may be republished without the express written permission of the City News Service, Inc.
Length: 589 words
Body


SAN DIEGO (CNS) - Scientists at UC San Diego may have helped answer how the brain ensures more active parts receive enough nourishment versus less demanding areas -- an old neuroscience mystery, according to a study published today.
Studying the brains of mice, a team of researchers led by UCSD researchers Xiang Ji, David Kleinfeld and their colleagues deciphered the question of brain energy consumption and blood vessel density through maps detailing brain wiring to a resolution finer than a millionth of a meter -- or one-hundredth of the thickness of a human hair.
A result of work at the crossroads of biology and physics, the maps provide novel insights into these "microvessels" and their various functions in blood supply chains. The techniques and technologies underlying the results are described in Tuesday's edition of the journal Neuron.
"We developed an experimental and computational pipeline to label, image and reconstruct the microvascular system in whole mouse brains with unprecedented completeness and precision," said Kleinfeld, a professor in UCSD's department of physics and section of neurobiology.
Kleinfeld says the effort was akin to reverse engineering nature.
"This allowed Xiang to carry out sophisticated calculations that not just related brain energy use to vessel density, but also predicted a tipping point between the loss of brain capillaries and a sudden drop in brain health," he said.
Questions surrounding how blood vessels carry nourishment to active and less active regions of the brain were posed as a general issue in physiology as far back as 1920. By the 1980s, a technology known as autoradiography, the predecessor of modern-day positron emission tomography, allowed scientists to measure the distribution of sugar metabolism across the mouse brain.
To fully grasp and solve the problem, Ji, Kleinfeld and their colleagues at the Howard Hughes Medical Institute's Janelia Research Campus and UCSD Jacobs School of Engineering filled 99.9% of the vessels in the mouse brain -- nearly 6.5 million vessels -- with a dye-labeled gel.
They then imaged the full extent of the brain with sub-micrometer precision, which resulted in fifteen trillion voxels, or individual volumetric elements, per brain, that were transformed into a digital vascular network that could be analyzed with the tools of data science.
With their new maps in hand, the researchers determined that the concentration of oxygen is roughly the same in every region of the brain. But they found that small blood vessels are the key components that compensate for varying energy requirements.
For example, white matter tracts, which transfer nerve impulses across the two brain hemispheres and to the spinal cord, are regions of low energy needs. The researchers identified lower levels of blood vessels there. By contrast, brain regions that coordinate the perception of sound use three times more energy and, they discovered, were found with a much greater level of blood vessel density.
"In the era of increasing complexities being unraveled in biological systems, it is fascinating to observe the emergence of shared simple and quantitative design rules underlaying the seemingly complicated networks across mammalian brains," said Ji, a graduate student in physics.
Next, the researchers hope to drill down into the finer aspects of their new maps to determine the detailed patterns of blood flow into and out from the entire brain. They will also pursue the largely uncharted relationship between the brain and the immune system.

Load-Date: March 3, 2021


End of Document




CITY BEAT; A tail-wagging tale of man and corgi
Los Angeles Times
January 30, 2021 Saturday
Final Edition


Copyright 2021 Los Angeles Times All Rights Reserved
Section: CALIFORNIA; Metro Desk; Part B; Pg. 1
Length: 1471 words
Byline: NITA LELYVELD
Body


Nearly a year ago, I told you about a small act of kindness to a homeless man and his corgi that had grown into something much larger for all those it touched.
It pulled at a lot of your heartstrings -- in good part, I think, because dog love runs deep, and corgi lovers are particularly ardent.
I told you about the man and his corgi and the couple who had stepped in to help them early last March, less than two weeks before COVID-19 shut down California.
And, despite how many more people have found themselves in dire straits in the ensuing crisis, I've never stopped hearing from those eager for the latest on how dog and man were faring.
I'm glad to say that the update I'm bringing you at long last may offer some respite from the current grim news. It gives me pleasure to say, too, that much of the good I have to share springs directly from the generosity of readers.
Still, that's really not my reason for telling it. I do so in this time of widespread need to reiterate the central point I set out to make in my first corgi column -- about the enormous potential benefit to both givers and receivers of doing something, however little, to help others.
But first, a recap.
The corgi I introduced you to last March had been having a hard time since his person lost his good job, then his home. Over four tough years, man and dog had ended up homeless -- and neither had adjusted easily.
The corgi's person had a PhD and had worked for years in a highly specialized field with few job openings. He was earning six figures in the job he lost, and when he couldn't find another one, his circumstances steadily worsened until he and his corgi no longer had a place to live. They bounced around, with stops at a group home and a single-room occupancy hotel on skid row in downtown L.A.
The man started driving for Uber while he tried to teach himself skills for a new line of work. When he no longer could make payments on his car, he rented cars from Uber by the hour -- working longer for less gain, with less time to spend prepping for a new career and less chance of climbing out of his hole.
And all the while, as he reached out to social-service organizations for short-term emergency housing, many doors remained closed because he had a dog.
Which is where the initial act of kindness came in.
Loath to be alone, but desperate, the man wrote a Craigslist post in the fall of 2019 asking for someone to step in and help him by fostering his corgi for a few months. A friend of Ted and Sandy Rogers of Hollywood forwarded the post to them, thinking that helping the dog might also help them as they grieved the loss late that summer of their own beloved corgi, Sienna, due to cancer.
Ted has diabetes, and Sienna alerted him with licks when his blood sugar dipped low. Sienna had been the center of their lives.
The foster corgi they took into their home was traumatized from all the instability. Ted and Sandy made it their mission to restore his sense of calm and security.
Meanwhile, the corgi's person showed up on Sunday afternoons to visit -- and while he and his dog cuddled and played, he gradually opened up and told Ted and Sandy all that he'd been through. He also needed their emotional support, and they gave it to him.
The corgi's person, I have to stop and say here, continues to feel a good deal of shame for his fall. Even now, after the pandemic has imploded so many other previously secure lives. For that reason, he continues to ask me to maintain his anonymity -- which I do, as I pray that he will stop beating himself up over time.
When I wrote the first column about him, he was still searching for a job, still separated from his corgi. I asked if anyone could help him find a stable home or a job in data science, the field he was trying to enter. I also said I hoped that soon he and his corgi would be reunited and that Ted and Sandy would find a new dog to love.
I put those wishes out to the universe, which instantly began delivering.
Some people sent the corgi's person loving notes, telling him that what had happened to him could happen to any of us. Others donated to a GoFundMe campaign set up to make his life easier. And within hours of the column's appearance, multiple people had written with job leads and offers to help him with housing.
One of the first came from Mike Kilroy, a real estate investor who offered the corgi and his person -- sight unseen, no strings attached -- a studio apartment in Palm Springs, rent-free for a year. Whenever Ted and Sandy wanted to visit, he said, he'd give them a free place to stay, too -- and the same for me if I wanted to come see how dog and man were doing.
"It was something I could do, so why not?" Kilroy told me simply when I asked him about it this week. "It's all well and good to talk about solving your problems and moving ahead. But you can't do that without a stable base."
The warmth of the offer proved irresistible, even if it meant relocation. On March 22, just a few days after California's stay-at-home order kicked in, the man went to get his corgi from Ted and Sandy and take him to their new Palm Springs home.
Around the same time, he got a job offer in data science. And even though he was told that, because of closures, it would not start until May, he had more than $4,000 in GoFundMe donations to help tide him over until his first paycheck.
Ted and Sandy, meanwhile, found themselves once again dogless and missing their foster corgi terribly.
Then Sandy lost the job she'd had for 18 years, doing accounts payable for a suddenly shuttered office. (Any leads, anyone? You've come through before.)
Their stint at fostering had readied them for a new dog -- and they wanted to rescue a corgi in need. But once the pandemic hit, people everywhere suddenly seemed to want a pet for companionship. When their corgi rescue searches came up empty, they started looking at other breeds and on the websites of corgi breeders.
Many corgi enthusiasts reached out.
Ted and Sandy felt like rock stars as their fame spread online through corgi groups. One owner who had read the column recognized Ted, Sandy and the foster corgi at Trader Joe's. Another who works in tech started trying simultaneously to find a job for their foster corgi's person and a corgi for Ted and Sandy.
Julie, who for privacy reasons asked that I use her first name only, said she's active with a local corgi rescue group that, pre-pandemic, organized frequent social hikes. She'd also researched breeders when looking for her own corgi, Truffle (who has her own Instagram profile), and had learned of one in Wyoming who is the friend of a friend she'd met running overnight relay races.
One Saturday in July, she saw a newly listed corgi puppy on that Wyoming breeder's site and immediately told Ted and Sandy. They put down a deposit that day.
And because Ted and Sandy don't drive and feared public transportation because of the coronavirus, Julie and her running friend did a relay of sorts to bring the 3-month-old corgi to them.
Julie's friend picked the puppy up in Wyoming and drove her to Utah. Julie rented a car and drove from L.A. to Utah to bring the dog to Ted and Sandy. Ted, Sandy and Julie met for the first time in person on delivery -- and realized they were neighbors. Now Ted and Sandy's puppy -- Tazzy, short for Tasmanian Devil -- sometimes meets Truffle for neighborhood play dates.
I met Tazzy the other day. She smooched me, as she smooches everyone she meets, person and dog alike. She zooms around Ted and Sandy's apartment, causes havoc, destroys toys and delivers endless joy. Ted and Sandy are thoroughly smitten, even though she likes to pull Sandy's hair.
As for dog and man in Palm Springs, the corgi's person sent me a selfie he took when they moved back in together that could serve as a visual definition of love. It hasn't all been easy. He initially felt lonely in a new city in a pandemic. His first job didn't last because he failed to get security clearance within six months -- likely due to the many moves in his recent history.
But then a friend in his old field asked him for a job reference -- and he asked her if she'd return the favor for the job she was leaving. He got that job. He's now back to six figures. He credits his rent-free year with giving him the grounding to be capable of leaping at the chance when offered. He, Ted and Sandy text back and forth every day. He paid the expenses for Tazzy's travels from Wyoming to Hollywood. He credits Ted and Sandy with rescuing him just as much as they rescued his corgi.
And he credits every single person who offered up words of encouragement, donations, positive thoughts, advice and assistance with playing a part in his comeback. He said he wished I could name every name.
Alas, I can't -- I can't even name his -- but I offer my thanks to all who helped him in so many ways.

Graphic

 
PHOTO: TED AND SANDY ROGERS walk their new corgi, Tazzy, in Hollywood. The couple had previously lost their 13-year-old corgi and fostered one for a man who was jobless and homeless but has gotten back on his feet.  PHOTOGRAPHER:Wally Skalij Los Angeles Times 

Load-Date: January 30, 2021


End of Document




D.A.'s New Investigative Chief Sworn In
City News Service
February 1, 2021 Monday 7:49 PM PST


Copyright 2021 City News Service, Inc. All Rights Reserved
No City News Service material may be republished without the express written permission of the City News Service, Inc.
Length: 266 words
Body


LOS ANGELES (CNS) - Robert Arcos was sworn in today as the head of the Los Angeles County District Attorney's Office Bureau of Investigation, the county's fourth largest law enforcement agency.
Arcos will lead nearly 300 sworn officers supporting the roughly 1,000 deputy district attorneys.
Arcos was an assistant chief of the Los Angeles Police Department and has more than 30 years of policing experience. He joined the department in 1988 and most recently served as director of LAPD's Office of Operations.
"Bobby Arcos brings a depth of law enforcement experience and compassion to the District Attorney's Office," said District Attorney George Gascon, who administered the oath to Arcos.
"I am proud that Bobby has agreed to join my executive management team and will help me fulfill my promises to voters that we will protect our community by preventing crime, reducing recidivism and restoring victims. Together, with other members of the team, we will rely on data, science and research to guide this office into the future."
Gascon described Arcos as a reform-minded leader who created lasting relationships with neighborhood advocates, nonprofit organizations, the business community and civic leaders.
Gascon's executive management team also includes interim Chief Deputy District Attorney Joseph Iniguez, Chief of Staff Victoria Adams, Chief Administrative Officer Dorinne Jordan, Assistant District Attorney for Line Operations James Garrison, Special Advisors to the District Attorney Diana Teran, Alisa Blair and Alex Bastian and Community and Government Affairs Liaison Mario Trujillo.

Load-Date: February 2, 2021


End of Document




Is it time to kill calculus?
Salon.com
September 26, 2020 Saturday


Copyright 2020 Salon.com, LLC. All Rights Reserved
Length: 3138 words
Highlight: Math curricula are designed to shepherd students toward calculus. Some mathematicians think this path is outdated


Body


Link to Image
Math formulas are written on the school board Getty Images
Many parents relish reliving moments from our childhoods through our children, and doing homework with them is its own kind of madeleine. For Steve Levitt of "Freakonomics" fame - who is, in his own words, "someone who uses a lot of math in my everyday life" - a trip down memory lane vis-a-vis math homework became a moment of frustrated incredulity rather than gauzy reverie. "Perhaps the single most important development over the last 50 years has been the rise of data and computers, and yet the curriculum my children were learning seemed to have been air-dropped directly from my own childhood," he told me. "I couldn't see anything different about what they were learning than what I learned, even though the world had transformed completely. And that didn't make sense."
Levitt has made a career of questioning the received dogma. In this case, what he saw was that "A mathematical way of thinking, numeracy, data literacy, is far more important today than it has been; the ability to visualize data, the ability to make sense out of a pile of numbers, has never been more important, but you wouldn't know that from looking at the math curriculum." Data combined with the use of mathematical ideas had transformed the way he and others look at the world. Should data also change the way we teach mathematics?
In most schools, children are grounded in basic arithmetic in elementary school, and then, somewhere between middle school and high school, force-fed the "algebra-geometry-algebra sandwich". The first year of algebra ("Algebra I") continues to reinforce basic arithmetic, and then brings in fractions. The familiar starts to give way to the unfamiliar when variables and functions are introduced. That's when "x the unknown" makes its first appearance in word problems and linear equations, which for many marks a first sign of confusion rather than buried epistemological treasure.
Things then take a big turn, and math class time-travels to the days of ancient Greece for lessons in formal geometric proofs ("Geometry") that Euclid would have little trouble stepping in to substitute teach. Following that is a yearlong return to algebra ("Algebra II: The Sequel!"), which given the previous year's partial hiatus from x's and y's and numbers first requires a lengthy review and then finally a return to new functions (exponentials, logarithms, polynomials) that either amuse or irritate you, depending on your taste, predilections, and teacher.
For some math stops here. For others there is often an honors track that speeds things up. Increasingly, honors or not, students get to pre-calculus or calculus, which is often revisited in the first year of college, and is the last bit of formal math a person will ever taste. Apologies to the reader for any unhappy flashbacks - or indigestion - incurred.
The sandwich - and actually the entire mathematical meal - has had a long shelf-life. If Levitt felt like his kids were air-dropped into his childhood math classes, odds are this was true for his parents too. The origins of the curriculum go back to a famous 1892 "Committee of Ten" that met at the behest of the National Education Association to standardize public education. Like any good committee their first act was to create more committees - nine to be exact - each tasked with the consideration of a "principal subject which enters into the programmes of secondary schools in the United States and into the requirements for admission to college". Each of these subcommittees then considered "the proper limits of its subject, the best methods of instruction, the most desirable allotment of time for the subject, and the best methods of testing the pupils' attainments therein." Mathematics was one of them. So was Latin and Greek. 
Pre-college mathematics education at that time was like today, composed of arithmetic, geometry, and algebra. The committee had recommendations for each of these areas. The teaching of arithmetic should be "abridged by omitting entirely those subjects which perplex and exhaust the pupil without affording any really valuable mental discipline, and enriched by a greater number of exercises in simple calculation and in the solution of concrete problems." That could stand as a mission statement today for the teaching of any kind of mathematics. "Concrete geometry" would be part of grammar school mathematics and combined with drawing. The committee also recommended that all students, regardless of their aspirations have the same kind of mathematics education up through the first year of algebra. After that, distinctions start to emerge depending on your goals: trigonometry and more algebra for those going to "scientific schools", "commercial arithmetic" for those thinking of a business career.
While "the pupil who solves a difficult problem in brokerage" may still learn some good mathematics,  "The movements of a race horse afford a better model of improving exercise than those of the ox on a treadmill." Setting aside the confusing animal analogy (okay, an ox is slow, but it is really strong and why are you putting it on a treadmill?) the spirit of the metaphor reflects something of a general tension in education: how much do we teach for the world as it is today and how much for the unknown tomorrow? Specific applications or general principles?
This is a tension that is perhaps felt most keenly in the teaching of mathematics and has led to something of a back and forth in mathematics education. Among the most well-known attempts to revamp the mathematics classroom was the move in the 1960s to the "new math", which was a reaction on the part of mathematicians - and some math teachers - that mathematics teaching had become too utilitarian, a curricular decision made decades earlier, at least in part because it was observed that our soldiers in World War II were lacking in basic mathematical skills. The energy behind that revamping of mathematics teaching was the Space Race, initiated by the surprising launch of Sputnik and a perceived "math gap" that would have to be closed in order for the United States maintain international supremacy. Getting people into space and beyond the clouds would mean that we needed to start teaching a kind of math that was already in the clouds. The "New Math" would strip mathematics to its roots, going as far down as basic set theory - Venn diagrams - and rebuild the world of numbers from the ground up.
By most accounts the program was a failure, sacrificing a direct inculcation of basic skills for the goal of exploring highly abstract general concepts, which while not completely disconnected from the day-to-day world of basic arithmetic and problem solving, was about as distant from it as the satellites it was supposed to help launch. Standards-based education and the Common Core followed soon after.
The uneasy relationship between applications and theory in the development of mathematics  curriculum is a reflection of  an ongoing - if slowly healing - rift within the discipline itself. There is pure mathematics and applied mathematics, the former a creation of mathematics for its own sake as opposed to in the service of solving a problem that is troubling someone in the real world. "Applied" is better than "impure," I guess, although it's an adjective that has ugly historical and elitist overtones. To the extent that engineering is a craft, it is a bias that one might trace back to the distinction between the scholar and the craftsperson, the university and the guild or technical school. In truth, a genetic family tree of math would show all kinds of connections and surprising worldly origins of even the "purest" mathematics.
Levitt's call for a mathematics curriculum centered around data is not born of anti-intellectualism. He is quick to point out that he is not "anti-math", rather that he is "anti-math-that-no-one-will-ever-use-in-the-first-place," at least in the classroom. As someone who loves math, he worries that a mathematics curriculum not connected to data and computers "runs the risk of being demoted." "If the best arguments  for mathematics is that it's part of the education of a well-rounded citizen and that it's good for brain development," that may very well be the undoing of math.
By his own admission, he is a Johnny-come-lately to the challenge of curriculum reform, but his academic star-power has enabled him to attract important and influential players to his mission of bringing data and computing to mathematics education. Levitt quickly stood up a small advisory committee of like-minded people that included statistics celebrity Nate Silver of 538.com, former Secretary of Education Arne Duncan, and former Google CEO Eric Schmidt.
Levitt's most important recruit may be Jo Boaler, the Nomellini & Olivier Professor of Education at Stanford. Boaler also directs YouCubed, a non-profit whose mission is to "inspire, educate and empower teachers of mathematics." Her work on mathematics education is widely cited and influential. The call she received from Levitt came at a perfect time, as she is currently working with California's Department of Education to revamp the mathematics curriculum around data.
For Boaler, the sclerotic nature of the mathematics curriculum is above all an equity issue, and for that she places calculus at the center. As Boaler points out, mathematics is usually the only subject in which kids - usually 6th graders - take a placement exam in middle school, the result of which sets them on their academic pathway through high school, on track - usually an "honors" track - to take calculus junior year of high school.
The curriculum as calculus funnel and an honors track to speed one's ride has downstream effects. My own home institution Dartmouth College is almost surely not an outlier among its peer group in that, while calculus is not required for admission, you would be hard-pressed to find a student here who didn't take calculus in high school. It's only the students who have made it through the initial placement - in sixth grade! - who have the ability to show colleges that they can succeed in calculus. For Boaler, calculus is a linchpin, not in and of itself, but because of the influence it has both on the curriculum that precedes it as well as its influence on students' college prospects moving forward.
Despite what some may think, calculus didn't end up as the last stop on the math track just to create a final hurdle for high schoolers. It was for many years, the most applicable math - outside of arithmetic - that you could find. It continues to be of great importance in all kinds of applied contexts, from medicine to engineering to finance, where modeling change - usually in the form of a "differential equation" - is crucial. It is mainly useful in continuous contexts (think fluids or asteroids in motion) and powerful for finding "analytic solutions" (formulas) that quantify a phenomenon indirectly encoded in a differential equation. All of this is still true, except what happened is that many new and interesting phenomena also started to be represented by numbers - i.e., the data revolution occurred. This didn't mean calculus became irrelevant, rather other important possibilities arose for mathematical thinking and learning and teaching.
Boaler calls calculus a "horrible and inequitable filter." Some of the inequity is around gender - placement testing preferences boys over girls, a finding that may be something of a surprise to many. Equity issues may also be redounding to the academy. A calculus-successful student body may very well be contributing to an over-representation in STEM in its entering classes, or rather that an underrepresentation or under-cultivation of humanities interests. Students have only so much time to take classes in high school and only so much energy. A history-interested student may very well be taking yet another difficult math course instead of another history or government or art course simply because she knows - or believes - that she has to wrap her head around calculus, which doesn't have great tangencies to her intellectual passions.
"Data science" is a name invented to distinguish the ideas and approaches used to analyze the new diversity and quantities of data that characterize modern data from those of classical statistics.  Done well, it is an integration of critical thinking and quantitative skills, storytelling with and through numbers, supported by evidence. It has strong connections to the humanities, both in spirit and practice, as many interesting kinds of data analyses are regularly performed on information derived from humanities subjects, often in digital humanities programs. The humanities context is reflexive, too: it's no coincidence that the new important work now being done in data bias came out of digital humanities programs. I'd wager that a student who excels at and enjoys data science is more likely to also have interest in and see beauty in the activity of close-reading a text, or image, or artifact, or working in the humanities more generally.
Data science is also highly collaborative, which a good deal of research shows is a working style where girls (and women) excel. Some of the same studies that show boys outperforming girls on timed tests, show girls outperforming boys when the tests have a collaborative framing. If data science were a part of a high school curriculum it could provide a mechanism for girls to show their quantitative skills and it also could be a boost to humanities programs as well. It would in short allow more students to showcase their talents to colleges in ways that could benefit both students and colleges.
Last March, Boaler and Levitt convened a Data Science Summit at Stanford. What was originally supposed to be a small working group soon mushroomed into a large meeting of more than fifty people that included well known mathematics educators, representatives from industry, and mathematicians. By the end of the Stanford meeting, rather than being energized by the day, Levitt was depressed. There were all these smart people in the room, committed to the idea of changing the curriculum, but all having different ideas. "It didn't seem like something that would happen in my lifetime," he says.
In fact, there already has been some substantial progress made in bringing data science to the schools. Notable is the Introduction to Data Science course that was co-developed by UCLA and the Los Angeles Unified School District and is already being rolled out in 15 southern California School Districts. In addition to a curriculum there is a professional development arm to help teachers acquire the skills to teach it.
In addition to explicit materials and courses like this, data is also making its way into the curriculum in more subtle ways. As part of thoughtful redesign twenty percent of the SAT now tests the ability of prospective college students to understand data, both in the quantitative and verbal parts of the test, the latter of which includes data in the reading comprehension piece. This is all part of concerted effort by the College Board CEO David Coleman to make the SAT more relevant to what is actually being taught in the "average" first year of college. Even more, from Coleman's perspective, if kids were going to be studying for the SAT, then that studying should be worthwhile even beyond its relevance to college admissions. Data literacy is a part of that mission.
The recognition of the centrality of data is also a part of a next generation of AP courses. The new AP Biology course has been redesigned to have a significant data analysis component. The AP footprint in computer science has been expanded to include an AP "Principles of Computer Science" course that focuses on data science and as such provides context for the a next programming course. Coleman is especially proud of the Principles course, as it has proved to be a gateway course for computing that is especially attractive to demographics that historically have been under-represented in computer science. Since its roll-out in 2016, the numbers of female, black, LatinX, and rural examinees have grown by 136%, 121%, 125% and 117% respectively. And in the first year after the Principles course was made available, enrollments in the well-known AP programming course doubled, with attendant and sustained increases in each of these populations in subsequent years. "the changes in who is doing computer science is something I'm really proud of," Coleman says. For Coleman, data science is a pathway to STEM diversity.
Another piece is a new initiative of Coleman's that the College Board is calling its "pre-AP curriculum". This fall "Pre-AP Geometry with Statistics" is being piloted around the country. It is a quarter the basics of data science with the rest basic geometry. The bridging conceit that both are contexts for deductive reasoning and the course joins the certainties of deduction with the probabilities of data science. A new Pre-AP Algebra II course will also have data analysis connections inserted through the appearance of functions with more than one variable.
Work like that being done at the College Board and other places does give Levitt some hope that math curricula will change - if not for his kids, then at least for his kids' kids. What Levitt, Boaler, and many others support - possibly as a short-term fix, but at least as a step forward - is the idea of streamlining the current curriculum. While it's not exactly as simple as "cutting two textbooks in half and gluing them together to make a new course," as Levitt says, there is something to that. It's the kind of thing that he and others could imagine organizing a group of mathematicians, and data scientists around to find a way to remove a year from the AGA sandwich.
A newly streamlined curriculum would then give space for a year of data exploration and integration of computing, maybe even more mathematics exploration - again, assisted by computing. A modification may also may put less strain on any requisite teacher training than a complete rewrite. It leaves open the possibility of a math curriculum that would be relevant for all the students, with a branchpoint that would depend upon interests: algebra, geometry/algebra+data followed either data science or calculus, or both! It would be better connected - maybe with the help of ideas from the new pre-AP courses - to the overall curriculum and in that, also possibly serve the purpose of getting more kids with a range of interests and abilities interested in mathematics. The devil is in the details, but this is the kind of near-term and seemingly achievable goal that Levitt, Boaler and others are now working toward.

Load-Date: September 26, 2020


End of Document




Is it time to kill calculus?
Salon.com
September 26, 2020 Saturday


Copyright 2020 Salon.com, LLC. All Rights Reserved
Length: 3138 words
Highlight: Math curricula are designed to shepherd students toward calculus. Some mathematicians think this path is outdated


Body


Link to Image
Math formulas are written on the school board Getty Images
Many parents relish reliving moments from our childhoods through our children, and doing homework with them is its own kind of madeleine. For Steve Levitt of "Freakonomics" fame - who is, in his own words, "someone who uses a lot of math in my everyday life" - a trip down memory lane vis-a-vis math homework became a moment of frustrated incredulity rather than gauzy reverie. "Perhaps the single most important development over the last 50 years has been the rise of data and computers, and yet the curriculum my children were learning seemed to have been air-dropped directly from my own childhood," he told me. "I couldn't see anything different about what they were learning than what I learned, even though the world had transformed completely. And that didn't make sense."
Levitt has made a career of questioning the received dogma. In this case, what he saw was that "A mathematical way of thinking, numeracy, data literacy, is far more important today than it has been; the ability to visualize data, the ability to make sense out of a pile of numbers, has never been more important, but you wouldn't know that from looking at the math curriculum." Data combined with the use of mathematical ideas had transformed the way he and others look at the world. Should data also change the way we teach mathematics?
In most schools, children are grounded in basic arithmetic in elementary school, and then, somewhere between middle school and high school, force-fed the "algebra-geometry-algebra sandwich". The first year of algebra ("Algebra I") continues to reinforce basic arithmetic, and then brings in fractions. The familiar starts to give way to the unfamiliar when variables and functions are introduced. That's when "x the unknown" makes its first appearance in word problems and linear equations, which for many marks a first sign of confusion rather than buried epistemological treasure.
Things then take a big turn, and math class time-travels to the days of ancient Greece for lessons in formal geometric proofs ("Geometry") that Euclid would have little trouble stepping in to substitute teach. Following that is a yearlong return to algebra ("Algebra II: The Sequel!"), which given the previous year's partial hiatus from x's and y's and numbers first requires a lengthy review and then finally a return to new functions (exponentials, logarithms, polynomials) that either amuse or irritate you, depending on your taste, predilections, and teacher.
For some math stops here. For others there is often an honors track that speeds things up. Increasingly, honors or not, students get to pre-calculus or calculus, which is often revisited in the first year of college, and is the last bit of formal math a person will ever taste. Apologies to the reader for any unhappy flashbacks - or indigestion - incurred.
The sandwich - and actually the entire mathematical meal - has had a long shelf-life. If Levitt felt like his kids were air-dropped into his childhood math classes, odds are this was true for his parents too. The origins of the curriculum go back to a famous 1892 "Committee of Ten" that met at the behest of the National Education Association to standardize public education. Like any good committee their first act was to create more committees - nine to be exact - each tasked with the consideration of a "principal subject which enters into the programmes of secondary schools in the United States and into the requirements for admission to college". Each of these subcommittees then considered "the proper limits of its subject, the best methods of instruction, the most desirable allotment of time for the subject, and the best methods of testing the pupils' attainments therein." Mathematics was one of them. So was Latin and Greek. 
Pre-college mathematics education at that time was like today, composed of arithmetic, geometry, and algebra. The committee had recommendations for each of these areas. The teaching of arithmetic should be "abridged by omitting entirely those subjects which perplex and exhaust the pupil without affording any really valuable mental discipline, and enriched by a greater number of exercises in simple calculation and in the solution of concrete problems." That could stand as a mission statement today for the teaching of any kind of mathematics. "Concrete geometry" would be part of grammar school mathematics and combined with drawing. The committee also recommended that all students, regardless of their aspirations have the same kind of mathematics education up through the first year of algebra. After that, distinctions start to emerge depending on your goals: trigonometry and more algebra for those going to "scientific schools", "commercial arithmetic" for those thinking of a business career.
While "the pupil who solves a difficult problem in brokerage" may still learn some good mathematics,  "The movements of a race horse afford a better model of improving exercise than those of the ox on a treadmill." Setting aside the confusing animal analogy (okay, an ox is slow, but it is really strong and why are you putting it on a treadmill?) the spirit of the metaphor reflects something of a general tension in education: how much do we teach for the world as it is today and how much for the unknown tomorrow? Specific applications or general principles?
This is a tension that is perhaps felt most keenly in the teaching of mathematics and has led to something of a back and forth in mathematics education. Among the most well-known attempts to revamp the mathematics classroom was the move in the 1960s to the "new math", which was a reaction on the part of mathematicians - and some math teachers - that mathematics teaching had become too utilitarian, a curricular decision made decades earlier, at least in part because it was observed that our soldiers in World War II were lacking in basic mathematical skills. The energy behind that revamping of mathematics teaching was the Space Race, initiated by the surprising launch of Sputnik and a perceived "math gap" that would have to be closed in order for the United States maintain international supremacy. Getting people into space and beyond the clouds would mean that we needed to start teaching a kind of math that was already in the clouds. The "New Math" would strip mathematics to its roots, going as far down as basic set theory - Venn diagrams - and rebuild the world of numbers from the ground up.
By most accounts the program was a failure, sacrificing a direct inculcation of basic skills for the goal of exploring highly abstract general concepts, which while not completely disconnected from the day-to-day world of basic arithmetic and problem solving, was about as distant from it as the satellites it was supposed to help launch. Standards-based education and the Common Core followed soon after.
The uneasy relationship between applications and theory in the development of mathematics  curriculum is a reflection of  an ongoing - if slowly healing - rift within the discipline itself. There is pure mathematics and applied mathematics, the former a creation of mathematics for its own sake as opposed to in the service of solving a problem that is troubling someone in the real world. "Applied" is better than "impure," I guess, although it's an adjective that has ugly historical and elitist overtones. To the extent that engineering is a craft, it is a bias that one might trace back to the distinction between the scholar and the craftsperson, the university and the guild or technical school. In truth, a genetic family tree of math would show all kinds of connections and surprising worldly origins of even the "purest" mathematics.
Levitt's call for a mathematics curriculum centered around data is not born of anti-intellectualism. He is quick to point out that he is not "anti-math", rather that he is "anti-math-that-no-one-will-ever-use-in-the-first-place," at least in the classroom. As someone who loves math, he worries that a mathematics curriculum not connected to data and computers "runs the risk of being demoted." "If the best arguments  for mathematics is that it's part of the education of a well-rounded citizen and that it's good for brain development," that may very well be the undoing of math.
By his own admission, he is a Johnny-come-lately to the challenge of curriculum reform, but his academic star-power has enabled him to attract important and influential players to his mission of bringing data and computing to mathematics education. Levitt quickly stood up a small advisory committee of like-minded people that included statistics celebrity Nate Silver of 538.com, former Secretary of Education Arne Duncan, and former Google CEO Eric Schmidt.
Levitt's most important recruit may be Jo Boaler, the Nomellini & Olivier Professor of Education at Stanford. Boaler also directs YouCubed, a non-profit whose mission is to "inspire, educate and empower teachers of mathematics." Her work on mathematics education is widely cited and influential. The call she received from Levitt came at a perfect time, as she is currently working with California's Department of Education to revamp the mathematics curriculum around data.
For Boaler, the sclerotic nature of the mathematics curriculum is above all an equity issue, and for that she places calculus at the center. As Boaler points out, mathematics is usually the only subject in which kids - usually 6th graders - take a placement exam in middle school, the result of which sets them on their academic pathway through high school, on track - usually an "honors" track - to take calculus junior year of high school.
The curriculum as calculus funnel and an honors track to speed one's ride has downstream effects. My own home institution Dartmouth College is almost surely not an outlier among its peer group in that, while calculus is not required for admission, you would be hard-pressed to find a student here who didn't take calculus in high school. It's only the students who have made it through the initial placement - in sixth grade! - who have the ability to show colleges that they can succeed in calculus. For Boaler, calculus is a linchpin, not in and of itself, but because of the influence it has both on the curriculum that precedes it as well as its influence on students' college prospects moving forward.
Despite what some may think, calculus didn't end up as the last stop on the math track just to create a final hurdle for high schoolers. It was for many years, the most applicable math - outside of arithmetic - that you could find. It continues to be of great importance in all kinds of applied contexts, from medicine to engineering to finance, where modeling change - usually in the form of a "differential equation" - is crucial. It is mainly useful in continuous contexts (think fluids or asteroids in motion) and powerful for finding "analytic solutions" (formulas) that quantify a phenomenon indirectly encoded in a differential equation. All of this is still true, except what happened is that many new and interesting phenomena also started to be represented by numbers - i.e., the data revolution occurred. This didn't mean calculus became irrelevant, rather other important possibilities arose for mathematical thinking and learning and teaching.
Boaler calls calculus a "horrible and inequitable filter." Some of the inequity is around gender - placement testing preferences boys over girls, a finding that may be something of a surprise to many. Equity issues may also be redounding to the academy. A calculus-successful student body may very well be contributing to an over-representation in STEM in its entering classes, or rather that an underrepresentation or under-cultivation of humanities interests. Students have only so much time to take classes in high school and only so much energy. A history-interested student may very well be taking yet another difficult math course instead of another history or government or art course simply because she knows - or believes - that she has to wrap her head around calculus, which doesn't have great tangencies to her intellectual passions.
"Data science" is a name invented to distinguish the ideas and approaches used to analyze the new diversity and quantities of data that characterize modern data from those of classical statistics.  Done well, it is an integration of critical thinking and quantitative skills, storytelling with and through numbers, supported by evidence. It has strong connections to the humanities, both in spirit and practice, as many interesting kinds of data analyses are regularly performed on information derived from humanities subjects, often in digital humanities programs. The humanities context is reflexive, too: it's no coincidence that the new important work now being done in data bias came out of digital humanities programs. I'd wager that a student who excels at and enjoys data science is more likely to also have interest in and see beauty in the activity of close-reading a text, or image, or artifact, or working in the humanities more generally.
Data science is also highly collaborative, which a good deal of research shows is a working style where girls (and women) excel. Some of the same studies that show boys outperforming girls on timed tests, show girls outperforming boys when the tests have a collaborative framing. If data science were a part of a high school curriculum it could provide a mechanism for girls to show their quantitative skills and it also could be a boost to humanities programs as well. It would in short allow more students to showcase their talents to colleges in ways that could benefit both students and colleges.
Last March, Boaler and Levitt convened a Data Science Summit at Stanford. What was originally supposed to be a small working group soon mushroomed into a large meeting of more than fifty people that included well known mathematics educators, representatives from industry, and mathematicians. By the end of the Stanford meeting, rather than being energized by the day, Levitt was depressed. There were all these smart people in the room, committed to the idea of changing the curriculum, but all having different ideas. "It didn't seem like something that would happen in my lifetime," he says.
In fact, there already has been some substantial progress made in bringing data science to the schools. Notable is the Introduction to Data Science course that was co-developed by UCLA and the Los Angeles Unified School District and is already being rolled out in 15 southern California School Districts. In addition to a curriculum there is a professional development arm to help teachers acquire the skills to teach it.
In addition to explicit materials and courses like this, data is also making its way into the curriculum in more subtle ways. As part of thoughtful redesign twenty percent of the SAT now tests the ability of prospective college students to understand data, both in the quantitative and verbal parts of the test, the latter of which includes data in the reading comprehension piece. This is all part of concerted effort by the College Board CEO David Coleman to make the SAT more relevant to what is actually being taught in the "average" first year of college. Even more, from Coleman's perspective, if kids were going to be studying for the SAT, then that studying should be worthwhile even beyond its relevance to college admissions. Data literacy is a part of that mission.
The recognition of the centrality of data is also a part of a next generation of AP courses. The new AP Biology course has been redesigned to have a significant data analysis component. The AP footprint in computer science has been expanded to include an AP "Principles of Computer Science" course that focuses on data science and as such provides context for the a next programming course. Coleman is especially proud of the Principles course, as it has proved to be a gateway course for computing that is especially attractive to demographics that historically have been under-represented in computer science. Since its roll-out in 2016, the numbers of female, black, LatinX, and rural examinees have grown by 136%, 121%, 125% and 117% respectively. And in the first year after the Principles course was made available, enrollments in the well-known AP programming course doubled, with attendant and sustained increases in each of these populations in subsequent years. "the changes in who is doing computer science is something I'm really proud of," Coleman says. For Coleman, data science is a pathway to STEM diversity.
Another piece is a new initiative of Coleman's that the College Board is calling its "pre-AP curriculum". This fall "Pre-AP Geometry with Statistics" is being piloted around the country. It is a quarter the basics of data science with the rest basic geometry. The bridging conceit that both are contexts for deductive reasoning and the course joins the certainties of deduction with the probabilities of data science. A new Pre-AP Algebra II course will also have data analysis connections inserted through the appearance of functions with more than one variable.
Work like that being done at the College Board and other places does give Levitt some hope that math curricula will change - if not for his kids, then at least for his kids' kids. What Levitt, Boaler, and many others support - possibly as a short-term fix, but at least as a step forward - is the idea of streamlining the current curriculum. While it's not exactly as simple as "cutting two textbooks in half and gluing them together to make a new course," as Levitt says, there is something to that. It's the kind of thing that he and others could imagine organizing a group of mathematicians, and data scientists around to find a way to remove a year from the AGA sandwich.
A newly streamlined curriculum would then give space for a year of data exploration and integration of computing, maybe even more mathematics exploration - again, assisted by computing. A modification may also may put less strain on any requisite teacher training than a complete rewrite. It leaves open the possibility of a math curriculum that would be relevant for all the students, with a branchpoint that would depend upon interests: algebra, geometry/algebra+data followed either data science or calculus, or both! It would be better connected - maybe with the help of ideas from the new pre-AP courses - to the overall curriculum and in that, also possibly serve the purpose of getting more kids with a range of interests and abilities interested in mathematics. The devil is in the details, but this is the kind of near-term and seemingly achievable goal that Levitt, Boaler and others are now working toward.

Load-Date: September 28, 2020


End of Document




Is it time to kill calculus?
Salon.com
September 26, 2020 Saturday


Copyright 2020 Salon.com, LLC. All Rights Reserved
Length: 3138 words
Highlight: Math curricula are designed to shepherd students toward calculus. Some mathematicians think this path is outdated


Body


Link to Image
Math formulas are written on the school board Getty Images
Many parents relish reliving moments from our childhoods through our children, and doing homework with them is its own kind of madeleine. For Steve Levitt of "Freakonomics" fame - who is, in his own words, "someone who uses a lot of math in my everyday life" - a trip down memory lane vis-a-vis math homework became a moment of frustrated incredulity rather than gauzy reverie. "Perhaps the single most important development over the last 50 years has been the rise of data and computers, and yet the curriculum my children were learning seemed to have been air-dropped directly from my own childhood," he told me. "I couldn't see anything different about what they were learning than what I learned, even though the world had transformed completely. And that didn't make sense."
Levitt has made a career of questioning the received dogma. In this case, what he saw was that "A mathematical way of thinking, numeracy, data literacy, is far more important today than it has been; the ability to visualize data, the ability to make sense out of a pile of numbers, has never been more important, but you wouldn't know that from looking at the math curriculum." Data combined with the use of mathematical ideas had transformed the way he and others look at the world. Should data also change the way we teach mathematics?
In most schools, children are grounded in basic arithmetic in elementary school, and then, somewhere between middle school and high school, force-fed the "algebra-geometry-algebra sandwich". The first year of algebra ("Algebra I") continues to reinforce basic arithmetic, and then brings in fractions. The familiar starts to give way to the unfamiliar when variables and functions are introduced. That's when "x the unknown" makes its first appearance in word problems and linear equations, which for many marks a first sign of confusion rather than buried epistemological treasure.
Things then take a big turn, and math class time-travels to the days of ancient Greece for lessons in formal geometric proofs ("Geometry") that Euclid would have little trouble stepping in to substitute teach. Following that is a yearlong return to algebra ("Algebra II: The Sequel!"), which given the previous year's partial hiatus from x's and y's and numbers first requires a lengthy review and then finally a return to new functions (exponentials, logarithms, polynomials) that either amuse or irritate you, depending on your taste, predilections, and teacher.
For some math stops here. For others there is often an honors track that speeds things up. Increasingly, honors or not, students get to pre-calculus or calculus, which is often revisited in the first year of college, and is the last bit of formal math a person will ever taste. Apologies to the reader for any unhappy flashbacks - or indigestion - incurred.
The sandwich - and actually the entire mathematical meal - has had a long shelf-life. If Levitt felt like his kids were air-dropped into his childhood math classes, odds are this was true for his parents too. The origins of the curriculum go back to a famous 1892 "Committee of Ten" that met at the behest of the National Education Association to standardize public education. Like any good committee their first act was to create more committees - nine to be exact - each tasked with the consideration of a "principal subject which enters into the programmes of secondary schools in the United States and into the requirements for admission to college". Each of these subcommittees then considered "the proper limits of its subject, the best methods of instruction, the most desirable allotment of time for the subject, and the best methods of testing the pupils' attainments therein." Mathematics was one of them. So was Latin and Greek. 
Pre-college mathematics education at that time was like today, composed of arithmetic, geometry, and algebra. The committee had recommendations for each of these areas. The teaching of arithmetic should be "abridged by omitting entirely those subjects which perplex and exhaust the pupil without affording any really valuable mental discipline, and enriched by a greater number of exercises in simple calculation and in the solution of concrete problems." That could stand as a mission statement today for the teaching of any kind of mathematics. "Concrete geometry" would be part of grammar school mathematics and combined with drawing. The committee also recommended that all students, regardless of their aspirations have the same kind of mathematics education up through the first year of algebra. After that, distinctions start to emerge depending on your goals: trigonometry and more algebra for those going to "scientific schools", "commercial arithmetic" for those thinking of a business career.
While "the pupil who solves a difficult problem in brokerage" may still learn some good mathematics,  "The movements of a race horse afford a better model of improving exercise than those of the ox on a treadmill." Setting aside the confusing animal analogy (okay, an ox is slow, but it is really strong and why are you putting it on a treadmill?) the spirit of the metaphor reflects something of a general tension in education: how much do we teach for the world as it is today and how much for the unknown tomorrow? Specific applications or general principles?
This is a tension that is perhaps felt most keenly in the teaching of mathematics and has led to something of a back and forth in mathematics education. Among the most well-known attempts to revamp the mathematics classroom was the move in the 1960s to the "new math", which was a reaction on the part of mathematicians - and some math teachers - that mathematics teaching had become too utilitarian, a curricular decision made decades earlier, at least in part because it was observed that our soldiers in World War II were lacking in basic mathematical skills. The energy behind that revamping of mathematics teaching was the Space Race, initiated by the surprising launch of Sputnik and a perceived "math gap" that would have to be closed in order for the United States maintain international supremacy. Getting people into space and beyond the clouds would mean that we needed to start teaching a kind of math that was already in the clouds. The "New Math" would strip mathematics to its roots, going as far down as basic set theory - Venn diagrams - and rebuild the world of numbers from the ground up.
By most accounts the program was a failure, sacrificing a direct inculcation of basic skills for the goal of exploring highly abstract general concepts, which while not completely disconnected from the day-to-day world of basic arithmetic and problem solving, was about as distant from it as the satellites it was supposed to help launch. Standards-based education and the Common Core followed soon after.
The uneasy relationship between applications and theory in the development of mathematics  curriculum is a reflection of  an ongoing - if slowly healing - rift within the discipline itself. There is pure mathematics and applied mathematics, the former a creation of mathematics for its own sake as opposed to in the service of solving a problem that is troubling someone in the real world. "Applied" is better than "impure," I guess, although it's an adjective that has ugly historical and elitist overtones. To the extent that engineering is a craft, it is a bias that one might trace back to the distinction between the scholar and the craftsperson, the university and the guild or technical school. In truth, a genetic family tree of math would show all kinds of connections and surprising worldly origins of even the "purest" mathematics.
Levitt's call for a mathematics curriculum centered around data is not born of anti-intellectualism. He is quick to point out that he is not "anti-math", rather that he is "anti-math-that-no-one-will-ever-use-in-the-first-place," at least in the classroom. As someone who loves math, he worries that a mathematics curriculum not connected to data and computers "runs the risk of being demoted." "If the best arguments  for mathematics is that it's part of the education of a well-rounded citizen and that it's good for brain development," that may very well be the undoing of math.
By his own admission, he is a Johnny-come-lately to the challenge of curriculum reform, but his academic star-power has enabled him to attract important and influential players to his mission of bringing data and computing to mathematics education. Levitt quickly stood up a small advisory committee of like-minded people that included statistics celebrity Nate Silver of 538.com, former Secretary of Education Arne Duncan, and former Google CEO Eric Schmidt.
Levitt's most important recruit may be Jo Boaler, the Nomellini & Olivier Professor of Education at Stanford. Boaler also directs YouCubed, a non-profit whose mission is to "inspire, educate and empower teachers of mathematics." Her work on mathematics education is widely cited and influential. The call she received from Levitt came at a perfect time, as she is currently working with California's Department of Education to revamp the mathematics curriculum around data.
For Boaler, the sclerotic nature of the mathematics curriculum is above all an equity issue, and for that she places calculus at the center. As Boaler points out, mathematics is usually the only subject in which kids - usually 6th graders - take a placement exam in middle school, the result of which sets them on their academic pathway through high school, on track - usually an "honors" track - to take calculus junior year of high school.
The curriculum as calculus funnel and an honors track to speed one's ride has downstream effects. My own home institution Dartmouth College is almost surely not an outlier among its peer group in that, while calculus is not required for admission, you would be hard-pressed to find a student here who didn't take calculus in high school. It's only the students who have made it through the initial placement - in sixth grade! - who have the ability to show colleges that they can succeed in calculus. For Boaler, calculus is a linchpin, not in and of itself, but because of the influence it has both on the curriculum that precedes it as well as its influence on students' college prospects moving forward.
Despite what some may think, calculus didn't end up as the last stop on the math track just to create a final hurdle for high schoolers. It was for many years, the most applicable math - outside of arithmetic - that you could find. It continues to be of great importance in all kinds of applied contexts, from medicine to engineering to finance, where modeling change - usually in the form of a "differential equation" - is crucial. It is mainly useful in continuous contexts (think fluids or asteroids in motion) and powerful for finding "analytic solutions" (formulas) that quantify a phenomenon indirectly encoded in a differential equation. All of this is still true, except what happened is that many new and interesting phenomena also started to be represented by numbers - i.e., the data revolution occurred. This didn't mean calculus became irrelevant, rather other important possibilities arose for mathematical thinking and learning and teaching.
Boaler calls calculus a "horrible and inequitable filter." Some of the inequity is around gender - placement testing preferences boys over girls, a finding that may be something of a surprise to many. Equity issues may also be redounding to the academy. A calculus-successful student body may very well be contributing to an over-representation in STEM in its entering classes, or rather that an underrepresentation or under-cultivation of humanities interests. Students have only so much time to take classes in high school and only so much energy. A history-interested student may very well be taking yet another difficult math course instead of another history or government or art course simply because she knows - or believes - that she has to wrap her head around calculus, which doesn't have great tangencies to her intellectual passions.
"Data science" is a name invented to distinguish the ideas and approaches used to analyze the new diversity and quantities of data that characterize modern data from those of classical statistics.  Done well, it is an integration of critical thinking and quantitative skills, storytelling with and through numbers, supported by evidence. It has strong connections to the humanities, both in spirit and practice, as many interesting kinds of data analyses are regularly performed on information derived from humanities subjects, often in digital humanities programs. The humanities context is reflexive, too: it's no coincidence that the new important work now being done in data bias came out of digital humanities programs. I'd wager that a student who excels at and enjoys data science is more likely to also have interest in and see beauty in the activity of close-reading a text, or image, or artifact, or working in the humanities more generally.
Data science is also highly collaborative, which a good deal of research shows is a working style where girls (and women) excel. Some of the same studies that show boys outperforming girls on timed tests, show girls outperforming boys when the tests have a collaborative framing. If data science were a part of a high school curriculum it could provide a mechanism for girls to show their quantitative skills and it also could be a boost to humanities programs as well. It would in short allow more students to showcase their talents to colleges in ways that could benefit both students and colleges.
Last March, Boaler and Levitt convened a Data Science Summit at Stanford. What was originally supposed to be a small working group soon mushroomed into a large meeting of more than fifty people that included well known mathematics educators, representatives from industry, and mathematicians. By the end of the Stanford meeting, rather than being energized by the day, Levitt was depressed. There were all these smart people in the room, committed to the idea of changing the curriculum, but all having different ideas. "It didn't seem like something that would happen in my lifetime," he says.
In fact, there already has been some substantial progress made in bringing data science to the schools. Notable is the Introduction to Data Science course that was co-developed by UCLA and the Los Angeles Unified School District and is already being rolled out in 15 southern California School Districts. In addition to a curriculum there is a professional development arm to help teachers acquire the skills to teach it.
In addition to explicit materials and courses like this, data is also making its way into the curriculum in more subtle ways. As part of thoughtful redesign twenty percent of the SAT now tests the ability of prospective college students to understand data, both in the quantitative and verbal parts of the test, the latter of which includes data in the reading comprehension piece. This is all part of concerted effort by the College Board CEO David Coleman to make the SAT more relevant to what is actually being taught in the "average" first year of college. Even more, from Coleman's perspective, if kids were going to be studying for the SAT, then that studying should be worthwhile even beyond its relevance to college admissions. Data literacy is a part of that mission.
The recognition of the centrality of data is also a part of a next generation of AP courses. The new AP Biology course has been redesigned to have a significant data analysis component. The AP footprint in computer science has been expanded to include an AP "Principles of Computer Science" course that focuses on data science and as such provides context for the a next programming course. Coleman is especially proud of the Principles course, as it has proved to be a gateway course for computing that is especially attractive to demographics that historically have been under-represented in computer science. Since its roll-out in 2016, the numbers of female, black, LatinX, and rural examinees have grown by 136%, 121%, 125% and 117% respectively. And in the first year after the Principles course was made available, enrollments in the well-known AP programming course doubled, with attendant and sustained increases in each of these populations in subsequent years. "the changes in who is doing computer science is something I'm really proud of," Coleman says. For Coleman, data science is a pathway to STEM diversity.
Another piece is a new initiative of Coleman's that the College Board is calling its "pre-AP curriculum". This fall "Pre-AP Geometry with Statistics" is being piloted around the country. It is a quarter the basics of data science with the rest basic geometry. The bridging conceit that both are contexts for deductive reasoning and the course joins the certainties of deduction with the probabilities of data science. A new Pre-AP Algebra II course will also have data analysis connections inserted through the appearance of functions with more than one variable.
Work like that being done at the College Board and other places does give Levitt some hope that math curricula will change - if not for his kids, then at least for his kids' kids. What Levitt, Boaler, and many others support - possibly as a short-term fix, but at least as a step forward - is the idea of streamlining the current curriculum. While it's not exactly as simple as "cutting two textbooks in half and gluing them together to make a new course," as Levitt says, there is something to that. It's the kind of thing that he and others could imagine organizing a group of mathematicians, and data scientists around to find a way to remove a year from the AGA sandwich.
A newly streamlined curriculum would then give space for a year of data exploration and integration of computing, maybe even more mathematics exploration - again, assisted by computing. A modification may also may put less strain on any requisite teacher training than a complete rewrite. It leaves open the possibility of a math curriculum that would be relevant for all the students, with a branchpoint that would depend upon interests: algebra, geometry/algebra+data followed either data science or calculus, or both! It would be better connected - maybe with the help of ideas from the new pre-AP courses - to the overall curriculum and in that, also possibly serve the purpose of getting more kids with a range of interests and abilities interested in mathematics. The devil is in the details, but this is the kind of near-term and seemingly achievable goal that Levitt, Boaler and others are now working toward.

Load-Date: September 29, 2020


End of Document




In the West, wildfire smoke accounts for more pollution; Air quality declines again after years of steady gains, study finds
Los Angeles Times
January 15, 2021 Friday
Final Edition


Copyright 2021 Los Angeles Times All Rights Reserved
Section: CALIFORNIA; Metro Desk; Part B; Pg. 1
Length: 1047 words
Byline: Tony Barboza
Body


Wildfire smoke now accounts for up to half of all fine-particle pollution in the Western U.S., according to a new study that blames climate change for worsening air quality and health risks in both urban and rural communities in recent years.
The study by researchers at Stanford University and UC San Diego found that the concentration of tiny, lung-damaging pollutants known as PM2.5 that are attributable to wildfire smoke roughly doubled between 2006 and 2018, while the share of pollution from other sources like car and truck exhaust declined.
The trend is most pronounced in Western states and highlights the rapidly growing health threat of wildfire smoke. This became shockingly apparent to millions during last year's record-breaking firestorm, which enveloped much of the West Coast in an unhealthy pall for weeks.
Levels of PM2.5 had been steadily improving over the last two decades in which they have been routinely monitored, as a result of regulations that have cut emissions from vehicles and power plants. But those gains started to slow, then reverse, over the last decade or so, according to the study.
"The overall picture is of a stalled and reversed improvement, which is a result of other sources getting cleaner and wildfires getting a lot worse," said Marshall Burke, a professor of Earth system science at Stanford University and lead author of the study published in the Proceedings of the National Academy of Sciences.
The two major factors driving the increase in wildfire smoke are the warming climate and decades of fire suppression that have allowed fuels to build up, according to researchers.
They made their estimates by developing a statistical model using fire and smoke data from satellites and readings from ground-based air quality monitoring stations.
Nationwide, wildfires are now responsible for up to 25% of fine-particle pollution, the study found.
"We know wildfires generate smoke. We know smoke is bad for health. But we really didn't have a comprehensive national picture for how much wildfires are contributing to poor air quality," Burke said.
Francesca Dominici, a professor of biostatistics at Harvard's T.H. Chan School of Public Health who was not involved in the research, called it "an excellent study that relies on sophisticated data science approaches" and "provides strong evidence that wildfires are an increasing threat to human health."
Dominici said its findings are concerning, "especially at the time where the U.S. EPA has recommended to retain the current standards for PM2.5 pollution and as we are fighting COVID-19 that is attacking our lungs."
"I hope that reducing the risk of climate change- related disasters, such as wildfires, will be a priority for the new administration," she added.
For decades, motor vehicles and industrial emissions have been responsible for most of the West's PM2.5, though at least some of that type of pollution has always come from fires. Previous studies have predicted that greenhouse gas emissions will dramatically increase wildfire smoke in the in the Western U.S. in the coming years as temperatures rise and dry the landscape.
In the latest study, Stanford and UC San Diego researchers predicted dramatic health impacts if nothing is done to slow climate change by slashing emissions and better managing forests. Within decades, they found, exposure to wildfire smoke alone could increase dramatically to the point of being one of the deadliest climate impacts.
The study projects an additional nine to 20 smoke-related deaths per 100,000 people by midcentury if emissions continue at their current pace, which is close to the roughly 24 additional deaths per 100,0000 people predicted directly from rising heat -- the deadliest effect of climate change on people.
"Wildfires are going to be the way that many of us experience climate change, as important as these direct heat impacts," Burke said. "These changes in wildfire risk are the combination of two main things we have done: a century of wildfire suppression and climate change. None of these future estimates are an inevitability. They are a choice."
The analysis also found that while people of color continue to be exposed to higher levels of total PM2.5 -- as has long been the case -- higher-income counties with a higher proportion of white people are on average more exposed to higher levels of PM2.5 from wildfire smoke.
Researchers acknowledge that measuring outdoor pollution does not necessarily correspond to people's actual exposure because it does not factor in how much time they spend outdoors or the age and quality of their home.
Past research shows that more outdoor pollutants seep into "older, smaller homes and for lower-income households and these differences could lead to disparities in overall individual exposure even if ambient exposures are not different," according to the study.
Scientists suspect the 2020 wildfires inflicted widespread health damage by fouling the air of nearly 96 percent of Californians with smoke levels exceeding federal standards, according to the state Air Resources Board. The weeks-long siege of smoky air generated both the highest readings and most widespread unhealthy levels of fine-particle pollution since continuous monitoring began in the late 1990s.
Of greatest concern are the microscopic particles in smoke that can be inhaled deep into the lungs and enter the bloodstream. Not only do those pollutants irritate the eyes, nose and throat, tighten the chest and cause difficulty breathing, they can trigger asthma attacks, strokes and heart attacks. Wildfire smoke poses serious risks to young children and the elderly, and people with chronic health conditions such as asthma, lung disease and heart disease face increased risk of hospitalization and death.
Scientists know from decades of research that breathing the fine-particle pollution in urban smog can lead to long-lasting health problems.
Though less is known about the long-term damage from the fine-particle pollution in wildfire smoke, early research suggests it impairs people's lungs long after the smoke clears. An ongoing health study in Montana reported last year that people in a community that was blanketed with wildfire smoke for 49 days in 2017 still had decreased lung function two years later.

Graphic

 
PHOTO: A WATER TRUCK operator is overcome with smoke during the Silverado fire on Oct. 26 in Irvine. A new study blames climate change for worsening health risks.  PHOTOGRAPHER:Allen J. Schaben Los Angeles Times PHOTO: MILL VALLEY RESIDENTS Patrick Kenefick, left and Dana Williams record video of San Francisco's Golden Gate Bridge on Sept. 9. Last year's wildfires covered the West Coast in an unhealthy pall for weeks.  PHOTOGRAPHER:Eric Risberg Associated Press PHOTO: FIREFIGHTER Ricardo Gomez works on the Creek fire Sept. 6 along Highway 168 in Shaver Lake, Calif.,  PHOTOGRAPHER:Kent Nishimura Los Angeles Times 

Load-Date: January 15, 2021


End of Document




County Reports 56 Deaths, Hospitalizations Surge As COVID-19 Variant Spreads
City News Service
January 5, 2021 Tuesday 8:18 PM PST


Copyright 2021 City News Service, Inc. All Rights Reserved
No City News Service material may be republished without the express written permission of the City News Service, Inc.
Length: 956 words
Body


SAN DIEGO (CNS) - San Diego County public health officials today reported 1,814 new COVID-19 infections, 56 deaths and a record number of hospitalizations, as well as 24 new confirmed and four probable cases of the more contagious strain of SARS-CoV2 that was first identified in the United Kingdom.
It was the 36th consecutive day with more than 1,000 new infections, but the first time in 26 days the number reported did not surpass 2,000. The 3,000 mark has been crossed 10 times since the start of the pandemic.
The number of deaths reported was the third-highest, after 62 deaths reported Wednesday and 58 on New Year's Day.
The county's cumulative cases now number 172,847, and the death toll rose to 1,654.
A record 4,478 cases were reported Friday, followed by the second- and fourth-highest numbers -- 4,427 Saturday and 3,520 Sunday.
Of 19,182 tests reported Tuesday, 9% returned positive, dropping the 14-day rolling average to 12.9%.
County health officials are attributing the increasing number in deaths to gatherings over the holidays and the presence of the new coronavirus variant known as B.1.1.7.
"Each of the 56 people who lost their lives to COVID-19 during this reporting period was someone's parent, sibling, friend or spouse, as well as a valued member of our community," said Dr. Wilma Wooten, San Diego County's public health officer.
"These deaths are a sobering reminder that we must all do our part to prevent the spread of COVID-19."
The latest deaths were of 26 women and 30 men who succumbed to the virus between Dec. 2 and Sunday. A dozen were in their 90s, 13 in their 80s, 13 in their 70s, 13 in their 60s, four in their 50s, and one in his or her 20s. The San Diego County Health and Human Services Agency said 50 had underlying medical conditions, with medical histories pending for the other six.
The county reported 62 new hospitalizations and 10 new intensive care unit admissions, bringing the number of patients hospitalized with COVID-19 to a record 1,609. The number of patients in ICU beds is 380.
The county's ICU bed availability is 20%, according to county health officials, although with staffing issues, the reality is likely much lower. The intensive care unit bed availability for the Southern California region remains at 0%.
The cases reported Tuesday of the more virulent strain bring the county's confirmed cases of the variant to 28. The cases were confirmed by whole genome sequencing and the four probable cases are directly linked to the confirmed cases and have positive diagnostic nucleic acid tests, but are not yet sequenced.
There have been no deaths locally connected to the variant, but one woman had to be hospitalized. She is now at home recovering, according to the HHSA.
The variant was first found in the U.S. last Tuesday in Colorado. The first San Diego case was confirmed in a man in his 30s with no history of travel, who first became symptomatic Dec. 27 and tested positive Dec. 29. He was hospitalized and contact tracing was initiated.
Three additional cases were reported Thursday.
The 24 newly confirmed patients are believed to have no travel history and come from 19 different households, but the investigation and contact tracing are ongoing, the HHSA reported.
New cases have been identified in San Diego, Chula Vista, La Mesa and Lakeside. While the four youngest cases are in children under 10 and the oldest is over 70, the average age of the variant cases is 36 -- the same as the overall average for all confirmed cases in the county to date.
"The fact that these cases have been identified in multiple parts of the region shows that this strain of the virus could be rapidly spreading," Wooten said.
"People should be extra cautions to prevent getting and spreading COVID-19, especially this variant, which research has shown is more contagious."
The county has asked all testing labs with the capability to identify the new strain to forward specimens for genome sequencing to determine if they are indeed cases of the variant.
Local doctors have also been requested to forward COVID-19 positive tests from patients with a travel history to the United Kingdom or other places overseas where variants have been detected.
Two new community outbreaks were reported Tuesday, one in a business setting and one in a retail setting. In the last seven days, there have been 44 confirmed community outbreaks, defined as three or more COVID-19 cases in a setting and in people of different households over the past 14 days.
In another development, San Diego County Fire Department spokesman Thomas Shoots announced a rotating vaccination schedule involving San Marcos, Escondido and Rancho San Diego for first responders.
"The partnership between local fire and health agencies has built the framework for the vaccination process going forward, and will ultimately provide all first responders in San Diego County the opportunity to get vaccinated for COVID-19," he said.
Supervisors Nathan Fletcher -- who was elected earlier in Tuesday by his colleagues as chairman of the San Diego County Board of Supervisors -- and Nora Vargas announced a plan Tuesday night to revamp the board's approach toward managing and helping defeat the coronavirus pandemic.
Fletcher said he will present the board next Tuesday with the new, overarching COVID-19 "framework," which he said would override previous "actions and statements that contradict having a response based on data and science," prioritize funding based on a "data-driven approach to targeted communities of need, taking into account health equity," stress working "collaboratively instead of antagonistically with the state" and pledge that the county will "rely on data, science and (rational) decision-making to keep us safe."

Load-Date: January 6, 2021


End of Document




SD County Reports 3,815 new COVID-19 Infections As Hospitalizations Increase
City News Service
January 6, 2021 Wednesday 5:00 PM PST


Copyright 2021 City News Service, Inc. All Rights Reserved
No City News Service material may be republished without the express written permission of the City News Service, Inc.
Length: 715 words
Body


SAN DIEGO (CNS) - San Diego County public health officials reported 3,815 new COVID-19 infections and 37 deaths today, as well as a record number of hospitalizations.
The new cases mark the third-highest number of infections reported in a single day, behind a record 4,478 cases reported Friday and 4,427 on Saturday, and the 37th consecutive day with more than 1,000 new diagnoses.
Cases have crossed the 2,000 mark in 27 of the last 28 days after Tuesday's 1,814 broke a 26-day streak. The 3,000 mark has been crossed 11 times since the start of the pandemic.
Hospitalizations resulting from the virus rose to a record 1,664 on Wednesday. Of those, 384 COVID-19 patients are in intensive care units. The number of available, staffed ICU beds in the county dwindled to 39.
County health officials attribute the increasing number of cases, hospitalizations and deaths to gatherings over the holidays and the presence of the new coronavirus variant known as B.1.1.7. that was first detected in the United Kingdom.
Supervisor Nathan Fletcher said it's likely the number of hospitalizations will continue to increase, as we're in the 21-24 day "lag" period between rising cases and rising hospitalizations.
The county's cumulative cases now number 176,662 and the death toll rose to 1,691. Of 26,320 tests reported Wednesday, 14% returned positive.
Kaiser Permanente San Diego began delivering second doses of COVID-19 vaccinations this week for the first health care workers who met "Phase 1a" criteria for vaccination at Kaiser's two hospital locations -- the San Diego and Zion Medical Centers.
"This is a big milestone. We're encouraged by this protection for our workforce and await the arrival of additional vaccine that can be distributed per the CDC guidelines," said Dr. Michael Lalich, Kaiser Permanente San Diego's area medical director.
"We are committed to educating our communities about the safety and effectiveness of the vaccine. We want everyone to understand how getting a vaccine, along with the simple public measures already in place -- wearing masks, social distancing and frequent hand washing -- will help to bring an end to this pandemic."
The county reported 24 confirmed diagnoses of the more virulent strain of the virus on Tuesday, bringing the county's confirmed cases of the variant to 28. The cases were confirmed by whole genome sequencing and the four probable cases are directly linked to the confirmed cases and have positive diagnostic nucleic acid tests, but are not yet sequenced.
There have been no deaths locally connected to the variant.
The variant was first found in the United States last week in Colorado. The first San Diego case was confirmed in a man in his 30s with no history of travel, who first became symptomatic Dec. 27 and tested positive Dec. 29. He was hospitalized and contact tracing was initiated.
The 24 newly confirmed patients are believed to have no travel history and come from 19 different households, but the investigation and contact tracing are ongoing, the county's Health and Human Services Agency reported.
New cases have been identified in San Diego, Chula Vista, La Mesa and Lakeside. While the four youngest cases are in children under 10 and the oldest is over 70, the average age of the variant cases is 36 -- the same as the overall average for all confirmed cases in the county to date.
"The fact that these cases have been identified in multiple parts of the region shows that this strain of the virus could be rapidly spreading," said Dr. Wilma Wooten, the county's public health officer.
Hours after being elected chair of the Board of Supervisors, Nathan Fletcher and fellow board member Nora Vargas on Tuesday night announced a plan to revamp the board's approach toward managing and helping defeat the pandemic.
Fletcher said he will present the new, overarching COVID-19 "framework" next Tuesday, which he said would override previous "actions and statements that contradict having a response based on data and science"; prioritize funding based on a "data-driven approach to targeted communities of need, taking into account health equity"; stress working "collaboratively instead of antagonistically with the state"; and pledge that the county will "rely on data, science and (rational) decision-making to keep us safe."

Load-Date: January 7, 2021


End of Document




San Diego Foundation Announced $750,000 in STEM Grants for Minority Students
City News Service
December 11, 2020 Friday 11:10 AM PST


Copyright 2020 City News Service, Inc. All Rights Reserved
No City News Service material may be republished without the express written permission of the City News Service, Inc.
Length: 761 words
Body


SAN DIEGO (CNS) - The San Diego Foundation today announced $750,000 in grants to 13 nonprofit programs to create more opportunities for students -- particularly from underrepresented communities -- pursuing STEM degrees and careers.
Since 1999, the Science & Technology Program at The San Diego Foundation has granted more than $8 million to support scientists and engineers in San Diego. The Science and Technology Program is funded in part by the Blasker-Rose-Miah Endowment Fund at The San Diego Foundation and The Reuben H. Fleet Foundation.
According to the San Diego Regional Economic Development Corporation's report, "Building San Diego's Talent Pipeline," the evolution of the economy has presented opportunities and challenges for the workforce. Among those challenges are Latino and Black communities being underrepresented in San Diego's highest-paying industries and occupations.
The nonprofit grants are:
-- PATHS at the University of California San Diego: The $75,000 grant will support the PATHS program, which provides students with financial aid, basic needs and socio-emotional support that allow them to concentrate their energies on pursuing their STEM degrees;
-- San Diego State University Research Foundation: The $75,000 grant will support the WE BELIEVE -- Women and Black Empowered Learners Interning in Engineering Environments -- program which develops a pipeline for historically underrepresented students in San Diego into STEM degree pathways via a research experience;
-- CREATE at the University of California San Diego: The $75,000 grant will expand the STEMULATE -- STEM Undergraduate Leadership and Teaching Empowerment -- program which provides low-income and underrepresented students with mentorship and internship opportunities;
-- Miramar College Foundation: The $75,000 grant will support the Supply Chain Experience program at the Southern California Biotechnology Center at San Diego Miramar College. The program improves career readiness among underrepresented community college students by connecting them with advanced internship opportunities and hands-on experience in a supportive environment;
-- California State University San Marcos: The $74,813 grant will allow more students to participate in the STEM Summer Scholars program, which provides instruction in the sciences to a diverse student population;
-- Office of Undergraduate Research, University of San Diego: The $70,000 grant will provide stipends and resources to underrepresented STEM students participating in the university's Pre-Undergraduate Research Experience and Summer Undergraduate Research Experience programs;
-- Access, Inc.: The $65,037 grant will help support youth who have experienced trauma and have overcome challenges, such as homelessness. Students are supported through case management to build incremental competencies and achieve academic and social milestones;
-- Palomar College Foundation: The $50,000 grant will fund the Palomar College STEM Robotics Summer Institute, which offers underserved students an opportunity to see themselves as future robotics engineers, developers and designers;
-- Interfaith Community Services: The $46,400 grant will support the Transitional Youth Academy 2021 STEM Summer Internship Program that provides historically underrepresented youth with opportunities to engage in hands-on experiences in STEM-focused paid internships;
-- Zoological Society of San Diego: The $42,600 grant will expand the reach of the San Diego Zoo Global Internships in Conservation Technology program, which helps underrepresented young adults who are pursuing degrees in computer science, data science and/or engineering;
-- San Diego Natural History Museum: The $40,000 grant will allow the Natural History Museum to offer STEM internships that build professional experience and expertise in research and biological consulting;
-- Salk Institute for Biological Studies: The $35,000 grant will enable more historically-underrepresented young adults to participate in the Heithoff-Brody High School Summer Scholars program. During the program, student scholars work on original research projects with Salk scientist-mentors and participate in other organized learning experiences as part of an eight-week, paid internship at the Salk Institute; and
-- Elementary Institute of Science: The $26,150 grant will connect underrepresented youth to STEM internships which move students into scientific and technical career pathways by engaging females from Title I high schools in a comprehensive drones education and certification program.

Load-Date: December 12, 2020


End of Document




Nielsen Sets Timetable for Cross-Platform Media Measurement
The Hollywood Reporter
December 8, 2020 Tuesday


Copyright 2020 Prometheus Global Media, LLC All Rights Reserved


Section: NEWS; TAG
Length: 384 words
Byline: Rick Porter
Highlight: Nielsen is preparing to move into the future (or present) of TV measurement with a cross-platform metric it plans to roll out in a couple of years.


Body


Nielsen is preparing to move into the future - or the present - of TV ratings measurement.
The company on Tuesday announced plans to roll out a cross-media measurement solution that will encompass consumption across platforms and devices. The new metric, Nielsen One, is slated to begin rolling out in late 2022, and Nielsen says its expects that measurement to become the industry standard for buying and selling ad inventory - a $100 billion annual business - by fall 2024.
With Nielsen One, we are delivering a single, comparable metric for TV and digital that will provide video consumption across all platforms, services and devices. For media buyers and sellers, this means better monetizing their assets and maximizing their investments, said Nielsen COO Karthik Rao. Todays announcement marks a major milestone for Nielsen as we put our cross-media vision into motion. Weve made significant enhancements over the last year to turbocharge the tech and data science required to make an industry wide cross-media solution a reality.
Once it begins, the new measurement would represent a sea change in the way media consumption is measured. Nielsen hasnt updated its core ratings metric since the mid-2000s when it started measuring delayed viewing of commercials over three and seven days as DVRs became more prevalent. Nielsen One will encompass all video consumption, regardless of platform and, crucially, the kind of device on which people are watching. Streaming platforms, for instance, have long said that Nielsen doesnt fully measure viewing away from a TV screen, thereby undercounting a segment of the audience.
TV outlets have also started keeping (and occasionally releasing) their one multi-platform audience figures, but theres currently not a comprehensive or uniform third party measurement for nonlinear viewing. Thats important, as U.S. consumers now spend more time on streaming and digital media consumption away from the TV screen - a combined 15 trillion minutes of time between March and August of this year - than they do on linear TV (11.1 trillion minutes).
The new platform will also deliver ratings at subminute intervals for individual ads and other content, a key move as personalized ads and addressable content becomes more prevalent in the TV realm.
Link to Image

Load-Date: December 8, 2020


End of Document




Learn to expertly work with data and earn an in-demand job
Salon.com
August 29, 2019 Thursday


Copyright 2019 Salon.com, LLC. All Rights Reserved
Length: 210 words
Highlight: This bundle includes 88 hours of training covering data science, statistics, Python and more.


Body


Link to Image
Data is a valuable thing, and companies big and small are scrambling to hire talented pros who can help them collect, analyze, and manage it. That being said, it's worthwhile to consider learning data analytics skills to not only bolster your current career but potentially put you on track to earn a lucrative income through a job that's in high demand. The Data Science for Business Mastery Bundle includes 10 different courses and 88 hours of training covering all the top tools and techniques to derive helpful insights from data.
From learning how to use data visualization software to draw actionable insights from numbers, to developing quantitative and econometric modeling skills typically used in finance, this bundle is your gateway to all things data science related. This bundle helps you discover how to use tools that have a myriad of applications, including Python and R programming.
There's even a course covering machine learning, one of the most highly sought-after skills in the industry today. Usually, the Data Science for Business Mastery Bundle is $790, but you can get it here for $29.
Link to Image
The Data Science for Business Mastery Bundle - $29
See Deal
Like this deal? Check out this trending 3-in-1 wireless charger, too!

Load-Date: April 13, 2020


End of Document




Back to the drawing board, maps
Los Angeles Times
November 13, 2020 Friday
Final Edition


Copyright 2020 Los Angeles Times All Rights Reserved
Section: CALENDAR; Entertainment Desk; Part E; Pg. 1
Length: 831 words
Byline: Carolina A. Miranda
Body


To survive a presidential election is to have the U.S. electoral map branded into our skulls. These cartographic depictions take our vast United States and reduce every nuance of politics down to a dichotomy of blue and red -- no texture permitted.
President Trump may have won Texas with nearly 5.9 million votes (as of Thursday afternoon), but former Vice President Joe Biden still raked in more than 5.2 million votes in the state. This reality is not reflected in the electoral maps blanketing every news website -- leaving one to think that everybody in Texas is a Republican. (The vote tally indicates that more than 5 million people would beg to disagree.)
And what about blue-blue California, where the map overlooks the nearly 5.5 million people who voted for Trump? Or Nebraska, which actually splits its electoral votes? Biden won one of the five electoral votes in that state, yet Nebraska is nonetheless shown on most electoral maps as a sea of red -- which is not just a distortion, it's patently incorrect. (Kudos to MSNBC's exuberant Steve Kornacki for showing Nebraska marked by blue and red stripes.)
In addition, the electoral map as we know it distorts the size of the different voting populations. The state of my birth, Wyoming, is the 10th largest U.S. state by area, but with a population of only 579,000 people, it has only three electoral votes. Yet its broad swath of redness plays an outsize role in our electoral maps. New Jersey, by contrast, with 9 million people and 14 electoral votes, takes up far less room.
Naturally, this design reflects the design of our electoral system, in which the votes for president are tabulated by state in a winner-takes-all fashion -- part of the electoral college system (which, frankly, has no place in a 21st century democracy). But as these maps continue to be widely employed by media across the political spectrum, they systematically reinforce the idea that the U.S. is either Red Territory or Blue Territory without overlap or exception. And that is wrong.
In response, designers have been trying to find better, more nuanced ways to visualize election data -- such as the unusual honeycomb-patterned map, created by the politics site FiveThirtyEight, which shows electoral college votes from the 2016 election to scale. Likewise, Greg Albers, an L.A.-based digital publisher who writes about issues of digital literacy, created an experimental map that shows the U.S. in gradients of purple.
During this election, France's Le Monde has experimented with more proportional graphics, showing the number of electoral votes for each state laid over a geographical map -- so Wyoming is marked by three red blocks, while New Jersey gets 14 blue ones. And the New York Times has also been experimenting with map design on its election site, including a Blokus-style electoral votes map, but the page opens with a traditional electoral map and you have to click through to find the alternates.
Last week, a series of GIF maps created by Karim Douieb, co-founder of Jetpack.AI, a data science company based in Brussels, went viral for showing different ways in which election data might be displayed in map form. His designs, first released in 2019, visualize election returns in ways that give more texture to where and how Americans are voting.
As he notes in an online presentation: "Acres don't vote, people do."
Douieb says he was inspired to create his maps after seeing a tweet posted last year by Lara Trump (a former TV producer who is married to Eric Trump) that showed a traditional county-by-county map along with the inscription, "Try to impeach this."
"I took that as a challenge," he tells me via email. "The picture she was showing was plain wrong given the context. By that I mean that it is an accurate map (displaying what party won each county) but used in a misleading way."
While designing better maps won't fix the fractures in our electoral system, they can help tell a better story about who we are and the spaces that we share.
"I believe traditional electoral maps are not bad as such," Douieb says, "but they are often used in a misleading way as they advantage geographical accuracy over electoral importance. ... [And] they don't represent how the people voted nor are they a good representation of how the electoral system works."
Our country is more complicated than blue and red. To illustrate that, I'd like to go back to Wyoming -- specifically, Casper, the town where I was born. In 2017, Marcus Patrick Ellsworth reported this very wonderful story about the LGBTQ community in Casper for MTV News.
"There is a tendency among certain swaths of liberals and leftists to disregard entire states like Wyoming," he wrote, "places where the populace tends to vote Republican and pass regressive laws."
But look beyond the red, he adds, and you'll find that "places like Casper, homes of fearless resistance waged out of necessity, shine bright with hope when seen up close."
We need maps that speak to that.

Graphic

 
PHOTO: AN ELECTORAL map by Karim Douieb shows voting by population, bottom, rather than by geography. Instead of vast swaths of red or blue, the map reveals the mixed nature of voting patterns  PHOTOGRAPHER:Jetpack.AI PHOTO: AN ELECTORAL map by Karim Douieb shows voting by population, bottom, rather than by geography. Instead of vast swaths of red or blue, the map reveals the mixed nature of voting patterns.  PHOTOGRAPHER:Jetpack.AI 

Load-Date: November 13, 2020


End of Document




Garcetti Announces Deputy Mayor to Lead Office of Budget and Innovation
City News Service
November 6, 2020 Friday 1:47 PM PST


Copyright 2020 City News Service, Inc. All Rights Reserved
No City News Service material may be republished without the express written permission of the City News Service, Inc.
Length: 539 words
Body


LOS ANGELES (CNS) - Mayor Eric Garcetti today appointed former NASA budget manager Jeanne Holm to serve as a deputy mayor to assist with the city's financial recovery from COVID-19 and its technological needs.
With more than 30 years of experience in technology innovation, data science and management, Holm most recently served as the city's chief data officer and the mayor's senior technology adviser, and she will lead Garcetti's Office of Budget and Innovation.
"Strong and creative leadership is absolutely critical to navigating an unprecedented pandemic, deep economic devastation and a severe fiscal crisis, and I have no doubt that Jeanne Holm will be a powerful force in steering our city toward a future of equity and resilience," Garcetti said.
"Jeanne has dedicated her career to deploying data, technology and innovation to serve the best interests of Angelenos and all Americans, and I look forward to tapping into her experience and insights to meet the needs of our residents now and in the years to come."
According to the mayor's office, Holm created the city's Telecommunications and Digital Equity Forum, established key partnerships with telecommunications companies and led the OurCycleLA program, all of which serve to help the lowest-income residents.
"I'm honored and excited to work with Mayor Garcetti to lift up Angelenos by leveraging the best of our city's innovation, dedicated workforce and resources," Holm said. "As we face the challenges from this pandemic head-on, we will continue to be guided by the principles of equity and justice to create opportunities for every Angeleno."
Prior to joining the city of Los Angeles in 2016, Holm worked at NASA, where she led large-scale budget changes during government downsizing that saved hundreds of millions of dollars while improving service and innovation across the agency, Garcetti said.
As an appointed Open Data Evangelist for the Obama administration, she led the development of Data.gov, which, by releasing open government data, led to $3 trillion in economic growth in the United States.
Holm succeeds Miguel Sangalang as deputy mayor and will build on his leadership in redefining and streamlining the daily operations of local government, developing the city's open data portal, reforming procurement processes, making service deployment more equitable and bringing more public services online, Garcetti said.
The office also oversees the annual city budget and six departments with more than 3,000 employees.
Sangalang leaves the mayor's office to serve as interim general manager of the Bureau of Street Lighting within the Department of Public Works, where he will deepen his focus on expanding neighborhood smart technologies and lighting infrastructure improvements, including 5G, electric vehicle chargers and air quality sensors.
"Miguel Sangalang has been a central figure in transforming government operations and maximizing our performance for Angelenos," Garcetti said. "He is a consummate public servant and a dynamic leader for our city, and he will bring that same determination, dedication and expertise to the task of modernizing our infrastructure and making our Bureau of Street Lighting more efficient and innovative in every way."

Load-Date: November 7, 2020


End of Document




Asian Americans split on affirmative action at UC; Proposition 16 divides university system's most overrepresented group of students.
Los Angeles Times
November 1, 2020 Sunday
Final Edition


Copyright 2020 Los Angeles Times All Rights Reserved
Section: CALIFORNIA; Metro Desk; Part B; Pg. 1
Length: 1641 words
Byline: Teresa Watanabe, Jennifer Lu
Body


Angela Li and Vivrd Prasanna have achieved the pinnacle of a public university education -- she a senior at UCLA, he a freshman at UC Berkeley. Both are children of immigrants, with Li's parents from China and Prasanna's from India.
They share values of hard work and high expectations. Li checked out school textbooks during the summer to get a head start on fall classes and in high school took test prep courses with money her working-class parents saved by giving up family vacations. Prasanna took college classes in data science as a high school student.
But when it comes to Proposition 16, the Tuesday ballot measure that would once again allow affirmative action in public education, contracting and hiring, the two UC students and their families sharply diverge.
Li supports the measure as a way to expand diversity in education -- but her parents oppose it, suspicious that it will limit the enrollment of Asian Americans. Prasanna opposes it as the wrong way to deal with root causes of educational inequity, while his parents are torn over their twin desires to stand for civil rights and to ensure equal opportunity for their community.
Their diverse views reflect the complexity of the affirmative action issue among Asian Americans, who represent more than 50 ethnic subgroups with varying politics, histories in the United States and levels of income, education and English language ability. Those differences play out in their views of Proposition 16 and concerns about their place at the University of California.
Asian Americans predominate at UC and are significantly overrepresented -- making up 40.3% of in-state freshmen last year compared with their 19.9% share among California high school graduates eligible for UC admission. By comparison, Latinos made up 31.5% of UC freshmen and 44.7% of that qualified pool; whites were 20.6% at UC and 27% of eligible students and Black freshmen were 4.5% at UC and 4.2% of those who met systemwide admission standards.
As a result, some Asian Americans are nervous that they would be squeezed out to make room for others if Proposition 16 passes and allows preferential admissions on the basis of race, ethnicity and sex.
Even back during the 1980s, when affirmative action was legal in California, Asian Americans were overrepresented at UCLA and UC Berkeley and fought admission policies they believed sought to limit their enrollment at those campuses.
After Proposition 209 banned race-based preferential treatment, the gap between their high admission rates and those of other ethnic groups widened with their rigorous high school course loads, high GPAs and competitive test scores.
It's not clear what would happen if affirmative action were restored. UC regents support restoring affirmative action, saying it's needed to fully diversify campuses but recently voted to ban quotas, which the U.S. Supreme Court already has nixed. Board Chairman John A. Perez has said that race would become one of more than a dozen factors currently evaluated in applications.
At UC Berkeley, Chancellor Carol Christ has said she favors expanding capacity to make more room for everyone.
UCLA Vice Provost for Enrollment Management Youlonda Copeland-Morgan told The Times last week that she didn't know if Asian American enrollment would decline at UCLA but that the campus would continue to focus on outreach to those from less represented communities, such as Hmong, Laotian, Vietnamese and Philippine students.
Last year, UCLA offered freshman seats to eight California Hmong students compared with 1,241 Chinese Americans, who were the single largest racial or ethnic group admitted after Mexican Americans and whites, according to UC data.
Some Asian Americans try to divine the future by looking to private California colleges, which were not bound by Proposition 209's ban on affirmative action at public campuses. They are not encouraged: the top privates enroll smaller proportions of Asian American undergraduates than the 33.5% at UC campuses, including 23% at Stanford University, 21% at USC and 16.3% at Pomona College.
But experts say the impact of Proposition 16 will probably vary among Asian American subgroups.
"You can't paint Asian Americans with a broad brush," said Karthick Ramakrishnan, a UC Riverside professor of public policy and political science who founded AAPIData.com, which publishes demographic data and policy research on Asian Americans and Pacific Islanders. "Some will benefit and others stand to lose."
Overall, more Asian Americans support Proposition 16 than oppose it -- 35% vs. 21% with most of the rest undecided, according to a September survey of Asian American voters in California by AAPI Data and two other Asian American and Pacific Islander organizations.
But intriguing differences emerged in the survey.
Ethnic Chinese -- who make up a maximum of 38% of Asian American students admitted by UC, by far the largest subgroup -- opposed the measure, 37% to 30%. Among ethnic Philippine, Vietnamese, Japanese and Korean voters, support was greater than opposition but topped out at 38% with large numbers undecided at the time.
Voters of Indian descent were by far the most supportive of the affirmative action ballot measure, 58% vs. 17%. The strong support among Indian Americans is all the more striking because their share among Asian American California freshmen has surged from 7.8% in 1996 to 17.4% in 2019.
Indian Americans had one of the highest admission rates among their California ethnic Asian peers last fall, about 79%. And South Asians, who include those of Indian, Pakistani, Bangladeshi and Sri Lankan descent, made up the second-largest Asian American subgroup after ethnic Chinese.
If any subgroup loses out under Proposition 16, Ramakrishnan said, it would probably be ethnic Chinese and Indian students because of their size. Nonetheless, many Indian Americans support Proposition 16 -- and other progressive political causes -- that reflect both homeland and domestic realities, he said. Their families have adapted to affirmative action in India, which is used to grant preferential access to many colleges in India based on caste, and are sensitive to race-based bias and discrimination as a darker-skinned minority in the United States, Ramakrishnan said.
Aidan Arasasingham, a South Asian high achiever and the son of Sri Lankan immigrants, is a UCLA senior in global studies and president of the UC Student Assn. He supports Proposition 16, as do the UC Student Assn. and most of the UCLA Asian Pacific Coalition's 19 member groups. Arasasingham said all students, Asian Americans included, will be enriched by a more diverse campus -- a point affirmed by research.
One synthesis of more than 500 studies found that greater cross-racial interaction is associated with lower levels of prejudice; other researchers have found that diversity spurs growth in cognitive skills, academic engagement and innovative problem solving, according to an October analysis by William C. Kidder for the Civil Rights Project at UCLA.
"Students ... across campuses really strongly believe that diversity is an essential part of our experience," Arasasingham said. "When we graduate, we're going to be in a world where we're engaging and interacting with communities that don't look like our own and it's setting us up for failure if we don't have an understanding of them."
Some Asian Americans, however, say their personal experiences have soured them on affirmative action.
Ling Kong, a Silicon Valley design engineer and mother of two school-age children, said her ethnic Chinese family faced bias in their native Malaysia because preferential college access is given to Malays and Indians. Her failure to get into local colleges there led her to study electrical engineering at Iowa and Arizona state universities.
"I don't want my kids to be treated differently on the basis of race," she said.
Kong, who describes herself as a liberal Democrat supportive of diversity and inclusion, said she opposes Proposition 16 because she doesn't believe it will solve educational inequity. Her experience on the Milpitas Unified School District's community advisory board and her children's school site council has shown her that affordable preschools and quality K-12 schools with adequate funding in all neighborhoods are needed, she said.
"Prop. 16 doesn't address any of this," she said. "It's just a quick fix and it won't really help close the achievement gap or solve systemic inequity."
For state Assemblyman Al Muratsuchi (D-Rolling Hills Estates), the debate sparks a sense of deja vu. As a UC Berkeley student in the 1980s, he joined protests over changes in campus admission policies, such as increasing the weight of verbal over math skills, that were seen by many Asian Americans as efforts to limit their surging enrollment.
Reviews of such allegations at Berkeley and UCLA by faculty, state and federal authorities generally found that some admission policies had hurt Asian Americans but were not intentionally discriminatory.
Muratsuchi supports Proposition 16 but says he will watch to make sure that a de facto ceiling is not placed on Asian American enrollment if it passes.
In what is supposed to be a race-blind environment today, he said, UC data suggest different standards are used for Asian Americans, who were admitted into the fall 2020 California freshman class with SAT scores up to 310 points higher than those for underrepresented minority students and up to 80 points higher than those for whites. UC, however, is eliminating the use of standardized test scores in admissions decisions.
"While I support the concept of affirmative action, we need to make sure there is transparency and accountability in how it is implemented so this complicated racial balancing act doesn't result in discriminating against any racial groups," he said.

Graphic

 
PHOTO: AMY HO, a student at UCLA, supports Proposition 16. More Asian Americans support the measure than oppose it, but significant numbers are undecided.  PHOTOGRAPHER:Carolyn Cole Los Angeles Times PHOTO: ASIAN AMERICAN students rally last month on the UCLA campus in support of Proposition 16, which would repeal the statewide ban on affirmative action.  PHOTOGRAPHER:Carolyn Cole Los Angeles Times 

Load-Date: November 1, 2020


End of Document




    
     
  Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved.   
  California Healthline'It's Science, Stupid': A School Subject Emerges as a Hot-Button Political Issue
       California Healthline
October 30, 2020


  Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved.   


Length: 1554 words
Byline: Victoria Knight
Body

At the top of Dr. Hiral Tipirneni's to-do list if she wins her congressional race: work with other elected officials to encourage mask mandates and to beef up COVID-19 testing and contact tracing. Those choices are backed up by science, said Tipirneni, an emergency room physician running for Arizona's 6th Congressional District.  
     
  On the campaign trail, she has called on her opponent, Rep. David Schweikert (R-Ariz.), to denounce President Donald Trump's gathering of thousands for a rally in Arizona and his comments about slowing down COVID-19 testing.  
     
  "I believe in data; I believe in facts," Tipirneni told KHN. "I believe in science guiding us ... whether it's the opioid crisis or tax policy or immigration reform. Those decisions could be and should be driven by the data. Science is not partisan."  
     
  Tipirneni is one of four Democratic physicians running as challengers for Congress in 2020, all in closely watched races mostly rated as toss-ups. And it's not just doctors. The group 3.14 Action (named for the value of pi) is working to help elect more scientists to office, promoting on its website candidates such as Mark Kelly, an engineer and former astronaut, who is seeking a Senate seat in Arizona, and Nancy Goroff, who has a doctorate in chemistry and is running for Congress in New York. Science is an integral part of their policy platforms, with an emphasis on the coronavirus pandemic.  
     
  These candidates hope to become part of an expanding pro-science caucus that includes three Democratic physician incumbents facing election challenges.  
     
  The candidates present themselves as foils to Trump and other Republicans who they say have dismissed scientific evidence and public health recommendations to battle the pandemic. Although climate change has propelled some people with science backgrounds into politics in recent years, the coronavirus crisis has galvanized the movement in this election cycle.  
     
  Still, political scientists and pollsters said that while Democrats' use of "pro-science" messaging in their campaigns could help them get elected, it also may ultimately lead to increased polarization.  
     
  "We've sometimes seen a modest difference in political parties when it comes to scientists generally, but it's gotten a little bit bigger," said Cary Funk, director of science and society research at the Pew Research Center.  
     
  Conservatives deny that they ignore science or downplay its significance. They say that, instead, Democrats often take positions that stifle scientific innovation by increasing taxes and regulation, citing research and development in the pharmaceutical field as an example.  
     
  "Democrats calling themselves the party of science sounds a bit like Trumpian self-flattery," wrote Doug Badger, a visiting fellow in domestic policy studies at the Heritage Foundation, in an email. He doesn't think Republicans and Democrats approach science differently since most research is conducted far from the political sphere.  
     
  This year, several Republican doctors are running for the first time for Congress, including Dr. Leo Valentín in Florida, Dr. Ronny Jackson, previously Trump's White House physician, in Texas. Dr. Roger Marshall, a current member of the House, is facing Democratic physician Dr. Barbara Bollier in the race for Kansas' open Senate seat. A cadre of Republican doctors already serve in Congress, with 11 in the House and three in the Senate.  
     
  Rep. Phil Roe (R-Tenn.), a physician who is a co-chair of the House GOP Doctors Caucus, said that sharing medical backgrounds has brought him together with Democratic doctors and other health professionals to work on health policy.  
     
  But new political action committees -- for instance, Doctors in Politics -- have cropped up with the goal of running up the score on the left.  
     
  Doctors in Politics was formed this year by a group of physicians who were frustrated by what they viewed as a failed federal response to COVID-19. The group's aim is to elect 50 Democratic or independent doctors to political office by 2022, said Dr. Dona Murphey, one of the group's founders and a neurologist. But for now, they're focused on 2020.  
     
  According to David Lazer, a professor of political science and computer science at Northeastern University in Boston and one of the leaders of a COVID-19 polling consortium, their timing might be right.  
     
  "My intuition is that this is a good year to be running as a doctor or scientist," he said, pointing to a September survey from the consortium that showed trust in doctors and scientists is higher than trust in any other American institution or political entity.  
     
  Much of that may be traced to COVID-19. But, as the science surrounding the disease has been on nearly everyone's mind, differing attitudes among the American electorate are likely to play out at the polls.  
     
  "The growing political divide around coronavirus is also seen in terms of trust in medical scientists," Funk said.  
     
  Funk pointed to a May report by the Pew Research Center that showed overall public trust increased in medical scientists since 2019, but that increase is attributed to a growing trust among Democrats. Republicans' trust in scientists stayed about the same from 2019 through the first few months of the pandemic. A more recent survey from Pew showed that those on the political right are often less trusting of scientists than are those on the left.  
     
  Trump's rhetoric around science may be contributing to the split. During the pandemic, the president has dismissed public health advice from experts, touted unproven coronavirus treatments and questioned the efficacy of masks.  
     
  "The Trump administration has systematically done everything it could to downplay, dismiss or deny science," said Michael Gerrard, an environmental lawyer and professor at Columbia University. "This is most prominent with climate change and now with the coronavirus, but it's all across the board." Gerrard has tracked more than 300 situations in which he found scientific initiatives to be restricted or questioned by federal officials since 2016, 19 of them COVID-related.  
     
  Such frustration during the course of this election cycle has become palpable, with organizations that don't normally step into the political fray doing so.  
     
  The presidents of the National Academy of Sciences and National Academy of Medicine, for instance, released a joint statement Sept. 24 expressing alarm over what they considered to be political interference in the response to COVID-19 by the president.  
     
  And a multitude of scientific publications have spoken out. Scientific American formally endorsed the Democratic presidential candidate, former Vice President Joe Biden -- its first time making such a political pick in its 175-year history. The journal Nature has also endorsed Biden. The New England Journal of Medicine published a scathing critique -- "Dying in a Leadership Vacuum" -- of the federal government's pandemic response. Although it was not a formal endorsement of any candidate, the editorial said, "Our current political leaders have demonstrated that they are dangerously incompetent."  
     
  Such picking sides has led to another phenomenon, said Dominik Stecu?a, an assistant professor of political science at Colorado State University.  
     
  "You'll see yard signs that say 'Science is real' and with other messages clearly aligning scientists with a group on the political spectrum," he said. But Stecu?a said pro-science messaging by Democrats could lead to deeper fissures in public opinion.  
     
  "From a scientist's point of view, it hurts the goals that you're trying to achieve," he said, "because what ends up happening is that, increasingly, Republicans treat scientists as an out-party group, a constituency of the Democrats."  
     
  Others offer a different take.  
     
  "I really reject that premise," said Rep. Lauren Underwood (D-Ill.), a registered nurse who flipped her district to Democratic when she was elected in 2018 on a pro-science platform. She's running for reelection this year. "I just don't think that's true. The American people may be uncomfortable with some findings and recommendations, but this is a core value set in our community."  
     
  "We learn science in every grade, in every level of education," she said. "There may be some partisan differences in how we take partisan findings, but I think it's dangerous if we start to presume that science is polarizing."  
     
  She also thinks her background as a health professional helps her in Congress to work across the aisle. For instance, she worked with Rep. Roe last spring to introduce legislation on protecting the medical supply chain.  
     
  Roe also dismissed the idea that science -- especially regarding the pandemic and the development of a COVID-19 vaccine -- is further polarizing the electorate. In his view, it's less about science and more about the race for the White House.  
     
  "Of course it's been politicized, it's a political year," said Roe. "If we hadn't had an election, I think it would look different."  
     
  This story was produced by Kaiser Health News, an editorially independent program of the Kaiser Family Foundation.


End of Document




Studies Link Viewership of Fox News to Reduced Pandemic Precaution
The Hollywood Reporter
October 28, 2020 Wednesday


Copyright 2020 Prometheus Global Media, LLC All Rights Reserved


Section: NEWS; TAG
Length: 1145 words
Byline: Gary Baum
Highlight: Four academic papers show a correspondence between watching Fox News and employing less stringent COVID-19 safety measures, a claim Fox calls cherry-picking.


Body


Fox News' coronavirus coverage is increasingly the focus of academic research which has concluded the network contributed to an adverse public health situation during the pandemic. All of the studies are still working papers, meaning none have yet been published in a peer-reviewed journal. Still, the groups' individual work - developed at the outset without knowledge of each other - is in itself "mutually affirming," in the words of Elliott Ash, the chair of law, economics and data science at ETH Zurich and member of one of the research coteries.
The first paper, which became public in April, examined how Fox's two most popular shows, Hannity and Tucker Carlson Tonight, had diverged in their coverage of COVID-19 early on. Tucker Carlson's approach was more urgent than that of Sean Hannity - with clear results on viewers. "  ," published by the University of Chicago's Becker Friedman Institute for Economics, determined that "greater exposure to Hannity relative to Tucker Carlson Tonight increased the number of total cases and deaths in the initial stages of the coronavirus pandemic."
The    - which use Fox's relative cable channel position across local markets to gauge the effect of the network on consumer behavior - have since arrived at a similar conclusion: Fox's editorial coverage persuaded its viewership away from the necessity of physical distancing, which public health authorities believe would've helped stem the spread of the pandemic. 
While some cable news viewers, especially with partisan leanings, seek out certain media outlets, others will watch whichever channel they come upon first, typically the one positioned lowest when clicking up from 2 on a U.S. television set. (Media companies are cognizant of this, which is why a prime position as far down the dial as possible is a coveted asset.) "We are mainly estimating the effect on this group: the 'compliers,'" explains Maxim Ananyev, a co-author of one of the studies and a fellow at the Melbourne Institute of Applied Economic and Social Research.
The three independent research contingents part ways, however, on whether Fox's messaging on social distancing resulted in excess illness and death among its audience. One, citing a desire to remain in what they perceive to be their academic lane, declined to attempt such measurement: "None of us are trained as epidemiologists," notes Columbia Business School professor Andrey Simonov, a specialist in quantitative marketing, of his cohort. "We are economists and political scientists." 
The others were split on whether the network's activity may have contributed to the demise of a portion of its audience: one found no proof, the other evidence that Fox's role was "consequential for mortality."
The scholars behind the Hannity-Carlson study went furthest. Their modeling analysis found that, early in the pandemic, greater exposure to Hannity relative to Tucker Carlson Tonight is associated with a greater number of COVID-19 cases and deaths. 
After Hannity's views merged with Carlson's, the effect on cases plateaued and declined, the study claims. The network responded to all four studies with a statement: "Fox News has been covering the threat of COVID-19 since mid-January and was among the first networks to spotlight both the severity of the virus and to warn the American public that cases would skyrocket into the hundred thousand range. These cherry-picking studies blatantly ignore key moments of our pandemic coverage and are nothing more than a transparent PR stunt by organizations seeking media attention."
Fox also sought to counter the academics' work in an accompanying facts sheet. It pointed to its dedicated coronavirus town halls, including with President Trump, as well as PSAs about the virus and new hires providing on-air medical expertise. In addition, the network provided a chronological breakdown of its coverage, highlighting how anchors Neil Cavuto and Maria Bartiromo had first raised concerns about COVID-19 in the early weeks of 2020, before it was a common discussion topic in the U.S., and a Feb. 27 moment on Hannity during which the host proclaimed: "Make no mistake. Coronavirus, it is dangerous."
As for the studies themselves, Fox directed attention to a Bloomberg Opinion  by University of Chicago public policy professor Anthony Fowler, who questioned whether COVID-19 social science research in general is being recklessly fast-tracked, citing in particular the Hannity-Carlson paper, which he ventured "was likely written in just a few days," as "the kind of study that might make one skeptical in normal times" before critiquing its methodology. "Maybe conservative commentators like Sean Hannity have exacerbated the spread of Covid-19," he wrote, "but it's dangerous for social scientists to publicize these kinds of results before they have been carefully vetted."
The studies, currently undergoing revisions, are part of the burgeoning field of media economics, which scrutinizes television coverage's impact on social behaviors. (In 2014, researchers found that MTV's 16 and Pregnant led to fewer teen births.) Aakaash Rao at the Institute for Quantitative Social Science at Harvard, one of the authors of the Hannity-Carlson inquiry, acknowledges a short turnaround time - "we wrote our first draft over the course of weeks, while a typical completed paper might take months or years" - but is confident in the group's approach. "We think the best way to ensure that our results are correct is to release the paper to the scientific community and solicit feedback from colleagues, as we've done," he says.
Fox disclosed on Oct. 26 that multiple employees at its Manhattan headquarters had tested positive for COVID-19 - shortly after a New York Times report that the network president as well as top on-air talent, including Bret Baier, Martha MacCallum and Dana Perino, had been advised to quarantine after being exposed to someone on a charter flight who later tested positive for the virus.
Some of the social scientists are now employing similar methods to look into Fox's influence on climate change skepticism. However, across the research groups, right now they're most attentive to how the network will cover the next phase of the pandemic - immunization. "Going forward, media will be persuasive," says Simonov. "We know it will be important in how people will comply with vaccination. If there's very different messaging and polarization, it's reasonable to expect similar effects: there will be different levels of compliance as well."
The Studies:
"  ," by Leonardo Bursztyn, Aakaash Rao, Christopher Roth, and David Yanagizawa-Drott.
"  ," by Andrey Simonov, Szymon Sacher, Jean-Pierre Dubé, and Shirsho Biswas.
"  ," by Elliott Ash, Sergio Galletta, Dominik Hangartner, Yotam Margalit and Matteo Pinna.
"  ," by Maxim Ananyev, Michael Poyker, and Yuan Tian.
Link to Image

Load-Date: October 28, 2020


End of Document




NASA Awards Los Angeles $1.3 Million for Air Quality Monitoring Program
City News Service
October 7, 2020 Wednesday 11:38 AM PST


Copyright 2020 City News Service, Inc. All Rights Reserved
No City News Service material may be republished without the express written permission of the City News Service, Inc.
Length: 405 words
Body


LOS ANGELES (CNS) - Mayor Eric Garcetti announced today that Los Angeles has been awarded $1.3 million to lead a NASA project that uses existing satellites to better understand, predict and address poor air quality.
"Clean air is a fundamental human right for people everywhere and securing it starts at home, with bold steps to fight air pollution and protect the health of communities left to deal with the consequences of dirty air for far too long," Garcetti said.
NASA's investment will fund two years of research to create a platform that integrates data from ground and spaced-based air quality measurements, according to the mayor's office.
"Our partnership with NASA hands our city another vital tool to predict air quality in our neighborhoods, measure the effectiveness of our efforts to clean our air, create a model for counterparts worldwide and deliver on our promise of equity and sustainability in Los Angeles," Garcetti said.
When complete, the model will be able to provide local officials with new information to predict local air quality issues and receive detailed feedback on the efforts to reduce air pollution citywide.
"A predictive model based on machine learning, such as the one developed by the city of Los Angeles, will enhance and enable focused air quality science investigations and predictions by facilitating the access, integration, understanding and visualization of disparate datasets locally to satellite sensors," said Jacqueline Le Moigne, the advanced information systems technology program manager for NASA's Earth Science Technology Office.
The project -- titled Predicting What We Breathe -- will also provide city officials with data to address air quality measurement gaps that exist in underserved communities, which are often those most adversely impacted by air pollution, the mayor's office said.
As a part of the grant, hundreds of small-scale sensors will be installed in areas of South Los Angeles, Wilmington and the San Fernando Valley.
Los Angeles County is home to more than half of the state's most disadvantaged communities, as determined by CalEnviroScreen, a ranking that uses air pollution and asthma rates as key indicators.'
The NASA program is a partnership between a group of public, private and academic organizations, including the Southern California Air Quality Management District, Safecast, OpenAQ, SmartAirLA, Cal State Los Angeles and the L.A. Data Science Federation.

Load-Date: October 8, 2020


End of Document




Super Aggregator ScreenHits TV Closes $2 Million in Funding
The Hollywood Reporter
October 2, 2020 Friday


Copyright 2020 Prometheus Global Media, LLC All Rights Reserved


Section: NEWS; TAG
Length: 399 words
Byline: Georg Szalai
Highlight: U.K.-based super aggregator service ScreenHits TV closed a further $2 million in funding, which will help bankroll its fall rollout across the U.S. and U.K. The service allows users to access content across all their streaming services, as well as live TV, via a single interface.


Body


ScreenHits TV, a new super-aggregator service for streaming content, has raised a further $2 million in Series A funding. 
The investment brings the companys total funding to $6 million and will be useful as ScreenHits TV prepares to launch for public use across the U.S. and U.K. this fall. The Series A round included U.K.-based angel investors Edward Mackay, Rory Flemming, LordReay, Jonathan Marshall, and Paul Atkinson of Par Equity, who were part of previous investment rounds in ScreenHits TV. 
The company has also secured match funding for the round via Britain's Future Fund, a government program for UK-based entrepreneurs that matches or supplements funding secured from third-party investors.
Based in London, ScreenHits TV is pitching itself as a one-stop-shop to help consumers navigate the increasingly crowded streaming landscape. The ScreenHits TV interface lets users search for content across their pre-existing streaming subscription services, as well as across online videos, and live channels, and directly stream the content within the platform without switching between apps and SVOD interfaces. 
ScreenHits founder and CEO Rose Adkins Hulse said the company aims to help both frustrated consumers, content creators, and streaming services by making it easier for users to find the shows they want to watch, and help drive traffic to platforms. 
Customers currently are spending 90 percent of their time on just one app out of all the apps they may subscribe to, and not fully taking advantage of their other subscriptions, which often leads to subscription cancellation, said Hulse. ScreenHits TV gives the consumer equal discovery opportunities on each platform and reminds them what they are watching across a number of apps and channels, minimizing churn for providers.
Hulse said the new funding would allow the company to continue to invest in marketing and advertising to increase awareness of the service as it rolls out globally. 
ScreenHits TV is truly disrupting the home viewing marketplace, through the creation of an intuitive platform and the thoughtful use of data science to improve content discovery across streaming services, said investor Atkinson. With impressive proprietary technology and an extremely user-friendly experience, ScreenHits TV has the opportunity to meaningfully change how people worldwide find and consume content.
Link to Image

Load-Date: October 2, 2020


End of Document




There is hard data that shows "Bernie Bros" are a myth
Salon.com
March 9, 2020 Monday


Copyright 2020 Salon.com, LLC. All Rights Reserved
Length: 1649 words
Highlight: A computational social scientist's study shows Bernie's Twitter followers act pretty much the same as everyone else


Body


Link to Image
Bernie Sanders Twitter/AP Photo/Paul Sancya
Mainstream pundits and politicians continue to obsess over the stereotype of the "Bernie Bro," a perfervid horde of Bernie Sanders supporters who supposedly stop at nothing to harass his opponents online. Elizabeth Warren,            Hillary Clinton and New York Times columnist            Bret Stephens have all helped perpetuate the idea that Sanders' supporters are somehow uniquely cruel, despite Sanders' platform and policy proposal being the most            humane of all the candidates.
The only problem? The evidence that Sanders supporters are uniquely cruel online, compared to any other candidates' supporters, is scant; much of the discourse around Bernie Bros seems to rely on            skewed anecdotes that don't stand up to scrutiny. Many Sanders supporters suspect that the stereotype is perpetuated in bad faith to help torpedo his candidacy. 
A few weeks ago I penned a story for Salon attempting to qualitatively disprove the Bernie Bro myth by pulling from psychological theory and the nature of online behavior. To summarize my conclusions: First, there is a general tendency for online behavior to be negative, known as the            online disinhibition effect - but it affects all people equally, not merely Sanders' supporters. Second, pundits systematically ignore when other candidates' supporters are mean online, perhaps because of the aforementioned established stereotype; in this sense, the Bernie Bro is not dissimilar from other political canards like the "welfare queen." Third, Twitter is not a representative sample size of the population, and is so prone to harboring propaganda outfits and bots such that it is not a reliable way of gauging public opinion. 
Now, to add to this qualitative assessment, there is quantitative evidence, too - reaped from studying hundreds of thousands of interactions online - that reveals the Bernie Bro myth as, well, a myth. Jeff Winchell, a computational social scientist and graduate student at Harvard University, crunched the numbers on tweet data and found that Sanders' supporters online behave the same as everyone else. Winchell used what is called a sentiment analysis, a technique used both in the digital humanities and in e-commerce, to gauge emotional intent from social media data.
"Bernie followers act pretty much the same on Twitter as any other follower," Winchell says of his results. "There is one key difference that Twitter users and media don't seem to be aware of.... Bernie has a lot more Twitter followers than Twitter followers of other Democrat's campaigns," he added, noting that this may be partly what helps perpetuate the myth.
I interviewed him about his work and his results over email; as usual, this interview has been condensed and edited for print. 
First, for those who haven't heard of this technique, what is a sentiment analysis?
Sentiment analysis summarizes human expression into various scores. Most commonly the score is how negative or positive it is. But it can also be used to evaluate subjectivity (for instance, is a politician's statement factual or mostly opinionated?). Even taking the simpler text analysis, there are multiple challenges due to sarcasm, negations (e.g "I don't like their service", "After what he did, this will be his last project"), ambiguity (words that are negative or positive depending on their context), and [the fact that] texts can contain both positive and negative parts.
How are sentiment analyses used? What are other examples of this technique being used?
The overwhelming application of sentiment analysis is in e-commerce (for instance, scoring how positive/negative customer feedback is). Customer service surveys are often analyzed this way. Marketing uses sentiment analysis to test product acceptance.
Other commercial applications are in recommendations. While a system may have the user given an overall rating, analyzing the comments they provide can identify the sentiment on subtopics within. 
So tell me about the sentiment analysis script that you wrote to study online behavior among different politicians' followers. How did this work?
I downloaded all the followers of the Twitter accounts of the nine most popular Democratic presidential candidates and the president ([around] 100 million Twitter accounts). I then randomly chose followers from them and downloaded all their tweets from 2015 to the present.
I have run two different sentiment analysis algorithms on these tweets. So far, nearly 6.8 million tweets from 280,000 Twitter accounts have been analyzed out of the 100 million-plus tweets I currently have downloaded (I continue downloading more).
One sentiment analysis algorithm uses a well-regarded example of grammar/word dictionary sentiment rules that were popular 5 to 10 years ago before deep learning became popular. This one is identified by the Python libary's name, Textblob.
The other algorithm is Microsoft's supervised deep learning-based algorithm with default parameters. To those unfamiliar with deep learning, the number of parameters in this model is in the millions, and no human can be expected to understand them. The deep learning model learns/generalizes from examples of text given sentiment ratings by humans through millions of trials, each time evaluating how well it predicts the results and passing that model and accuracy to the next iteration.
https://twitter.com/CompSocialSci/status/1236091350640201728
The categories of negative and very negative are based on ranges of values in the two algorithm's outputs. Textblob generates a number from most negative (-1) to most positive (+1). I classified scores of [below]  -0.75 as very negative and -.75 to -.5 as negative. Microsoft's algorithm predicts the chance that some text is classified as positive. Based on the frequencies of a specific chance, I separated the lowest 1.5 percent of tweet ratings as very negative and the lowest 1.5 percent to 5 percent of all tweet ratings as negative.    
What did your results find?
The chance that some tweet is negative when it comes from a follower of candidate X is pretty much the same as if it came from a follower of candidate Y. 
This uses two different algorithms, once very sophisticated (Microsoft's supervised Deep Learning-based model), the other a good algorithm based on the algorithm standards of 5 to 10 years ago (Textblob's grammar/dictionary-based rules). Microsoft's algorithm calculates the chance a tweet is positive. Textblob's rates the tweet from most negative (-1) to most positive (+1). But the variation of these measures changes little among tweets from followers from different candidates.  
I deliberately round my numbers to 1 digit for smaller samples (negative or very negative percentage) or 2 digits if it's about an average over all the tweets. I don't like false accuracy and it is rampant in the political media. Any NLP [Natural Language Processing] expert will tell you that reducing a tweet to a single number denoting its negativity/positivity is not an exact science. So the rounding reflects that uncertainty.
Given this data, what do you think of the "Bernie Bro" narrative about his online supporters?
Bernie followers act pretty much the same on Twitter as any other follower. There is one key difference that Twitter users and media don't seem to be aware of. Bernie has a lot more Twitter followers than Twitter followers of other Democrat's campaigns.
Link to Image
People responding to hundreds of millions of people online tend to dehumanize others. They remember that someone is female/male or follows some candidate or is of some race, but they frequently don't pay attention to differentiate actions of one member of that group versus another. So rather than consider how frequently an individual of some group acts, they think of how frequently the group acts as a whole. If they interact with many more members of one group than another, that perception of the group is magnified by the number of members they see.
Interesting. Did your opinion change after doing this little analysis?

 Yes. I believed that Bernie's followers are more likely to like him because they are more likely to experience the very negative life circumstances that Bernie Sanders wants to fix. People in a negative situation are more likely to interact negatively with people, particularly those anonymous online people that they have no in-person relationship with. So I had anticipated that Bernie's followers on average would have a much higher chance to be negative. This does not appear to be the case or at least not as much as the claims I read on Twitter, political media reports or on TV.
Is there actually any difference between different candidates' supporters online behavior, based on this?
As a data scientist, I am usually skeptical of any result. So I'll say maybe not or at least much less than claimed.
I still would like to dig deeper into this. This analysis looks at all tweets. I would like to look just at twitter interactions between candidate's supporters, look at tweets responding or mentioning media professionals. I want to use some algorithms in the research that evaluate hate speech, racism, sexism. I'd like to look at specific topics of discussion, and possibly evaluate the influence of negative tweets (eg. retweets and number of followers who could see a tweet/retweet).

What is your academic background?
I have a bachelor's degree in math from Northwestern. I then worked in healthcare analytics with very large databases, branched into other applications of large scale data analysis before recently returning to grad school at Harvard to study data science. While there my interest in psychology and sociology has led me to pursue applications of data science in the social sciences to help people. 
This story was updated on March 10 with additional interview questions to add context.

Load-Date: April 13, 2020


End of Document




Biden campaign adds more staff in Texas
Salon.com
September 14, 2020 Monday


Copyright 2020 Salon.com, LLC. All Rights Reserved
Length: 481 words
Highlight: Biden's campaign has now named 19 staffers in Texas


Body


Link to Image
Donald Trump and Joe Biden | Texas Photo illustration by Salon/Getty Images
Joe Biden's campaign is expanding its staff in Texas, bringing on 13 more people as the state continues to look competitive with just over seven weeks to go before the November election.
The Democratic nominee's latest hires, shared first with The Texas Tribune, include several experienced Democratic operatives from the state. They include Dallas Jones, a Houston political consultant who will serve as Biden's Texas political director, and Jackie Uresti and Jerry Phillips, who will each serve as political advisers to the campaign in Texas. Uresti was Hillary Clinton's 2016 state director, while Phillips brings deep experience around Texas House politics and previously was executive director of the House Democratic Campaign Committee.
Biden's campaign has also named Bethanie Olivan as digital organizing director and Terry Bermea as organizing director. Olivan recently held similar roles for the state party and Juli n Castro's presidential campaign, while Bermea is the former organizing director for Battleground Texas and was deputy state director for Michael Bloomberg's White House bid earlier this year.
The campaign also said David Gins will serve as state operations director. Gins is a former U.S. Senate staffer who has since worked for the LGBTQ Victory Fund and the data science company Civis Analytics.
The campaign announced thatVictoria Godinez, a former staffer to state Rep. Diego Bernal of San Antonio, is being hiredas communications associate.
Rounding out the hires are six deputy coalitions directors, most with varying levels of Texas political experience: Deidre Rasheed, Karim Farishta, Dominique Calhoun, Teri Ervin, Lola Wilson and Joseph Ramirez.
Biden's campaign has now named 19 staffers in Texas, following its initial hiring announcement in early August. The first six hires included a state director, Rebecca Acu a.
For months, polls have pointed to a close contest in Texas between Biden, the former vice president, and President Donald Trump. While Biden's campaign has discussed Texas as competitive territory and made TV ad reservations here this fall, Trump officials continue to dismiss the notion that Biden will seriously contend in the historically red state.
The Trump campaign has touted what it says isa far bigger - and longer established - presence in the state, though it has not provided specific numbers.
Still, the Trump campaign has made recent moves to shore up support in Texas. Lt. Gov. Dan Patrick, who chairs Trump's reelection effort in Texas, led a bus tour through the state earlier this month, while another group of surrogates went on a Women for Trump bus tour in the state this past weekend.
This article originally appeared in The Texas Tribune at https://www.texastribune.org/2020/09/14/biden-texas-campaign-staff-president-election/.

Load-Date: September 14, 2020


End of Document




CDC softened school reopening guidelines criticized in fringe group's letter to President Trump
Salon.com
September 18, 2020 Friday


Copyright 2020 Salon.com, LLC. All Rights Reserved
Length: 2013 words
Highlight: "The CDC 'considerations' are entirely inappropriate, and must be rewritten from top to bottom" the group wrote


Body


Link to Image
Donald Trump | Center for Disease Control (CDC) headquarters Getty Images/Salon
Quack medical opinions promoted by a right-wing organization appear to have influenced federal guidelines for reopening schools amid the coronavirus pandemic.
The month after the Centers for Disease Control and Prevention (CDC) published its initial guidelines for school reopenings, President Donald Trump received a letter bashing the recommendations from a group known as the Tea Party Patriots (TPP)
The group, which has raised a combined $24 million for conservatives since 2014, is behind a website called the "Second Opinion Project," where doctors espousing marginal medical opinions attack mainstream scientific consensus on the new coronavirus. The TPP was also the organizing force behind America's Frontline Doctors, a fringe collection of medical professionals who drew fierce backlash in Julyfor a viral video promoting scientifically inaccurate information about the virus. One doctor in the video, later revealed to have preached that diseases are caused by dream sex with demons, falsely claimed that scientists were developing a vaccine to destroy a gene in the human brain associated with religion. 
Recently, Democratic senators have demanded an investigation into whether administration officials pressured the CDC to soften its school reopening guidelines in service of the president's political interests. While many of the talking points in the TPP letter are shared by a number of other conservative think tanks, the letter was not only published on the group's website but also sent directly to President Donald Trump at the White House.
Trump admitted in private to journalist Bob Woodward that he had purposely downplayed the risks posed by the looming pandemic in order to avoid creating a public panic, even though he had long known the coronavirus was "deadly stuff." This included acknowledging the threat it posed to young people.
"Just today and yesterday, some startling facts came out. It's not just old - older," Trump told Woodward on March 19. "Young people, too - plenty of young people."
The president publicly said as recently as Aug. 5 that children were "almost immune."
"While Donald Trump was downplaying the pandemic and calling children 'COVID stoppers,' he acknowledged behind closed doors that children were just as vulnerable to the virus. Trump knew rushing schools to re-open was dangerous. Still, he did it anyway, because he thought gambling away their health could help him win re-election," Kyle Morse, a spokesperson from Democratic PAC American Bridge 21st Century, told Salon. "With lives hanging in the balance, Donald Trump is now doctoring reports and trying to cover up just how bad infection rates among school-aged children has become. Trump views students and teachers as political pawns, and we cannot trust him with one more day - let alone four more years."
The New York Times revealed Thursday that CDC scientists had not written a controversial guideline released last month saying it was not necessary to test individuals without symptoms of COVID-19 - even if they had been exposed to the virus. The guideline from the Department of Health and Human Services was reportedly "dropped" into the agency's website against protocol and the objections of CDC experts.
In mid-May, the CDC published its first road map for reopening the economy - including child-care, restaurants and mass transit - which warned that some institutions should remain closed. The CDC laid out a number of social distancing policies for schools: staggered arrival times; desks at least 6 feet apart, facing the same direction; eating lunch in classrooms; face coverings for all staff; and daily temperature checks for everyone.
Two weeks later, the TPP wrote to Trump demanding a "top to bottom" overhaul as it , criticized those safety recommendations:
We have reviewed these "considerations," and find them wholly out of touch with the way people live in the real world that is America in 2020. They are totally unworkable, and we urge senior officials to review and revise these new CDC "considerations" to bring them into line with how real people live. [...] The CDC "considerations" are entirely inappropriate, and must be rewritten from top to bottom.
The letter (available in full here) went on to make a number of recommendations which ultimately found their way into the CDC's school reopening guidelines published on July 23. As CNN noted, that guidance came down "hard in favor of opening schools." Some of the recommendations from the TPP included the following:
 Eliminate disinfection and cleaning protocols
 Drop recommendation to wear face coverings
 Remove "dystopian" suggestions to reconfigure classrooms for social distancing
 Get rid of "impossible" suggestions to stagger school schedule or create small groups
 Include importance of schools in delivering nutrition to children
 Emphasize low risk posed by COVID-19 to children
The TPP was not the only non-medical conservative group whose recommended changes found their way into CDC guidelines. As Trump attacked the CDC, conservative groups, including the Heritage Foundation and Cato Institute, published their own reopening recommendations. Members of the same groups were also appointed to Trump's Coronavirus Reopening Task Force.
Politico posited in a July 8 article that Trump's drive to reopen the country's schools was also about the economy and his re-election odds as "the issues are interlinked": 
With children out of the house, they argue, parents can more easily return to work and juice the economy - something even the president's allies consider a necessity for Trump to win re-election. And with Trump's sagging poll numbers against presumptive 2020 rival Joe Biden, aides also hope the campaign for in-person schooling will play well with the female and suburban voters the president needs to remain in office.
In early July, the president vowed to personally intervene as he blasted the CDC's guidelines as "very tough & expensive."
"I disagree with @CDCgov on their very tough & expensive guidelines for opening schools. While they want them open, they are asking schools to do very impractical things. I will be meeting with them!!!" Trump tweeted on July 7.
Vice President Mike Pence joined the fight, saying at a press conference that day that Trump did not want the CDC guidance to create reopening "barriers." 
The original recommendations emphasized caution: "The more people a student or staff member interacts with, and the longer that interaction, the higher the risk of COVID-19 spread." They also laid out a risk table for distancing scenarios, ranging from complete virtual learning to "full sized, in-person classes, activities and events" with students "not spaced apart."
The TPP letter claimed those "strange" recommendations were so extreme that they could only be implemented through "the use of force":
You're going to enforce social distancing on a playground or in gym class? Impossible. You're going to order children not to use the jungle gyms, the slides, the horizontal ladders, and the carousels, because they cannot be disinfected between each use? And we are expected to believe this will succeed without the use of force? Try telling a nine-year-old he can't climb on the horizontal ladders, or a five-year-old she can't use the slide. Good luck with that.
The TPP complained about the inefficiencies of sanitizing surfaces. It also wanted the CDC to emphasize that children were unlikely to get sick and spread the disease. In addition to reflecting those concerns, the revised CDC guidelines emphasized another of the group's qualms: the "social, emotional, and mental health needs of students" that could be impacted by "potential learning loss."
The letter also inaccurately cited a medical journal report to argue against requirements for face coverings included in the CDC recommendations:
This insistence on masks flies in the face of data, science, and common sense. A study that appeared in The New England Journal of Medicine on May 21st reports that masks provide 'little, if any, protection from infection.' Furthermore, the possibility of 'catching COVID-19 from a passing interaction in a public space is therefore minimal.' So, why the insistence on masks? The article provides this explanation: 'In many cases, the desire for widespread masking is a reflexive reaction to anxiety over the pandemic.'
The authors of that study wrote a clarifying letter to the NEJM pushing back against this misinterpretation, which had become widespread. It read, "In truth, the intent of our article was to push for more masking - not less."
"We did state in the article that 'wearing a mask outside health care facilities offers little, if any, protection from infection,' but as the rest of the paragraph makes clear, we intended this statement to apply to passing encounters in public spaces - not sustained interactions within closed environments," they said.
"We therefore strongly support the calls of public health agencies for all people to wear masks when circumstances compel them to be within 6 feet of others for sustained periods," the letter added. 
Nevertheless, the revised CDC guidelines tempered the mask guidance, saying that while it was "critical" that schools include cloth face coverings as part of a larger strategy, "more research and evaluation is needed" to determine how effective it would be in schools.
The TPP also criticized the CDC's recommendation that classes configure seating charts so that all students face forward, minimizing face-to-face contact, calling the idea "straight out of a dystopian science fiction novel." For support, the letter cited an ophthalmologist, or eye doctor, who claimed that the agency's recommendations would create a "dystopian environment for our children." Again, the CDC eased its recommendations in this case.
The early guidance also suggested that small groups of students and staff stick together throughout the day as much as possible, including staggered arrival and drop-off times. Those guidelines were also softened after the TPP letter told the president that schools had been intended to "prepare our next generation to be productive members of society - a society that will not always allow for social distancing, staggered scheduling, herding into small groups and the like."
While influential conservative think tanks combatted the guidance with white papers, the TPP - which has raised tens of millions of dollars for conservative causes - took a more grassroots approach.
Four days after the CDC released its revised guidance, the TPP funded and hosted a press conference in front of the Supreme Court featuring a group called "America's Frontline Doctors." Those individuals made false claims about COVID-19 "cures," and alleged that public health measures such as mask mandates and school closings were ineffective and unnecessary.
One of the speakers, Stella Immanuel, claimed that she had cured 350 COVID-19 patients using a hydroxychloroquine treatment. Immanuel said doctors who refused to use the Trump-backed trherapy, which by that time had its emergency authorization revoked by the Food and Drug Administration, were like the "good Germans who allow the Nazis to kill the Jews."
Trump and his eldest son, Donald Jr., both shared the video, but social media platforms soon began to strip it from their platforms for violating rules about promoting misinformation related to the pandemic. 
Trump, however, called those in the group "very respected doctors." He specifically singled out Immanuel - who did not receive her medical degree in the U.S. and espoused ridiculous theories, such as that diseases come from "demon sperm" - as having been particularly "spectacular."
"I thought she was very impressive in the sense that, from where she came - I don't know what country she comes from - but she said that she's had tremendous success with hundreds of different patients," the president said.
Immanuel's practice operates out of a strip mall in suburban Houston.

Load-Date: September 19, 2020


End of Document




CDC softened school reopening guidelines criticized in fringe group's letter to President Trump
Salon.com
September 18, 2020 Friday


Copyright 2020 Salon.com, LLC. All Rights Reserved
Length: 2013 words
Highlight: "The CDC 'considerations' are entirely inappropriate, and must be rewritten from top to bottom" the group wrote


Body


Link to Image
Donald Trump | Center for Disease Control (CDC) headquarters Getty Images/Salon
Quack medical opinions promoted by a right-wing organization appear to have influenced federal guidelines for reopening schools amid the coronavirus pandemic.
The month after the Centers for Disease Control and Prevention (CDC) published its initial guidelines for school reopenings, President Donald Trump received a letter bashing the recommendations from a group known as the Tea Party Patriots (TPP)
The group, which has raised a combined $24 million for conservatives since 2014, is behind a website called the "Second Opinion Project," where doctors espousing marginal medical opinions attack mainstream scientific consensus on the new coronavirus. The TPP was also the organizing force behind America's Frontline Doctors, a fringe collection of medical professionals who drew fierce backlash in Julyfor a viral video promoting scientifically inaccurate information about the virus. One doctor in the video, later revealed to have preached that diseases are caused by dream sex with demons, falsely claimed that scientists were developing a vaccine to destroy a gene in the human brain associated with religion. 
Recently, Democratic senators have demanded an investigation into whether administration officials pressured the CDC to soften its school reopening guidelines in service of the president's political interests. While many of the talking points in the TPP letter are shared by a number of other conservative think tanks, the letter was not only published on the group's website but also sent directly to President Donald Trump at the White House.
Trump admitted in private to journalist Bob Woodward that he had purposely downplayed the risks posed by the looming pandemic in order to avoid creating a public panic, even though he had long known the coronavirus was "deadly stuff." This included acknowledging the threat it posed to young people.
"Just today and yesterday, some startling facts came out. It's not just old - older," Trump told Woodward on March 19. "Young people, too - plenty of young people."
The president publicly said as recently as Aug. 5 that children were "almost immune."
"While Donald Trump was downplaying the pandemic and calling children 'COVID stoppers,' he acknowledged behind closed doors that children were just as vulnerable to the virus. Trump knew rushing schools to re-open was dangerous. Still, he did it anyway, because he thought gambling away their health could help him win re-election," Kyle Morse, a spokesperson from Democratic PAC American Bridge 21st Century, told Salon. "With lives hanging in the balance, Donald Trump is now doctoring reports and trying to cover up just how bad infection rates among school-aged children has become. Trump views students and teachers as political pawns, and we cannot trust him with one more day - let alone four more years."
The New York Times revealed Thursday that CDC scientists had not written a controversial guideline released last month saying it was not necessary to test individuals without symptoms of COVID-19 - even if they had been exposed to the virus. The guideline from the Department of Health and Human Services was reportedly "dropped" into the agency's website against protocol and the objections of CDC experts.
In mid-May, the CDC published its first road map for reopening the economy - including child-care, restaurants and mass transit - which warned that some institutions should remain closed. The CDC laid out a number of social distancing policies for schools: staggered arrival times; desks at least 6 feet apart, facing the same direction; eating lunch in classrooms; face coverings for all staff; and daily temperature checks for everyone.
Two weeks later, the TPP wrote to Trump demanding a "top to bottom" overhaul as it , criticized those safety recommendations:
We have reviewed these "considerations," and find them wholly out of touch with the way people live in the real world that is America in 2020. They are totally unworkable, and we urge senior officials to review and revise these new CDC "considerations" to bring them into line with how real people live. [...] The CDC "considerations" are entirely inappropriate, and must be rewritten from top to bottom.
The letter (available in full here) went on to make a number of recommendations which ultimately found their way into the CDC's school reopening guidelines published on July 23. As CNN noted, that guidance came down "hard in favor of opening schools." Some of the recommendations from the TPP included the following:
 Eliminate disinfection and cleaning protocols
 Drop recommendation to wear face coverings
 Remove "dystopian" suggestions to reconfigure classrooms for social distancing
 Get rid of "impossible" suggestions to stagger school schedule or create small groups
 Include importance of schools in delivering nutrition to children
 Emphasize low risk posed by COVID-19 to children
The TPP was not the only non-medical conservative group whose recommended changes found their way into CDC guidelines. As Trump attacked the CDC, conservative groups, including the Heritage Foundation and Cato Institute, published their own reopening recommendations. Members of the same groups were also appointed to Trump's Coronavirus Reopening Task Force.
Politico posited in a July 8 article that Trump's drive to reopen the country's schools was also about the economy and his re-election odds as "the issues are interlinked": 
With children out of the house, they argue, parents can more easily return to work and juice the economy - something even the president's allies consider a necessity for Trump to win re-election. And with Trump's sagging poll numbers against presumptive 2020 rival Joe Biden, aides also hope the campaign for in-person schooling will play well with the female and suburban voters the president needs to remain in office.
In early July, the president vowed to personally intervene as he blasted the CDC's guidelines as "very tough & expensive."
"I disagree with @CDCgov on their very tough & expensive guidelines for opening schools. While they want them open, they are asking schools to do very impractical things. I will be meeting with them!!!" Trump tweeted on July 7.
Vice President Mike Pence joined the fight, saying at a press conference that day that Trump did not want the CDC guidance to create reopening "barriers." 
The original recommendations emphasized caution: "The more people a student or staff member interacts with, and the longer that interaction, the higher the risk of COVID-19 spread." They also laid out a risk table for distancing scenarios, ranging from complete virtual learning to "full sized, in-person classes, activities and events" with students "not spaced apart."
The TPP letter claimed those "strange" recommendations were so extreme that they could only be implemented through "the use of force":
You're going to enforce social distancing on a playground or in gym class? Impossible. You're going to order children not to use the jungle gyms, the slides, the horizontal ladders, and the carousels, because they cannot be disinfected between each use? And we are expected to believe this will succeed without the use of force? Try telling a nine-year-old he can't climb on the horizontal ladders, or a five-year-old she can't use the slide. Good luck with that.
The TPP complained about the inefficiencies of sanitizing surfaces. It also wanted the CDC to emphasize that children were unlikely to get sick and spread the disease. In addition to reflecting those concerns, the revised CDC guidelines emphasized another of the group's qualms: the "social, emotional, and mental health needs of students" that could be impacted by "potential learning loss."
The letter also inaccurately cited a medical journal report to argue against requirements for face coverings included in the CDC recommendations:
This insistence on masks flies in the face of data, science, and common sense. A study that appeared in The New England Journal of Medicine on May 21st reports that masks provide 'little, if any, protection from infection.' Furthermore, the possibility of 'catching COVID-19 from a passing interaction in a public space is therefore minimal.' So, why the insistence on masks? The article provides this explanation: 'In many cases, the desire for widespread masking is a reflexive reaction to anxiety over the pandemic.'
The authors of that study wrote a clarifying letter to the NEJM pushing back against this misinterpretation, which had become widespread. It read, "In truth, the intent of our article was to push for more masking - not less."
"We did state in the article that 'wearing a mask outside health care facilities offers little, if any, protection from infection,' but as the rest of the paragraph makes clear, we intended this statement to apply to passing encounters in public spaces - not sustained interactions within closed environments," they said.
"We therefore strongly support the calls of public health agencies for all people to wear masks when circumstances compel them to be within 6 feet of others for sustained periods," the letter added. 
Nevertheless, the revised CDC guidelines tempered the mask guidance, saying that while it was "critical" that schools include cloth face coverings as part of a larger strategy, "more research and evaluation is needed" to determine how effective it would be in schools.
The TPP also criticized the CDC's recommendation that classes configure seating charts so that all students face forward, minimizing face-to-face contact, calling the idea "straight out of a dystopian science fiction novel." For support, the letter cited an ophthalmologist, or eye doctor, who claimed that the agency's recommendations would create a "dystopian environment for our children." Again, the CDC eased its recommendations in this case.
The early guidance also suggested that small groups of students and staff stick together throughout the day as much as possible, including staggered arrival and drop-off times. Those guidelines were also softened after the TPP letter told the president that schools had been intended to "prepare our next generation to be productive members of society - a society that will not always allow for social distancing, staggered scheduling, herding into small groups and the like."
While influential conservative think tanks combatted the guidance with white papers, the TPP - which has raised tens of millions of dollars for conservative causes - took a more grassroots approach.
Four days after the CDC released its revised guidance, the TPP funded and hosted a press conference in front of the Supreme Court featuring a group called "America's Frontline Doctors." Those individuals made false claims about COVID-19 "cures," and alleged that public health measures such as mask mandates and school closings were ineffective and unnecessary.
One of the speakers, Stella Immanuel, claimed that she had cured 350 COVID-19 patients using a hydroxychloroquine treatment. Immanuel said doctors who refused to use the Trump-backed trherapy, which by that time had its emergency authorization revoked by the Food and Drug Administration, were like the "good Germans who allow the Nazis to kill the Jews."
Trump and his eldest son, Donald Jr., both shared the video, but social media platforms soon began to strip it from their platforms for violating rules about promoting misinformation related to the pandemic. 
Trump, however, called those in the group "very respected doctors." He specifically singled out Immanuel - who did not receive her medical degree in the U.S. and espoused ridiculous theories, such as that diseases come from "demon sperm" - as having been particularly "spectacular."
"I thought she was very impressive in the sense that, from where she came - I don't know what country she comes from - but she said that she's had tremendous success with hundreds of different patients," the president said.
Immanuel's practice operates out of a strip mall in suburban Houston.

Load-Date: September 18, 2020


End of Document




Student debt and the end of the liberal arts dream
Salon.com
September 13, 2020 Sunday


Copyright 2020 Salon.com, LLC. All Rights Reserved
Length: 1660 words
Highlight: The corporatization of the university experience is reaching its conclusion


Body


Link to Image
Student Debt Getty Images
Most people know that student debt is a problem, and most people agree that "something should be done about it." The consensus opinion appears to be that something should be done because it's "not fair" to young people. According to a recent poll conducted by the Pew Charitable Trust, 80% of Americans believe that "the government should make it easier to repay student loans."
Even so, there is very little understanding of why student debt has become so burdensome in recent decades. It's as if we thought student debt were an unhappy fact of nature, like a weather front that has passed through leaving us with no option but to put on a warmer coat if we can find one, and if we can't find one, it's welcome to the Brave New World of Cold and Indifference.
And yet, it was not so long ago that things were very different. I was born in a working class suburb of San Francisco in 1951. At that time, public education was good, teachers still had some social prestige, universities were affordable (cheap actually), and few students graduated with debt. It was possible for me to sally forth without the threat of young adult bankruptcy.
In other words, at that time students were freer to choose what they wanted to study and freer to explore careers. As for me, I was free to be a student of literature and philosophy at the University of San Francisco. I learned to play the classical guitar up the hill at Lone Mountain College. I was also a longhair war resistor and draft counselor in the chaplain's office. And I was mostly sure that I wouldn't be punished for these decisions, or not punished any time soon.
Unhappily, the university as I knew it no longer exists. Through the decades of Reaganomics and neo-liberal austerity, an elite determination was made that the state should no longer pay for public higher education; henceforth, universities would be funded through personal debt. Tuition to public colleges and universities became a bloated "user fee" for access to a government-affiliated service, like gaining access to a parking lot. Novel arguments gained force: students were just another type of consumer, and "student demand" should determine the content of the curriculum. What was lost in such market logic was the fact that programs in the arts and humanities-not just in universities but at all levels of education-had become the primary way in which we were allowed to think about who we are, where we are, how we got here, and what, if anything, we'd like to see changed. In the place of that worthy process, we were left with what David Harvey described succinctly: "The traditional university culture, with its odd sense of community, has been penetrated, disrupted, and reconfigured by raw money power."
In both the public and private sectors, the corporate university has been slowly growing for many years, but it has now become more brazen in its destructive tendencies. For an example that is close to me personally, the Board of Trustees at Illinois Wesleyan University (IWU) in Bloomington, Illinois, announced in May of 2020 that it was considering the elimination of many long established programs all of which were in the liberal arts and social sciences. I say this is personal because my wife, Georganne Rundblad, taught sociology at Wesleyan for twenty-five years, and many of the faculty there were among our friends.
Without the consent of the faculty, the Board of Trustees sent pre-termination letters to twenty-five faculty members and instructional staff in philosophy, anthropology, music, foreign languages, sociology, art, and religion - in total, about a quarter of IWU's faculty. A liberal arts university without philosophers is a contradiction in terms, but a Wesleyan university without a department of religion is an exercise in self-mockery. No doubt, IWU will continue to claim that its "primary focus" is in "opening students' minds," but the college of business will have to do most of the opening.
That, too, is already happening. Those departments that were not eliminated were said to be in need of "transformation." Provost Mark Brodl told faculty member Scott Sheridan that "philosophy will support business and accounting, computer science and data science." In another example of what Harold Bloom called the trahison des clerks, university president Georgia Nugent (a classicist by training) said that the department of art will "move more in the direction of art and design," including graphic design and product design. In other words, before the arts can transform students, commerce will first transform the arts. Andy Warhol saw it coming: art is a can of soup.
Of course, there's no reason to think that this is the end of these transformations, these latest shudders of what Bill Readings called "the university in ruins." (Readings: "The contemporary University is busily transforming itself from an ideological arm of the state into a bureaucratically organized and relatively autonomous consumer-oriented corporation.") Remaining faculty and programs can't sleep comfortably, wondering who and what will be cut next, a year from now, five years or ten. University boards can wait - they've been waiting patiently for fifty years. For fifty years American universities have moved toward a sense of mission that a businessman can recognize, respect, and, most importantly, donate money to.
But from the perspective of the liberal arts itself, a liberal arts university that doesn't recognize the value of the liberal arts shouldn't transform anything. It should shut its doors, fold it all up. Better that than becoming what Illinois Wesleyan has become: a watery version of the Wharton School of Business.
Wesleyan trustees felt no need to provide a rationale for their actions beyond the crude idea that the affected programs were not cost efficient and that financial exigency threatened them in the near future, somehow - never mind the university's $200 million endowment. They offered no educational reasons for the changes, and no argument for how the changes were consistent with the values of the institution. Like the bandits in John Huston's "The Treasure of the Sierra Madre," they said, "Reasons? We don't need no stinking reasons."
The feelings of faculty and alums are raw now because they feel, rightfully, betrayed. One member of the IWU Alumni for the Liberal Arts, Molly McLay, wrote to me:
I am infuriated by the lack of humanity and empathy in this process. Most of my professional work revolves around understanding, responding to, and preventing trauma. I truly think this experience at IWU has been a collective trauma.
Affirming McLay's thought, Scott Sheridan, a professor of French and Italian at Wesleyan, commented to me:
I feel like I'm just caught in the middle of an administration that has lost its bearings, that is changing the rules on me late in my career. I've bought into the ivory tower model, I've done everything asked of me, and now when the economy has tanked, Covid is blazing, higher ed is collapsing, and the University has damaged my professional reputation, I find myself completely cast aside, looking at the necessity not only to retool my professional objectives but to rethink my life.
Professor Sheridan has lawyered up.
Finally, and elegiacally, Professor of English Michael Theune commented to the Chicago Tribune, "It is a bad day. The faculty's control of the curriculum has been taken away."
The motto over Wesleyan's portal should no longer be "Scientia et Sapientia," knowledge and wisdom. I propose that it should read like the legend over Dante's Inferno: "No Admittance Except on Business."
After the second world war, there was an enormous investment in public education. For the first time, the children of the working class had an opportunity to study subjects, like literature, that were formerly a privilege for the children of the affluent. We studied the humanities and the social sciences and in so doing found ways to critique and resist corporate culture and all of its murderous inequalities. In the 1960s, universities became best known for their "student protests." The protests may now be in the streets and not on campus, but many of the protestors of the present-marching with Black Lives Matter, or against gender bigotry, or against the world as organized for the benefit of oil companies-got their intellectual chops in universities taking courses like Professor Rundblad's "Sex and Gender in Society" or "Race and Ethnic Relations."
But all this time our masters have been paying attention, and they have seen clearly and correctly: for many, many students, going to college was and remains a liberalizing experience (thus Biden's enormous lead over Trump among college-educated voters). Our plutocrat masters concluded, "So this is what happens when you let the working class and minorities go to college. They study things of no value to us, and they learn to hate us."
As a consequence, slowly, decade after decade, universities were starved and students were put in debt. Meanwhile, the wealthy came to the rescue and became university trustees. In these fallen days, the ideal trustee is someone who has money or knows people with money, ideally both. (The chair of Wesleyan's Board of Trustees is Timothy Szerlong, former president of worldwide field operations at CNA Financial Corporation.)
The ultimate benefit of all this for our oligarchs, the 1%, is a new but very powerful form of social regimentation. Their message to students: "If you want a job, you will study what we want you to study, or else you will live in debt." We could call this naked coercion, but for students it is their first adult taste of American Un-freedom.
In 1969, the year before I entered the University of San Francisco, I was working at a McDonalds in the East Bay. The franchise owner liked me and offered me a scholarship to McDonald's Hamburger University. I laughed. I shouldn't have. 

Load-Date: September 14, 2020


End of Document




Student debt and the end of the liberal arts dream
Salon.com
September 13, 2020 Sunday


Copyright 2020 Salon.com, LLC. All Rights Reserved
Length: 1660 words
Highlight: The corporatization of the university experience is reaching its conclusion


Body


Link to Image
Student Debt Getty Images
Most people know that student debt is a problem, and most people agree that "something should be done about it." The consensus opinion appears to be that something should be done because it's "not fair" to young people. According to a recent poll conducted by the Pew Charitable Trust, 80% of Americans believe that "the government should make it easier to repay student loans."
Even so, there is very little understanding of why student debt has become so burdensome in recent decades. It's as if we thought student debt were an unhappy fact of nature, like a weather front that has passed through leaving us with no option but to put on a warmer coat if we can find one, and if we can't find one, it's welcome to the Brave New World of Cold and Indifference.
And yet, it was not so long ago that things were very different. I was born in a working class suburb of San Francisco in 1951. At that time, public education was good, teachers still had some social prestige, universities were affordable (cheap actually), and few students graduated with debt. It was possible for me to sally forth without the threat of young adult bankruptcy.
In other words, at that time students were freer to choose what they wanted to study and freer to explore careers. As for me, I was free to be a student of literature and philosophy at the University of San Francisco. I learned to play the classical guitar up the hill at Lone Mountain College. I was also a longhair war resistor and draft counselor in the chaplain's office. And I was mostly sure that I wouldn't be punished for these decisions, or not punished any time soon.
Unhappily, the university as I knew it no longer exists. Through the decades of Reaganomics and neo-liberal austerity, an elite determination was made that the state should no longer pay for public higher education; henceforth, universities would be funded through personal debt. Tuition to public colleges and universities became a bloated "user fee" for access to a government-affiliated service, like gaining access to a parking lot. Novel arguments gained force: students were just another type of consumer, and "student demand" should determine the content of the curriculum. What was lost in such market logic was the fact that programs in the arts and humanities-not just in universities but at all levels of education-had become the primary way in which we were allowed to think about who we are, where we are, how we got here, and what, if anything, we'd like to see changed. In the place of that worthy process, we were left with what David Harvey described succinctly: "The traditional university culture, with its odd sense of community, has been penetrated, disrupted, and reconfigured by raw money power."
In both the public and private sectors, the corporate university has been slowly growing for many years, but it has now become more brazen in its destructive tendencies. For an example that is close to me personally, the Board of Trustees at Illinois Wesleyan University (IWU) in Bloomington, Illinois, announced in May of 2020 that it was considering the elimination of many long established programs all of which were in the liberal arts and social sciences. I say this is personal because my wife, Georganne Rundblad, taught sociology at Wesleyan for twenty-five years, and many of the faculty there were among our friends.
Without the consent of the faculty, the Board of Trustees sent pre-termination letters to twenty-five faculty members and instructional staff in philosophy, anthropology, music, foreign languages, sociology, art, and religion - in total, about a quarter of IWU's faculty. A liberal arts university without philosophers is a contradiction in terms, but a Wesleyan university without a department of religion is an exercise in self-mockery. No doubt, IWU will continue to claim that its "primary focus" is in "opening students' minds," but the college of business will have to do most of the opening.
That, too, is already happening. Those departments that were not eliminated were said to be in need of "transformation." Provost Mark Brodl told faculty member Scott Sheridan that "philosophy will support business and accounting, computer science and data science." In another example of what Harold Bloom called the trahison des clerks, university president Georgia Nugent (a classicist by training) said that the department of art will "move more in the direction of art and design," including graphic design and product design. In other words, before the arts can transform students, commerce will first transform the arts. Andy Warhol saw it coming: art is a can of soup.
Of course, there's no reason to think that this is the end of these transformations, these latest shudders of what Bill Readings called "the university in ruins." (Readings: "The contemporary University is busily transforming itself from an ideological arm of the state into a bureaucratically organized and relatively autonomous consumer-oriented corporation.") Remaining faculty and programs can't sleep comfortably, wondering who and what will be cut next, a year from now, five years or ten. University boards can wait - they've been waiting patiently for fifty years. For fifty years American universities have moved toward a sense of mission that a businessman can recognize, respect, and, most importantly, donate money to.
But from the perspective of the liberal arts itself, a liberal arts university that doesn't recognize the value of the liberal arts shouldn't transform anything. It should shut its doors, fold it all up. Better that than becoming what Illinois Wesleyan has become: a watery version of the Wharton School of Business.
Wesleyan trustees felt no need to provide a rationale for their actions beyond the crude idea that the affected programs were not cost efficient and that financial exigency threatened them in the near future, somehow - never mind the university's $200 million endowment. They offered no educational reasons for the changes, and no argument for how the changes were consistent with the values of the institution. Like the bandits in John Huston's "The Treasure of the Sierra Madre," they said, "Reasons? We don't need no stinking reasons."
The feelings of faculty and alums are raw now because they feel, rightfully, betrayed. One member of the IWU Alumni for the Liberal Arts, Molly McLay, wrote to me:
I am infuriated by the lack of humanity and empathy in this process. Most of my professional work revolves around understanding, responding to, and preventing trauma. I truly think this experience at IWU has been a collective trauma.
Affirming McLay's thought, Scott Sheridan, a professor of French and Italian at Wesleyan, commented to me:
I feel like I'm just caught in the middle of an administration that has lost its bearings, that is changing the rules on me late in my career. I've bought into the ivory tower model, I've done everything asked of me, and now when the economy has tanked, Covid is blazing, higher ed is collapsing, and the University has damaged my professional reputation, I find myself completely cast aside, looking at the necessity not only to retool my professional objectives but to rethink my life.
Professor Sheridan has lawyered up.
Finally, and elegiacally, Professor of English Michael Theune commented to the Chicago Tribune, "It is a bad day. The faculty's control of the curriculum has been taken away."
The motto over Wesleyan's portal should no longer be "Scientia et Sapientia," knowledge and wisdom. I propose that it should read like the legend over Dante's Inferno: "No Admittance Except on Business."
After the second world war, there was an enormous investment in public education. For the first time, the children of the working class had an opportunity to study subjects, like literature, that were formerly a privilege for the children of the affluent. We studied the humanities and the social sciences and in so doing found ways to critique and resist corporate culture and all of its murderous inequalities. In the 1960s, universities became best known for their "student protests." The protests may now be in the streets and not on campus, but many of the protestors of the present-marching with Black Lives Matter, or against gender bigotry, or against the world as organized for the benefit of oil companies-got their intellectual chops in universities taking courses like Professor Rundblad's "Sex and Gender in Society" or "Race and Ethnic Relations."
But all this time our masters have been paying attention, and they have seen clearly and correctly: for many, many students, going to college was and remains a liberalizing experience (thus Biden's enormous lead over Trump among college-educated voters). Our plutocrat masters concluded, "So this is what happens when you let the working class and minorities go to college. They study things of no value to us, and they learn to hate us."
As a consequence, slowly, decade after decade, universities were starved and students were put in debt. Meanwhile, the wealthy came to the rescue and became university trustees. In these fallen days, the ideal trustee is someone who has money or knows people with money, ideally both. (The chair of Wesleyan's Board of Trustees is Timothy Szerlong, former president of worldwide field operations at CNA Financial Corporation.)
The ultimate benefit of all this for our oligarchs, the 1%, is a new but very powerful form of social regimentation. Their message to students: "If you want a job, you will study what we want you to study, or else you will live in debt." We could call this naked coercion, but for students it is their first adult taste of American Un-freedom.
In 1969, the year before I entered the University of San Francisco, I was working at a McDonalds in the East Bay. The franchise owner liked me and offered me a scholarship to McDonald's Hamburger University. I laughed. I shouldn't have. 

Load-Date: September 13, 2020


End of Document




L.A Times Endorses Biden
City News Service
September 10, 2020 Thursday 7:30 AM PST


Copyright 2020 City News Service, Inc. All Rights Reserved
No City News Service material may be republished without the express written permission of the City News Service, Inc.
Length: 510 words
Body


LOS ANGELES (CNS) - The Los Angeles Times today editorially endorsed Joe Biden for president.
"This year's presidential election confronts voters with the most consequential choice they have faced in decades, and for many, their lifetimes: between a divisive, authoritarian-leaning incumbent and a seasoned patriot who brings not only five decades of experience, ability and commitment to American values, but also bold ideas at a time of national crisis. Nothing less than the health of our constitutional democracy is at stake," the Times wrote in a front-page editorial on its online edition.
"So stark is the contrast between Donald Trump and Joe Biden that we feel compelled to announce our endorsement of the Democratic nominee and his running mate, Sen. Kamala Harris of California, now, at what is traditionally the beginning of the fall campaign and before the candidates take part in televised debates. We will watch those exchanges with interest, but it's inconceivable that anything that will be said on the debate stage will close the cavernous fitness gap between the two candidates."
The Times wrote that re-electing Presidnt Donald Trump "would be a calamity. Trump's trafficking in falsehoods; his stoking of racial divisions; his inability to distinguish between the national interest and his personal interests; his attempt to ride a false and dangerous narrative about 'anarchy and mayhem' to reelection all require repudiation by the voters.
"He has pursued policies at home and abroad that have harmed working Americans, exacerbated inequality, weakened the United States and strained America's alliances. Even actions that can be defended -- such as his support for modest criminal-justice reform or his attempt to negotiate an agreement with North Korea over its nuclear weapons program -- have been marred by exaggerated claims or amateurish execution."
According to The Times' editorial board, "Biden isn't just preferable to Trump; in many respects he is Trump's antithesis.
"Biden has a record of seeking expert advice and listening to it. Progressives may take issue with his choice of advisers, many of whom are establishment figures he's known for years. But he clearly has a level of respect for data, science and research that the incumbent does not. As 81 U.S. Nobel Prize winners in chemistry, physics and medicine wrote in an open letter endorsing Biden, 'At no time in our nation's history has there been a greater need for our leaders to appreciate the value of science in formulating public policy."
Temperamentally, too, Biden seems like "an ideal fit for our polarized time," according to The Times' editorial, which called the former vice president "a famously empathetic figure."
The editorial concluded: "To the surprise of supporters and detractors alike, Biden, a 77-year-old centrist, first elected to federal office 48 years ago, has mounted a disciplined campaign. He is poised to wrest the White House from an unfit incumbent but also take the nation in a progressive direction. We enthusiastically endorse his election."

Load-Date: September 11, 2020


End of Document




New Measurement of Home Condition Without Inspections - Pomar Lane Completes Condition Scores for 90,000 Homes
CaliforniaNewswire
August 27, 2020 Thursday


Copyright 2020 CaliforniaNewswire, distributed by Contify.com All Rights Reserved
Length: 396 words
Byline: Christopher Simmons
Body


SANTA BARBARA, Calif. /California Newswire/ - Pomar Lane, a data analytics firm specializing in real estate modeling, today completed a demonstration project that estimated condition scores for over 90 thousand homes. Home condition is represented by the Pomar Condition Score, the first measure of home condition based on advanced analytics rather than expensive inspections or appraisals.
"Online real estate sites commonly present home prices, but have a tremendous blind spot when it comes to home condition. This changes with the availability of the Pomar Condition Scoring System. Our intent is to provide the home buyer with individual home condition scores and renewal costs for all major U.S. residential markets," said company CEO Peter Lufkin. "And the need for condition data extends beyond online search."
He adds, "We foresee a time when any transaction depending on the value of your house will require a condition rating. Apply for a mortgage, purchase homeowner insurance, or market a short-term rental-these all can depend on the Pomar Score as an objective and inexpensive measure of home condition."
The Pomar Score is based on the structural value of the home. Scaled from 1 to 100, a low value indicates little investment in maintenance and repair, while a high value indicates the homeowner is keeping up with wear and tear as the home ages. The predicted score comes from a unique learning model that incorporates changing home characteristics, neighborhood features and local economic data.
Developed over the last three years, the proprietary algorithm delivers for each home:
* a condition score that measures the remaining value of the home structure
* an average condition score for the vicinity
* a range of estimated renewal costs depending on maintenance style
The estimated scores are highly correlated with actual appraisal results, yet can be generated for a small fraction of appraisal costs. As a service, Pomar Lane will provide regular condition updates for homes in key U.S. markets.
About Pomar Lane
Pomar Lane LLC is developing a nationwide database of home condition scores and renewal costs. We use data science instead of costly inspections to score property condition. Pomar models can inform the choices of individual home buyers. Our tools can help reduce insurance claims, improve investor decisions, and make valuation models more accurate.

Load-Date: August 29, 2020


End of Document




    
     
  Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved.   
  California HealthlineAs Georgia Reopened, Officials Knew of Severe Shortage of PPE for Health Workers
       California Healthline
August 20, 2020


  Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved.   


Length: 1865 words
Byline: Kaiser Health News, Contributor
Body

After Georgia eased its lockdown, its COVID-19 cases spiked and so did its need for PPE.  
     
  As the coronavirus crisis deepened in April, Georgia officials circulated documents showing that to get through the next month, the state would need millions more masks, gowns and other supplies than it had on hand.  
  [SEE: The Latest News on the Coronavirus Outbreak]  
     
  The projections, obtained by KHN and other organizations in response to public records requests, provide one of the clearest pictures of the severe PPE deficits states confronted while thousands fell ill from rising COVID-19 cases, putting health workers at risk.  
     
  Georgia on April 19 had 932,620 N95 respirator masks -- one of the best protections for health workers against infection -- and expected to burn through nearly 7 million within a month. It urgently needed to buy 1.4 million more, according to documents obtained by the Brown Institute for Media Innovation and shared with KHN. For gowns, officials expected to go through 16.1 million in 30 days, a staggering amount compared with the 21,810 the state had at the time.  
     
  "Making progress with PPE needs. Biggest challenge now is gowns and we are working it," Georgia Emergency Management and Homeland Security Agency Director Homer Bryson wrote on April 19 to two of Gov. Brian Kemp's senior-most aides.  
     
  Even so, one day later, the first-term Republican governor announced he would begin to reopen the state's economy, including gyms, restaurants, hair salons, theaters and a host of other businesses.  
     
  "We have relied on data, science and the advice of health care professionals to guide our approach and decision-making," he said at a news conference, "putting the health and well-being of our citizens first and doing our best to protect lives and livelihoods."  
     
  "Our state agencies and the governor felt confident in the state's ability to meet daily PPE requests from our local emergency preparedness partners and medical facilities when Georgia began implementing its measured reopening plan," Cody Hall, the governor's spokesperson, said in response to questions. "We have continued to meet those needs since April." He noted the state is now building a PPE stockpile.  
     
  A Matter of Life or Death  
     
  After Georgia eased its lockdown, COVID cases spiked. Requests for PPE from health workers in the Atlanta area escalated through April and May, according to numbers provided by the nonprofit Atlanta Beats COVID-19, which makes face shields for health workers and other residents.  
     
  According to public data on the Georgia Department of Public Health's website, at least 80 Georgia health care workers have died from COVID-19, including after the state reopened its economy.  
  [COMMENTARY: Heroes Need Help, Too]  
     
  One was John "Derrick" Couch, a nurse practitioner who worked in Fort Oglethorpe, Georgia. Shortly after graduating with his master's degree in nursing on May 10, the worker at Med First Immediate Care Medical Center grew sick with COVID-19. His wife, Karol, cared for him at home for a time before he was hospitalized. He died after 36 days on a ventilator, according to a GoFundMe page set up to help his family cover his health care expenses.  
     
  "Karol wants everyone to know that Covid-19 doesn't care or discriminate. She says John would want all of his colleagues and friends in healthcare and community to demand proper equipment and protection," it said. Med First Immediate Care did not respond to a request for comment.  
     
  Between March 16 and Aug. 9, 48 COVID-19-related complaints regarding inadequate PPE in Georgia health care facilities were closed by the Occupational Safety and Health Administration, the federal agency responsible for workplace safety. The PPE complaints accounted for the majority -- roughly 6 in 10 -- of Georgia's COVID-19 complaints submitted to OSHA during the four-month period.  
     
  In April and May, "we received thousands of requests for N95 masks, but we couldn't get our hands on the right materials to even make an N95 mask," said Caroline Dunn, Atlanta Beats COVID-19's communications coordinator.  
     
  Nationally, health workers continue to express alarm about protective equipment supplies as COVID-19 hot spots reemerge across the country. A National Nurses United survey in July found 87% of nurses working in hospitals reported reusing at least one piece of single-use PPE. Only a quarter of nurses surveyed felt their employers were providing a safe workplace.  
     
  "There's really been this normalization and this acceptance that some people are going to be expendable. And that's completely unacceptable," said Dabney Evans, director of the Center for Humanitarian Emergencies at Emory University in Atlanta.  
     
  Another document projecting PPE supplies, dated April 10 and developed by Georgia health and emergency management officials, relied on a calculator from the U.S. Centers for Disease Control and Prevention to estimate how quickly Georgia would burn through supplies across hospitals, nursing homes, dialysis clinics, jails and prisons. The state had 527,424 N95 respirators but needed a total of nearly 1.1 million to get through the ensuing seven days. The projected need grew to 4.8 million masks when estimating supplies for the following 31 days.  
  [READ: States With Biggest Health Improvements]  
     
  It had 196,500 gloves on hand but would need more than 12.1 million to get through a week, and 54 million for 31 days. The state had about 122,000 face shields but required more than 458,000 for the coming seven days. For a month, the projected need ballooned to over 2 million.  
     
  The April 10 estimates -- a day when Georgia's new COVID-19 case count rose by about 1,000 people -- were sent to the U.S. Department of Health and Human Services and Federal Emergency Management Agency as part of a broad effort to assess what states needed across health care settings to operate for at least seven days and up to a month. Federal officials asked state public health and emergency management officials to submit PPE projections daily, according to emails among state personnel, HHS and FEMA.  
     
  PPE estimates would be used "to determine projections for our region and the next hot spots within each state," Jeanne Eckes, an HHS official working with FEMA on the federal government's COVID-19 response, wrote in an April 3 email to officials in multiple states throughout the South, according to correspondence obtained by KHN.  
     
  Calculations Matter  
     
  Georgia officials contend the state's estimated PPE deficits were larger early in the pandemic because projections accounted for all COVID-positive cases. Once the state had more information on how many of these positives were asymptomatic cases and how many led to hospitalizations, it could better gauge what was needed, they argued. Multiple changes were made to its burn-rate calculations, including a May 8 adjustment that replaced the total case count with hospital-based COVID cases, which reduced the projected demand for PPE.  
     
  However, multiple experts disputed the idea that knowing the number of asymptomatic patients would be relevant for PPE projections. In facilities like nursing homes and jails -- both of which were accounted for in the Georgia estimates -- asymptomatic individuals could spread the virus if not quarantined immediately.  
     
  "Because there's not on-the-spot, point-of-care testing available for the most part, you have to use PPE throughout the hospital all the time," said Dr. Eric Toner, a senior scholar with the Johns Hopkins Center for Health Security. "In this day and age, you just have to presume that everyone has COVID."  
     
  When the state's case count began surging in March, many COVID-19 patients treated at Tift Regional Medical Center in Tifton, Georgia, needed ICU-level care and were from nearby Dougherty County, a Georgia hot spot where hospitals were quickly overwhelmed.  
     
  "There were times to which we were down to only having a few days of PPE left," said Dr. Kaine Brown, a physician and medical director at Tift, adding that the hospital was partly saved by donations of N95 and cloth masks. Gowns were the biggest problem. PPE supplies have since improved -- as of early July, the hospital had stockpiled more than eight months' worth of surgical masks and enough N95s and gowns to last six months and about three months, respectively.  
  [READ: Health Sector Shedding Jobs Amid Pandemic]  
     
  Georgia's stay-at-home order for most residents expired April 30; it remains in place for individuals at higher risk of severe illness.  
     
  "We were very apprehensive about [easing restrictions]," Brown said. "Those of us who had been working on the front lines knew how infectious this was."  
     
  Since May, Georgia has reopened a broad swath of businesses. In early July, more than 1,000 health care workers signed a letter to Kemp urging him to institute a statewide mandate requiring face coverings, to close bars and nightclubs, and prohibit indoor gatherings of more than 25 people. Georgia currently bans gatherings of more than 50 people if social distancing cannot be observed.  
     
  State officials say PPE supplies have "greatly improved" since the start of the public health emergency. As of Aug. 14, the state had distributed 3.9 million N95s, 13.1 million surgical masks, 36.6 million gloves, 4.6 million gowns and 1.6 million face shields, among other items, according to the Georgia Department of Public Health. Early on, Georgia also relied on donations to bolster PPE supplies when many items were unattainable through normal supply channels, which have since become more reliable.  
     
  However, even with the increased stocks, workers still reuse protective equipment and many fret over the uncertainty about how long they can do so safely. Another community-based organization, the Atlanta chapter of Sewing Masks for Area Hospitals, said that from April to June the organization gave out over 59,000 cloth masks to 152 health care facilities in the Atlanta area, including large hospitals, such as Children's Healthcare of Atlanta and Emory St. Joseph's Hospital. Kayla Hittig, a co-founder of the sewing group, said that health care workers were using the cloth masks to cover their N95 or surgical masks to make them last longer.  
     
  "That's the thing we hear the most -- how often do we have to use these and how protective are they, for how long?" said Richard Lamphier, president of the Georgia Nurses Association.  
     
  Lamphier wasn't critical of the state officials' efforts to ensure health workers are protected.  
     
  "I think they've done the best they could with the situation they had," he said.  
     
  It wasn't enough to protect John Couch, whose family is reeling from his death.  
     
  "He was my whole life," Karol Couch said. "My life is shattered."  
     
  This story was produced by Kaiser Health News, an editorially independent program of the Kaiser Family Foundation that is not affiliated with Kaiser Permanente. It has been republished with permission.


End of Document




Andrew Cuomo saw COVID-19's threat to nursing homes - yet he still risked adding to it
Salon.com
June 20, 2020 Saturday


Copyright 2020 Salon.com, LLC. All Rights Reserved
Length: 4672 words
Highlight: A New York nursing home followed the governor's order to accept COVID patients. 18 residents died of the disease.


Body


Link to Image
New York Governor Andrew Cuomo Getty/Drew Angerer
On April 3, Stephanie Gilmore, a 34-year-old nurse working at the Diamond Hill nursing home in Troy, New York, was summoned to a supervisor's office. The home's administrator and nursing director were there to relay some distressing news.
Gilmore said they told her that a resident in the home had recently gone to the hospital, where she tested positive for COVID-19. The resident was set to return to Diamond Hill, making her the first confirmed COVID-19 case at the 120-bed facility north of Albany.
The risks to the home's staff and other residents were obvious: The virus was ravaging nursing homes across the country.
But the week before, New York Gov. Andrew Cuomo and his health commissioner, Howard Zucker, had all but made such discharges mandatory. If a hospital determined a patient who needed nursing home care was medically stable, the home had to accept them, even if they had been treated for COVID-19. Moreover, the nursing home could not test any such prospective residents - those treated for COVID-19 or those hospitalized for other reasons - to see if they were newly infected or perhaps still contagious despite their treatment. It was all laid out in a formal order, effective March 25. New York was the only state in the nation that barred testing of those being placed or returning to nursing homes.
In the weeks that followed the March 25 order, COVID-19 tore through New York state's nursing facilities, killing more than 6,000 people - about 6% of its more than 100,000 nursing home residents. In all, as many as 4,500 COVID-19 infected patients were sent to nursing homes across the state, according to a count conducted by The Associated Press.
The state declined to say if it knew how many COVID-19 patients had been sent or returned to Diamond Hill. Officials with Diamond Hill refused to disclose the number.
By June, 18 of Diamond Hill's residents had died from the virus and 58 had been infected. At least 50 of the facility's more than 100 workers had also been sickened with COVID-19.
As deaths mounted at Diamond Hill, new COVID-19 patients were transferred in
Link to Image
Source: Rensselaer County Department of Health
States that issued orders similar to Cuomo's recorded comparably grim outcomes. Michigan lost 5% of roughly 38,000 nursing home residents to COVID-19 since the outbreak began. New Jersey lost 12% of its more than 43,000 residents.
In Florida, where such transfers were barred, just 1.6% of 73,000 nursing home residents died of the virus. California, after initially moving toward a policy like New York's, quickly revised it. So far, it has lost 2% of its 103,000 nursing home residents.
The decision by Cuomo and Zucker, whose department regulates all nursing homes in the state, drew fire as soon as it was announced from medical experts, nursing home operators and the families of residents. Cuomo himself had said protecting nursing home residents was the state's top priority, once calling the threat "fire through dry grass."
Steve McLaughlin, the county executive where Diamond Hill is located, viewed the state's directive as madness and chose to defy it, refusing to allow any COVID-19 patients to be returned to, or placed in, the one nursing home run by the county. The 320-bed facility, Van Rensselaer Manor, has not seen a single COVID-19 death.
Cuomo and New Jersey Gov. Phil Murphy have defended their approach as a way to open up crucial beds at a moment when it appeared hospitals would be overwhelmed by COVID-19 patients needing intensive care.
Charles Branas, who leads the epidemiology department at Columbia University's Mailman School of Public Health, said he could appreciate New York state's concern about a shortage of hospital beds in hard-hit areas.
"The New York state advisory looks like it was intended as a 'reverse triage' strategy to clear acute and critical care hospital beds, regardless of whether those beds had people with COVID-19 or not," Branas said. "Possibly, the positive trade-off they had in mind with the policy was that more lives would be saved with additional open critical care beds than would be lost in transfer to nursing homes."
But Branas said he believes the policy could well have increased New York's COVID-19 death toll by a magnitude that will be determined by future researchers. "If you introduce 4,500 people sick with a potentially lethal disease into a vulnerable and notoriously imperfectly monitored population," he said, "people are apt to die."
Former employees and families of patients portray Diamond Hill as a case study of a facility ill-prepared to cope with the complexities of containing the virus. The day Gilmore was told of the resident with COVID-19, she said she was also told that the information should not be shared with other staffers or patients - the management didn't want to provoke panic. Gilmore said she refused to go along and was later fired.
Three days after Diamond Hill was informed of its first case, six other residents tested positive, suggesting the virus had been present at the home for days, maybe weeks. The resident with COVID-19 who was returned to the home might have provided more fuel for the virus's spread.
Gilmore said the home made inadequate adjustments to try to care for its residents' safely. The COVID-19 patients were not isolated in a separate unit, and the facility lacked adequate protective gear for staff, she said. Gilmore and county officials said staffers who'd been exposed were encouraged to break their quarantines and return to work.
All told, the virus has claimed the lives of nine women and nine men at Diamond Hill, three younger than 60, three older than 90. Among those lost: a church deacon, a bowling alley manager, a former nurse and a beloved grandfather called Pop Pop by his grandchildren.
"Uncalled for, unnecessary, should never have occurred, and wouldn't have but for a tragically misguided order from the state," McLaughlin, the Rensselaer County executive, said of the outcomes at Diamond Hill.
The state Health Department said its personnel visited the home at least twice in April as part of broader efforts to track and control the virus inside the state's nursing homes. They deemed Diamond Hill capable of caring for its residents. Documents show Zucker, the health commissioner, was fully aware of events at Diamond Hill and reassured local leaders that the department had offered help moving patients to other facilities, but was told it wasn't needed.
Cuomo and Zucker, after escalating criticism, revoked the March 25 directive on May 10.
The Cuomo administration would not say who conceived of the order or answer the question of whether it believed the order had led to additional deaths. The administration said the Health Department was conducting "a thorough review" of COVID-19's impact on nursing homes.
"Science will determine whether the spread in nursing homes came as a result of returning residents or from asymptomatic staff who were already there," said Jonah Bruno, a spokesman for the New York Health Department.
Officials have said the directive was based on federal guidance saying that nursing homes could accept residents with COVID-19 as long as they had enough personal protective equipment, could disinfect medical equipment regularly, could limit the movement of patients, could house them in their own rooms if necessary and meet other requirements. It's not clear, however, who in New York was responsible for assessing this - the discharging hospitals, the receiving nursing homes or the state.
The state Health Department said Diamond Hill instituted "universal isolation precautions" but did not explain what that meant. The state also said the home ceased taking COVID-19 patients in late April but did not say what led to that development.
A complicating factor in evaluating the effect of the March 25 directive is that the state Health Department did not track in real time what happened when COVID-19 patients were transferred from hospitals to nursing homes. One senior nursing home industry official said the state Health Department didn't even begin comprehensively counting COVID-19 deaths in these facilities until well into April, although the department has disputed that claim.
Bruno said the agency had adjusted the way it tracked deaths as the pandemic progressed. He added that the state did hundreds of safety checks on homes throughout the state and cited scores for various shortcomings. But he would not say if that information had been shared with the hospitals and families making decisions about the suitability of homes to receive COVID-19 patients.
Diamond Hill sent ProPublica a statement saying that its management team had taken over the facility recently, and that it had taken dozens of measures to protect the facility from the coronavirus, informed residents and their families of the presence of the virus, and worked with the county and state to keep patients and staff safe. The facility appears to have changed its name recently to Collar City Nursing and Rehabilitation. The statement did not answer questions about Gilmore's specific allegations, citing "labor laws."
"Despite our stepped up efforts, the stealth virus entered our community in early April likely through someone who appeared perfectly healthy," the statement said. "While we were likely successful in delaying infections and reducing spread, like thousands of other skilled nursing facilities affected, we were not able to fully quash the virus."
In an interview, Ari Grinspan, Diamond Hill's CEO, declined to respond to questions about the state's March 25 order, the home's preparedness or the deaths of specific residents.
"Now is not the time for a facility to be in the spotlight," he said. "It gives others the chance to pick on you."
"There will be a time when the pandemic mercifully ends, that we can talk on the record about the role of facilities and the government in what has happened."
"It was chaos"
Gilmore started work at Diamond Hill on Oct. 31, 2019, months before the first virus case was reported in the United States. Even then, she said, the facility was in "damage control" mode.
Family members visiting a patient with dementia on her 74th birthday found her in a bed soiled with urine and feces. The state Health Department investigated, citing the home for failing to provide basic care, and the family shared pictures with the media.
Diamond Hill's owners brought in new managers, Gilmore among them, but in retrospect, she said, the move was mainly for optics. "So they could tell the news we have new management," she said.
Gilmore saw no sign of meaningful improvement. She and another former nurse told ProPublica the facility was chronically short of staff and equipment. Sometimes the facility had as few as four aides and one nurse looking after as many as 80 patients on a single floor, each suffering a variety of ailments that screamed for attention - incontinence, dementia, basic mobility.
In late February, she recalled an elderly patient being admitted from a local hospital. Gilmore said she told the administrators the resident needed a special oxygen delivery device to stay alive. Diamond Hill didn't have one, but the administrators took the admission anyway. Gilmore and another former Diamond Hill nurse said the patient died the next day.
Diamond Hill did not respond to questions about that patient.
As the coronavirus began to grip New York City, 162 miles south of Troy, Gilmore's concerns intensified. On March 13, she hammered out an email in all capital letters to the owners of the company, with the subject line: ATTEMPTING TO DO THE IMPOSSIBLE!
"As a nursing professional, it's unsafe to admit residents into the facility when we are critical with staffing and unable to care for the residents already in the building ... I have tried to assist in any way that I can, however I am being stretched way too thin," she wrote.
Gilmore told ProPublica she advocated in meetings for hiring roughly 20 additional aides and six nurses.
Grinspan, the home's CEO, attempted to reassure her, she said.
"We are working feverishly on hiring and bettering the situation, while we hope our appreciation to you and your peers are known," he said in an email back.
Gilmore thanked him, but the situation only got worse. She said she checked in as many as five new patients per day coming into the facility's 40-bed rehabilitation unit from all over the capital region. To her, it seemed only a matter of time before the virus began to spread in a facility already straining to provide proper care.
In reviewing paperwork for new residents, she became even more troubled. There was no indication whether incoming patients had ever been tested for the coronavirus.
Gilmore said her superiors brushed off her concerns, offering vague promises to isolate new residents. She saw little evidence that the home had the space or staff to do that.
Then came the April 3 meeting with her superiors about the woman who was diagnosed with COVID-19. With the request to keep it secret, her frustration boiled over.
"We aren't going to tell the direct care staff that they were exposed?" she said she asked.
"No," she recalled her bosses saying. "We don't want to cause a panic."
"I was like, well, we can't do that," she said. She thought it might be against the law.
Diamond Hill, in its statement to ProPublica, said it alerted all residents, their families and the state Health Department of any coronavirus infections in a timely and responsible manner.
But Gilmore said she left her boss's office and called the county Health Department herself. By coincidence, she reached Shannon Testo, a registered nurse in charge of communicable disease testing. Testo, who also spoke to ProPublica, told Gilmore that she had tried to contact Diamond Hill earlier that day.
She needed the administrators there to tell her who the positive patient had been in contact with, but she told Gilmore the home's administrators had stopped returning her calls.
Gilmore gave Testo a handwritten list of staff that had treated the patient. She was on it. Testo told her she would have to quarantine herself for 14 days. Gilmore signed an order from the county promising to do so.
Later that day, Diamond Hill's management circulated a memo to staff members informing them that the home had its first coronavirus case and that many of them had been exposed.
Several staffers called out sick. Some, like Gilmore, also quarantined themselves at the county's request.
After Gilmore's call, Testo and Lisa Phillips, the county Health Department's director of patient services, said they reported the situation to the state Department of Health, which then tested 30 randomly selected residents on April 6 using the limited number of tests available at the time. The county said at least six tests came back positive. The state told ProPublica it tested 26 residents and three came back positive.
Testo and Phillips said they tried to trace the contacts of the six positive patients, but Diamond Hill declined to answer their questions.
"With other facilities, the administrators gave us all the information we needed about who the positive patients may have had contact with," Phillips said. "But Diamond Hill, they kept telling us that the staff was protected, they had PPE, and therefore they had no contact, but then we started getting calls from other people who worked there saying they did not have PPE."
Testo and Phillips also said they later received reports from Diamond Hill employees that the administration tried to entice their employees to return to work before they completed their 14-day quarantines.
"They were offering Dunkin' Donuts gift cards to staff members who were sick but had no fever," Testo said.
Diamond Hill did not respond to the county's specific claims about how it managed the outbreak, but it insisted in its statement that it had cooperated with both the state and county to combat the virus's spread.
Gilmore said she twice received requests from Diamond Hill administrators to come back to work in spite of the county's order that she stay home. She tested negative for the coronavirus and returned to work on April 13. When she came back into the nursing home, she could not believe her eyes.
The facility had fewer staff than ever, but more patients. Gilmore said at least four aides and two nurses, including one who handled infection control, had quit. Others called out sick for fear of exposure. Those who continued working often had no protective gear. Patients who had the coronavirus were housed right next to those who did not, or even in the same room. "It was chaos," she said.
"Their system was that they would keep everyone new to the facility in isolation for seven to 14 days, but some of them are not coherent," Gilmore said. "They have dementia, and they were just wandering around."
At that point, Gilmore said Diamond Hill's corporate parent, the WeCare Centers, dispatched a new nurse manager to help bring some sense of order to the growing crisis. Gilmore said the new manager made things worse by continuing to insist that the facility was doing just fine.
She said she complained to him and then sent another email to Grinspan, the CEO, which she shared with ProPublica.
"I would not have reached out, if this didn't require immediate attention," she wrote. "These issues have been a problem since before this covid situation. The staff are coming to me with concerns... about residents covid status being withheld and PPE. The state already has issues with the way the facility handled the situation."
"I have seen nurses come and go as fast as they came due to a lack of staffing, support and appreciation," she said.
Gilmore said she was fired on April 15, two days after she sent her email to Grinspan.
She said she was told that she was not management material and was in fact "anti-management."
To the surprise of Testo and Phillips, even as the case counts and staff complaints grew at Diamond Hill, state health officials seemed fine with allowing more COVID-19 patients to be discharged to the facility. The county officials said they were on weekly conference calls with the state Health Department and the home's top administrators throughout April.
"They were telling the state that they were able to take more patients and the state wasn't getting involved. We didn't necessarily agree, because we were getting calls from their staff saying they were in crisis mode," Phillips said. The state's people raised no objections, however. "They were saying, 'If the [hospital] feels it's safe to discharge residents there and they say they can accept the patient, then that is their decision.'"
Documents show that Zucker, the health commissioner, said the facility had been assessed multiple times during April and into early May, and that no deficiencies were found.
"We have been in frequent communication with Diamond Hill nursing and rehabilitation center and they have attested to the Department that they are in full compliance with state and federal guidelines and have stated unequivocally that they are in need of no further assistance at this time," Zucker wrote in a May 10 letter to McLaughlin, the county executive.
Gilmore described her experience on local television and has filed a complaint against her managers at Diamond Hill with the New York State Division of Human Rights, alleging employment discrimination.
ProPublica shared Gilmore's story with the state Health Department. In a statement, it said that it had no record of her complaint, but that her "allegations would be unacceptable if true."
Phillips and Testo said that if Gilmore hadn't spoken up, the outbreak underway at Diamond Hill might have escaped scrutiny for far longer.
"Without those calls to us, the state never would have investigated," Phillips said. "She was worried that the administration was not being forthcoming with information we needed. So she took it upon herself to alert us."
The calls to the state came too late for Cynthia Falle, 73, a quadriplegic woman who had spent three years at Diamond Hill.
In mid-March, she contracted pink eye and showed other signs of failing health. Her family said they urged the administration to test her for the coronavirus. Twice, the facility refused, insisting a test was unnecessary and all she needed were antibiotics.
Her brother, Robert Falle, said that he complained to the state Department of Health, and that by late March the department had ordered Diamond Hill to test her. The facility told Sandra Wood, Falle's niece, the test had been done on April 4, and that on April 6 it had come back negative.
On April 13, a deteriorating Falle was taken to Samaritan Hospital. Wood said the hospital staff told her records they received from Diamond Hill did not reflect a COVID-19 test of any kind.
Wood said Falle was tested at Samaritan, the result was positive and she was dead within a week.
Diamond Hill did not respond to questions about Falle's care.
Falle had been diagnosed with cerebral palsy at age 3 and eventually lost the ability to move her limbs, yet her family said her life remained remarkably full until COVID-19 took it away. She'd fallen in love, traveled with her partner of 38 years, became a church deacon and worked with health care professionals to improve care for people with disabilities.
"Cindy will be remembered for her indomitable spirit, sense of humor, love, genuine interest in the people she knew and her amazing ability to thrive even under what many would consider insurmountable odds," her obituary read.
"It was dictatorial"
McLaughlin, the Rensselaer County executive, watched the troubles unfold at Diamond Hill with a sense of impotent fury. The home was the county's worst hot spot, its cases and deaths dwarfing those everywhere else.
He'd had a simple reaction to Cuomo and Zucker's March 25 order - "No way. Not ever." - and had blocked the transfer of COVID-19 patients from hospitals to the county-run Van Rensselaer Manor unless they tested negative before being moved. But he couldn't do that at Diamond Hill, a privately run home overseen by the state.
Some New York nursing home professionals themselves, already hit hard by the pandemic, say they had concerns much like McLaughlin's about the New York order. Those concerns were then compounded, they said, by the state acting without consulting them.
"We were struggling and overwhelmed already," said Elaine Healy, a medical director for a New Rochelle nursing home and acting president of the New York Medical Directors Association. "When the directive came, the thing that was most stunning was not only the content but the manner in which it was delivered. It was a one-way communique with no opportunity for dialogue and no opportunity to express concerns with the Department of Health. It was dictatorial."
Other states, notably California, adjusted their policies on hospital discharges to nursing homes after getting the industry's input, said Christopher Laxton, who heads the Society for Post-Acute and Long-Term Care Medicine, an organization of some 55,000 nursing home medical directors, physicians, nurses and other health care professionals.
Not New York, Laxton said, where Cuomo and Zucker "unaccountably failed to include clinical expertise in operational leadership when these policies were formed and we don't know why."
Bruno, the Health Department spokesman, disputed that claim. In an email, he noted that the state had weekly webinars with health care professionals beginning Feb. 2 and said it had engaged in ongoing dialogue with a range of experts, doctors, nurses, family members and advocates.
"There's been no shortage of industry, expert or stakeholder opinions in anything we've done during the most devastating global pandemic in a century," he said.
The state, in defending its performance in safeguarding nursing homes, told ProPublica New York ranks 35th among the 50 states when counting nursing home deaths as a percentage of its statewide loss of more than 30,000 lives.
McLaughlin inveighed against the state discharge order in interviews and on Twitter, attacking its logic in his more rural part of New York. In an April 28 letter petitioning Cuomo and Zucker to end the policy, he noted that hospitals in Rensselaer County had not been overwhelmed by the virus and that beds for COVID-19 patients remained available.
He wrote again on May 1, saying Diamond Hill was tied to 12 of the county's 20 COVID-19 deaths to that point and asking why COVID-19 patients were being discharged to a facility with long-standing care problems, particularly with infection control. He noted Diamond Hill had been cited in a June 2019 federal report as needing special oversight.
McLaughlin has long been at odds with Cuomo over a variety of issues, and he has sharply criticized various aspects of the governor's preparation for, and response to, the COVID-19 pandemic.
McLaughlin, in an interview with ProPublica, said the governor was a bully who ran from responsibility when his policies went awry.
Rich Azzopardi, an adviser to Cuomo, had acerbic words for McLaughlin.
"From the very beginning of this global pandemic, our response has been based on data, science, and the ability to adjust our approach as the evidence dictates," he said in an emailed statement. "Steve McLaughlin's response has been based on political cheap shots, public relations stunts, and an inability to make fact-based decisions."
New York state lawmakers, Republicans and Democrats, have now called for an independent investigation of Cuomo's policy.
Richard Gottfried, chair of the New York Assembly Health Committee, said he has asked Attorney General Letitia James to bring in outside counsel to examine not just the March 25 directive, but the state's long-term oversight, funding and standards for nursing homes.
In an interview, he said that the state has done "nothing about the chronic problem of understaffing in nursing homes," that it has kept "Medicaid funding for nursing homes sparse" and that state inspection teams are "seriously understaffed and have a track record of very lax enforcement."
"All of that leads to inadequate care, a culture that tolerates poor care and does not properly support the nursing homes that are trying to provide quality care," Gottfried said.
The Cuomo administration fired back at Gottfried.
"As long-time chair of the Assembly health committee and with 30 years in the state legislature, nobody in Albany has been in a better position than Assemblyman Gottfried to affect real change in the long-term health care system," Azzopardi said in his statement. "We welcome him to the discussion and applaud him for speaking out about the chronic problems in nursing homes under his three decades of leadership."
Five days after the Cuomo administration reversed its policy on discharging COVID-19 patients to nursing homes, a 76-year-old woman died of COVID-19 at Samaritan Hospital after having been infected at Diamond Hill.
McLaughlin ran into her son shortly after her death. He had few words of comfort.
The son said he'd last seen his mother on Feb. 25, but then all visitors had been barred. He said on the phone one day in April he noticed a change in her voice. She was soon hospitalized and spent three weeks in an intensive care unit before dying.
Now he wishes he'd brought her to live with him before she got COVID-19.
"I feel like I could have brought her home if they'd let me," said the son, who did not want to be identified by name because he is exploring legal action against the home.
Diamond Hill did not respond to questions about the case.
The woman had held a variety of jobs across 30 years of her working life - for Montgomery Ward, for the state motor vehicle department, for a local bowling center. She'd gone into Diamond Hill for what was supposed to be a brief stint of rehab.
"I thought I was putting her someplace safe," her son said. "Instead, I put her 6 feet under."
Mollie Simon and Benjamin Hardy contributed reporting.

Load-Date: June 20, 2020


End of Document




Mathematicians urge peers to stop working on racist "predictive policing" technology
Salon.com
June 24, 2020 Wednesday


Copyright 2020 Salon.com, LLC. All Rights Reserved
Length: 695 words
Highlight: "It is simply too easy to create a 'scientific' veneer for racism," the open letter states


Body


Link to Image
Male police officer using his laptop while out on patrol Getty Images
A group of American mathematicians wrote a letter in the trade journal Notices of the American Mathematical Society earlier this month calling on their disciplinary peers to stop working with law enforcement officials on predictive policing software. Such software purports to algorithmically "predict" where crimes will occur before they happen, to help police allocate resources; yet studies have found that such technology tends to exhibit            algorithmic bias that reproduces racial inequality. 
"In light of the extrajudicial murders by police of George Floyd,            Breonna Taylor,            Tony McDade and numerous others before them, and the subsequent brutality of the police response to protests, we call on the mathematics community to boycott working with police departments," the            letter states. Its authors go on to explain how many mathematicians work with police departments to develop models and data that will ostensibly help            prevent crime. They cite as one example how the Institute for Computational and Experimental Research in Mathematics (ICERM) sponsored a workshop on predictive policing. 
The authors also express concern over how artificial intelligence, facial recognition technologies and the use of machine learning could exacerbate systemic racism.
"Given the structural racism and brutality in US policing, we do not believe that mathematicians should be collaborating with police departments in this manner," the authors state. "It is simply too easy to create a 'scientific' veneer for racism. Please join us in committing to not collaborating with police. It is, at this moment, the very least we can do as a community."
They urge that any algorithm with a potential high impact receive a public audit and for "mathematicians to work with community groups, oversight boards, and other organizations dedicated to developing alternatives to oppressive and racist practices." Finally, they argue that universities with data science courses "implement learning outcomes that address the ethical, legal, and social implications of these tools."
Predictive policing technology is designed to identify which neighborhoods are supposedly more likely to experience violent crime and which individuals are supposedly more likely to either perpetrate it or be victims of it. Yet research from groups like the Human Rights Data Analysis Group (HRDAG) indicates that predictive policing reinforce racist practices among police officers because it often relies on data that is compromised by racial biases.
"Neighborhoods with lots of police calls aren't necessarily the same places the most crime is happening," William Isaac and Andi Dixon of HRDAG wrote in 2017. "They are, rather, where the most police attention is - though where that attention focuses can often be biased by gender and racial factors." Their research found that "predictive policing vendor            PredPol's purportedly race-neutral algorithm targeted black neighborhoods at roughly twice the rate of white neighborhoods when trained on historical drug crime data from Oakland, California. We found similar results when analyzing the data by income group, with low-income communities targeted at disproportionately higher rates compared to high-income neighborhoods." This is in spite of the fact that estimates from public health surveys and population models indicate that illegal drug activity in that city occurs approximately evenly across racial and income groups.
As Matthew Harwood and Jay Stanley from the American Civil Liberties Union observed:
Even if the data underlying most predictive policing software accurately anticipates where crime will indeed occur - and that's a gigantic if - questions of fundamental fairness still arise. Innocent people living in or passing through identified high crime areas will have to deal with an increased police presence, which, given recent history, will likely mean more questioning or stopping and frisking - and arrests for things like marijuana possession for which more affluent citizens are rarely brought in.

Load-Date: June 24, 2020


End of Document




    
     
  Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved.   
  California HealthlineCoronavirus Tests The Value Of Artificial Intelligence In Medicine
       California Healthline
May 21, 2020


  Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved.   


Length: 1230 words
Byline: Ashley Gold
Body

Dr. Albert Hsiao and his colleagues at the University of California-San Diego health system had been working for 18 months on an artificial intelligence program designed to help doctors identify pneumonia on a chest X-ray. When the coronavirus hit the United States, they decided to see what it could do.  
     
  The researchers quickly deployed the application, which dots X-ray images with spots of color where there may be lung damage or other signs of pneumonia. It has now been applied to more than 6,000 chest X-rays, and it's providing some value in diagnosis, said Hsiao, the director of UCSD's augmented imaging and artificial intelligence data analytics laboratory.  
     
  His team is one of several around the country that has pushed AI programs developed in a calmer time into the COVID-19 crisis to perform tasks like deciding which patients face the greatest risk of complications and which can be safely channeled into lower-intensity care.  
     
  The machine-learning programs scroll through millions of pieces of data to detect patterns that may be hard for clinicians to discern. Yet few of the algorithms have been rigorously tested against standard procedures. So while they often appear helpful, rolling out the programs in the midst of a pandemic could be confusing to doctors or even dangerous for patients, some AI experts warn.  
     
  "AI is being used for things that are questionable right now," said Dr. Eric Topol, director of the Scripps Research Translational Institute and author of several books on health IT.  
     
  Topol singled out a system created by Epic, a major vendor of electronic health records software, that predicts which coronavirus patients may become critically ill. Using the tool before it has been validated is "pandemic exceptionalism," he said.  
     
  Epic said the company's model had been validated with data from more 16,000 hospitalized COVID-19 patients in 21 health care organizations. No research on the tool has been published, but, in any case, it was "developed to help clinicians make treatment decisions and is not a substitute for their judgment," said James Hickman, a software developer on Epic's cognitive computing team.  
     
  Others see the COVID-19 crisis as an opportunity to learn about the value of AI tools.  
     
  "My intuition is it's a little bit of the good, bad and ugly," said Eric Perakslis, a data science fellow at Duke University and former chief information officer at the Food and Drug Administration. "Research in this setting is important."  
     
  Nearly $ 2 billion poured into companies touting advancements in health care AI in 2019. Investments in the first quarter of 2020 totaled $ 635 million, up from $ 155 million in the first quarter of 2019, according to digital health technology funder Rock Health.  
     
  At least three health care AI technology companies have made funding deals specific to the COVID-19 crisis, including Vida Diagnostics, an AI-powered lung-imaging analysis company, according to Rock Health.  
     
  Overall, AI's implementation in everyday clinical care is less common than hype over the technology would suggest. Yet the coronavirus crisis has inspired some hospital systems to accelerate promising applications.  
     
  UCSD sped up its AI imaging project, rolling it out in only two weeks.  
     
  Hsiao's project, with research funding from Amazon Web Services, the University of California and the National Science Foundation, runs every chest X-ray taken at its hospital through an AI algorithm. While no data on the implementation has been published yet, doctors report that the tool influences their clinical decision-making about a third of the time, said Dr. Christopher Longhurst, UC San Diego Health's chief information officer.  
     
  "The results to date are very encouraging, and we're not seeing any unintended consequences," he said. "Anecdotally, we're feeling like it's helpful, not hurtful."  
     
  AI has advanced further in imaging than other areas of clinical medicine because radiological images have tons of data for algorithms to process, and more data makes the programs more effective, said Longhurst.  
     
  But while AI specialists have tried to get AI to do things like predict sepsis and acute respiratory distress -- researchers at Johns Hopkins University recently won a National Science Foundation grant to use it to predict heart damage in COVID-19 patients -- it has been easier to plug it into less risky areas such as hospital logistics.  
     
  In New York City, two major hospital systems are using AI-enabled algorithms to help them decide when and how patients should move into another phase of care or be sent home.  
     
  At Mount Sinai Health System, an artificial intelligence algorithm pinpoints which patients might be ready to be discharged from the hospital within 72 hours, said Robbie Freeman, vice president of clinical innovation at Mount Sinai.  
     
  Freeman described the AI's suggestion as a "conversation starter," meant to help assist clinicians working on patient cases decide what to do. AI isn't making the decisions.  
     
  NYU Langone Health has developed a similar AI model. It predicts whether a COVID-19 patient entering the hospital will suffer adverse events within the next four days, said Dr. Yindalon Aphinyanaphongs, who leads NYU Langone's predictive analytics team.  
     
  The model will be run in a four- to six-week trial with patients randomized into two groups: one whose doctors will receive the alerts, and another whose doctors will not. The algorithm should help doctors generate a list of things that may predict whether patients are at risk for complications after they're admitted to the hospital, Aphinyanaphongs said.  
     
  Some health systems are leery of rolling out a technology that requires clinical validation in the middle of a pandemic. Others say they didn't need AI to deal with the coronavirus.  
     
  Stanford Health Care is not using AI to manage hospitalized patients with COVID-19, said Ron Li, the center's medical informatics director for AI clinical integration. The San Francisco Bay Area hasn't seen the expected surge of patients who would have provided the mass of data needed to make sure AI works on a population, he said.  
     
  Outside the hospital, AI-enabled risk factor modeling is being used to help health systems track patients who aren't infected with the coronavirus but might be susceptible to complications if they contract COVID-19.  
     
  At Scripps Health in San Diego, clinicians are stratifying patients to assess their risk of getting COVID-19 and experiencing severe symptoms using a risk-scoring model that considers factors like age, chronic conditions and recent hospital visits. When a patient scores 7 or higher, a triage nurse reaches out with information about the coronavirus and may schedule an appointment.  
     
  Though emergencies provide unique opportunities to try out advanced tools, it's essential for health systems to ensure doctors are comfortable with them, and to use the tools cautiously, with extensive testing and validation, Topol said.  
     
  "When people are in the heat of battle and overstretched, it would be great to have an algorithm to support them," he said. "We just have to make sure the algorithm and the AI tool isn't misleading, because lives are at stake here."


End of Document




Cal State LA researchers use data visualization, AI in fight against COVID-19
CaliforniaNewswire
April 20, 2020 Monday


Copyright 2020 CaliforniaNewswire, distributed by Contify.com All Rights Reserved
Length: 1020 words
Byline: Christopher Simmons
Body


LOS ANGELES, Calif. /California Newswire/ - Cal State LA researchers are joining the fight against COVID-19, developing tools that government officials and health care workers can use to make decisions during the global pandemic.
During the past month, as coronavirus cases escalated across the United States, teams of Cal State LA faculty, students and staff have focused their academic research on areas that can help with the response to the pandemic.
Researchers in the College of Business and Economics created an interactive data visualization dashboard that forecasts the number of COVID-19 cases and deaths by region in the U.S. and around the world. A faculty-student team in the College of Engineering, Computer Science, and Technology developed a predictive model using artificial intelligence (AI) and machine learning that determines COVID-19 patient health risks and predicts mortality risk to help hospitals and medical facilities.
"I commend our faculty, students, and staff for their continued dedication to engagement and service for the public good during this difficult and uncertain period," said Cal State LA President William A. Covino.
Executive Vice President and Provost Jose A. Gomez added: "These research contributions exemplify the commitment of our entire University community as we work together to fulfill Cal State LA's mission amid this pandemic."
Dalya (Manatova) Dauletbak, a 2019 graduate of the Master of Science in Information Systems program in the College of Business and Economics, was inspired by Johns Hopkins University's COVID-19 Map and wanted to develop a simple tool to help people better understand the COVID-19 situation in their regions, while also making it easier to make comparisons with other geographical areas.
The dashboard, developed using Tableau data visualization software, allows users to filter by state or region of interest and to see the trends of confirmed and forecasted new cases as well as deaths over time, which can help with resource planning, Dauletbak said.
The tool sources data from the Center for Systems Science and Engineering at Johns Hopkins University and is updated automatically daily.
"I believe that help can be found on different scales and from different domains, and even a small opportunity to help during this outbreak means a lot to me," said Dauletbak, who works as an information security data analyst at Cal State LA. "Keeping people informed with the correct data is one of the keys to flattening the curve."
Dauletbak is a member of the Big Data AI Center, which is housed in the Department of Information Systems in the College of Business and Economics. Dauletbak worked on the project under the direction of Professor Jongwook Woo, a faculty member in the department and director of the Big Data AI Center.
"Technology has developed to make the lives of human beings easier and safer," Woo said. "I am proud that the Big Data AI Center can use big data to contribute to the community quickly with the knowledge that people need now in this crisis."
The rapid spread of COVID-19 around the world and lack of sufficient medical resources prompted College of Engineering, Computer Science, and Technology Professor Mohammad Pourhomayoun and graduate student Mahdi Shakibi to focus their research on using AI and predictive analytics to help medical decision-making and support doctors and caregivers.
Pourhomayoun, an assistant professor of computer science, and Shakibi used AI and machine learning algorithms to develop a predictive model to determine the health risks and predict the mortality risk of patients with COVID-19 based on symptoms, physiological information, and demographic data.
Using data from 117,000 patients worldwide with laboratory-confirmed COVID-19, the model results have demonstrated an overall 93% prediction accuracy, Pourhomayoun and Shakibi reported in an article posted publicly online. They posted the article so medical facilities and other researchers can access the research while it undergoes peer review. The article has been submitted to the journal, IEEE Transactions on Biomedical Engineering.
The model can help hospitals and medical facilities decide who needs to get attention first, which patients have higher priority to be hospitalized, and which patients can stay at home, said Pourhomayoun, who also serves as director of the AI and Data Science Research Lab in the College of Engineering, Computer Science, and Technology.
The model can also help medical workers and caregivers triage patients if the healthcare system becomes overwhelmed and eliminate delays in providing necessary care, he said.
Pourhomayoun said he has begun discussions with health officials and local government about the potential use of the model.
"We have to remember that the main players and the real heroes are our doctors, nurses, and all health care workers risking their lives to save people on the front lines of the coronavirus fight," Pourhomayoun said. "But I think everyone with any expertise can try to help. Every researcher in every field of research-whether it is medicine or biology or computer science or engineering or social science-can contribute to help address the COVID-19 crisis."
Cal State LA is ranked number one in the nation based on the upward mobility of its students. Founded in 1947, Cal State LA is the premier public comprehensive university in the heart of Los Angeles and is dedicated to the mission of engagement, service, and the public good. The University serves more than 26,000 students and more than 250,000 distinguished alumni, who are as diverse as the region we serve. Led by an award-winning faculty, the University offers nationally recognized programs in science, the arts, business, criminal justice, engineering, nursing, education and the humanities.
Cal State LA is home to the critically-acclaimed Luckman Fine Arts Complex, Pat Brown Institute for Public Affairs, Hertzberg-Davis Forensic Science Center, Hydrogen Research and Fueling Facility, Billie Jean King Sports Complex and the TV, Film and Media Center. For more information, visit www.CalStateLA.edu.

Load-Date: April 21, 2020


End of Document




Statistic of the decade: The massive deforestation of the Amazon
Salon.com
December 28, 2019 Saturday


Copyright 2019 Salon.com, LLC. All Rights Reserved
Length: 729 words
Highlight: About 24,000 square miles of Amazon rainforest have been deforested over the last decade.


Body


Link to Image
This year, I was on the judging panel for the Royal Statistical Society's International Statistic of the Decade.
Much like Oxford English Dictionary's "Word of the Year" competition, the international statistic is meant to capture the zeitgeist of this decade. The judging panel accepted nominations from the statistical community and the public at large for a statistic that shines a light on the decade's most pressing issues.
On Dec. 23, we announced the winner: the 8.4 million soccer fields of land deforested in the Amazon over the past decade. That's 24,000 square miles, or about 10.3 million American football fields.
This statistic, while giving only a snapshot of the issue, provides insight into the dramatic change to this landscape over the last 10 years. Since 2010, mile upon mile of rainforest has been replaced with a wide range of commercial developments,            including cattle ranching, logging and the palm oil industry.
This calculation by the committee is based on deforestation monitoring results from Brazil's National Institute for Space Research, as well as            FIFA's regulations on soccer pitch dimensions.
Calculating the cost
There are a number of reasons why this deforestation matters - financial, environmental and social.
First of all, 20 million to           30 million people live in the Amazon rainforest and depend on it for survival. It's also the home to            thousands of species of plants and animals, many at risk of extinction.
Second, one-fifth of the world's fresh water is in the Amazon Basin, supplying water to the world by releasing water vapor into the atmosphere that            can travel thousands of miles. But unprecedented droughts have plagued Brazil this decade,            attributed to the deforestation of the Amazon.
During the droughts, in Sao Paulo state, some farmers say they lost over one-third of their crops due to the water shortage. The government promised the coffee industry almost US$300 million to help with their losses.
Finally, the Amazon rainforest is responsible for storing over 180 billion tons of carbon alone. When trees are cleared or burned,            that carbon is released back into the atmosphere. Studies show that            the social cost of carbon emissions is about $417 per ton.
Finally, as a November 2018 study shows, the Amazon            could generate over $8 billion each year if just left alone, from sustainable industries including nut farming and rubber, as well as the environmental effects.
Financial gain?
Some might argue that there has been a financial gain from deforestation and that it really isn't a bad thing. Brazil's president, Jair Bolsonaro, went so far as to say that saving the Amazon is an            impediment to economic growth and that "where there is indigenous land, there is wealth underneath it."
In an effort to be just as thoughtful in that sense, let's take a look. Assume each acre of rainforest converted into farmland is worth about $1,000, which is about what U.S. farmers have paid to buy productive farmland in Brazil. Then, over the past decade, that farmland amounts to about $1 billion.
The deforested land mainly contributes to cattle raising for slaughter and sale. There are a little over 200 million            cattle in Brazil. Assuming the            two cows per acre, the extra land means a gain of about $20 billion for Brazil.
Chump change compared to the economic loss from deforestation. The farmers, commercial interest groups and others looking for cheap land all have a clear vested interest in deforestation going ahead, but any possible short-term gain is clearly outweighed by long-term loss.
Rebounding
Right now, every minute, over three football fields of Amazon rainforest are            being lost.
What if someone wanted to replant the lost rainforest? Many charity organizations are raising money to do just that.
At the cost of over $2,000 per acre - and that is the cheapest I could find - it isn't cheap, totaling over $30 billion to replace what the Amazon lost this decade.
Still, the studies that I've seen and my calculations suggest that trillions have been lost due to deforestation over the past decade alone.
Liberty Vittert, Professor of the Practice of Data Science, Washington University in St Louis
This article is republished from The Conversation under a Creative Commons license.

Load-Date: April 13, 2020


End of Document




Uber's data revealed nearly 6,000 sexual assaults. Does that mean it's not safe?
Salon.com
December 15, 2019 Sunday


Copyright 2019 Salon.com, LLC. All Rights Reserved
Length: 663 words
Highlight: What's your safest option for a ride home?


Body


Link to Image
Since Uber released its first ever safety report on Dec. 5, the            media has raised alarms for the 5,981 instances of sexual assault included in the document.
This also includes 464 reports of rape over a two-year period - 2017 to 2018.
Uber also reported 97 fatal car accidents and 107 total deaths during the same period.
From my perspective as a data scientist, however, the numbers may not be as alarming as some reports have claimed.
On the road
In 2018, 36,560 people lost their lives in motor vehicle fatalities in the U.S., and            58 of those deaths were Uber-related.
One might think those are the only numbers necessary to inform a decision on Uber's safety record, but they aren't. In order to have an accurate assessment of Uber's motor vehicle fatality, the raw numbers must be compared to the number of miles traveled.
In 2018, Uber, when all the miles driven by all the drivers are added up, has 10.2 billion miles on the road. The national amount of miles driven in the U.S., during the same time period, in total is 3.2 trillion.
This means that the risk of dying in a motor vehicle accident as of 2018 with Uber is 0.57 deaths per 100 million miles driven, while the national risk is 1.13 deaths per 100 million miles driven.
For 2017, the U.S. Department of Transportation reported 1.16 deaths per 100 million miles driven for all motor vehicles.
This data implies that it is safer to drive in an Uber than your own car. There is no existing data for taxis.
There are many different reasons this could be true, including fewer drunk Uber drivers versus individual drivers and Uber drivers having more experience driving than an individual driver.
What about sexual assaults?
Uber reported 3,045 instances of sexual assaults in 2018, including everything from nonconsensual kissing to nonconsensual sexual penetration.
That raw number is shocking, and even one case of sexual assault is one too many. But do those numbers say that Uber is less safe than any other form of transportation?
In this case, Uber was responsible for over 1.3 billion rides that year, meaning the chance of being sexually assaulted in an Uber was 0.0002%.
In order to compare the safety of Uber to yellow cabs and other vehicles, researchers would need to know the number of reported sexual assaults for all types of transportation vehicles.
That data does not exist for the United States. However, it does exist for London, where Uber was            banned in 2017 due to "a lack of corporate social responsibility."
In 2016, there were 154 allegations of rape or sexual assault in London made to the police where the suspect was alleged to be a taxi driver - this includes Hackney cabs and Ubers. Uber drivers were allegedly involved in 32.
So Uber drivers are responsible for about 20% of the sexual assaults by taxi drivers.
Crunching the numbers
Is that number disproportionately high or low compared to the number of journeys?
According to the Transport for London office, every week there were two to three million journeys in all kinds of taxis, including Uber. Uber in London accounted for more than a million in 2016.
So Uber was responsible for over 30% of the journeys, but only 20% of the sexual assaults, meaning that there is no reason to believe that Uber drivers are any more dangerous then any other kind of taxi driver. Actually, quite the opposite.
In June 2018, the Magistrates' Court overturned the ban on Uber, giving the company a 15-month probationary license. But in November, Uber was again            banned in London due to alleged "repeated            safety failures."
While any sexual assault is one too many and one can never diminish the seriousness of these issues, critics need to take a closer look at the statistics to make a truly informed decision about Uber's safety.
Liberty Vittert, Professor of the Practice of Data Science, Washington University in St Louis
This article is republished from The Conversation under a Creative Commons license.

Load-Date: April 13, 2020


End of Document




QAnon conspiracy theories about the coronavirus pandemic are a public health threat
Salon.com
April 12, 2020 Sunday


Copyright 2020 Salon.com, LLC. All Rights Reserved
Length: 964 words
Highlight: QAnon is spreading harmful misinformation about COVID-19.


Body


Link to Image
Supporters cheer for Donald Trump at a rally, August 4, 2018. Getty/Scott Olson
First there was the pandemic, then came the "infodemic" - a term the head of the World Health Organization defines as the spread of false information about COVID-19.
The most dangerous conspiracy theories about the coronavirus are now part of the QAnon phenomenon. For months now, actors in QAnon have downplayed the severity of the crisis, amplified medical disinformation and have been originators of hoaxes.
The QAnon movement started in 2017 after someone using an anonymous account known only as Q posted wild conspiracy theories about U.S. President Donald Trump on the internet forum 4chan.
QAnon conspiracy theorists believe a deep state cabal of global elites is responsible for all the evil in the world. They also believe those same elites are seeking to bring down Trump, whom they see as the world's only hope to defeat the deep state. QAnon has now brought the same conspiracy mentality to the coronavirus crisis.
As a researcher of online movements like QAnon, I use a combination of data science and digital ethnography to research how extremist movements use technology to create propaganda, recruit members to ideological causes, inspire acts of violence or impact democratic institutions.
Bottom-up approach
A central component of QAnon is the crowdsourcing of narratives. This bottom-up approach provides a fluid and ever changing ideology. My analysis of Twitter shows from January to March, there was a 21 per cent increase (a total of 7,683,414 posts) in hashtags used by the QAnon community. This means the misinformation they spread has the capacity to reach a wider audience.
For instance, QAnon community influencers on Twitter promoted Miracle Mineral Supplement as a way of preventing COVID-19. The toxic product was sold by the Texas-based Genesis II Church of Health and Healing for US$45. The U.S. Food and Drug Administration            had previously issued a warning about the dangerous and potentially life threatening side effects of the supplement.
In January, QAnon was amplifying narratives on 8kun (           the internet forum formally known as 8chan), Facebook and Telegram (an encrypted instant messaging plaform) about a false theory that Asians were more susceptible to the coronavirus and that white people were immune to COVID-19. Not only are there racist undertones associated with this disinformation, it minimizes the threat posed by the virus.
Downplayed threat
From February until the second week of March, QAnon followed the lead of Trump in downplaying the threat of the virus and calling it a hoax. They believed the virus was a deep state plot to damage the president's chance at re-election. The QAnon community said those warning about the pandemic threat were trying to detract from U.S. domestic politics, stop Trump rallies and remove all the economic gains they contended had occurred during the Trump presidency.
After the WHO upgraded COVID-19 to pandemic status and the U.S. announced it was closing its borders to most people from Europe for 30 days, QAnon changed the narrative again. Suddenly, QAnon thought the pandemic was something to celebrate because it was a cover for the Trump administration's secret plan            to arrest deep state agents.
Evangelicals within the the QAnon movement viewed the pandemic as the promised coming of the Kingdom of God on Earth. David Hayes, who is better known as the Praying Medic and an influencer in the QAnon community with 300,000 YouTube subscribers, said in a March 14 livestream that there was no reason to be concerned about COVID-19. Hayes reassured his viewers that they may not be affected by the disease because this was "spiritual warfare" - only those who have not been chosen by God will be affected by the disease.
The person known as Q, who spawned the QAnon movement, didn't post anything online about COVID-19 until March 23. Up until then, all of the medical disinformation, hoaxes and downplaying of the pandemic had been sourced from QAnon influencers and community.
Public health threat
In his first post on the topic of COVID-19, Q pushed a conspiracy theory with racial undertones about COVID-19 being a Chinese bioweapon and that the virus release was a joint venture between China and the Democrats to stop Trump's re-election by destroying the economy.
The QAnon conspiracies have created an environment of complacency among its followers who aren't taking the risks posed by the virus seriously.
Florida pastor Rodney Howard-Browne, who has given credence to QAnon in the past and has preached that the coronavirus was planned by the Bill and Melinda Gates Foundation,            was arrested after holding Sunday services and disregarding federal, state and county orders to limit gatherings to less than 10 people. His conspiratorial beliefs led to his negligent actions, which put hundreds of people from his congregation at risk.
In another instance, right-wing media figures were spreading an "empty hospital" conspiracy, downplaying the pandemic and its death toll.
A QAnon account originally launched the #FilmYourHospital hashtag. This was amplified by QAnon influencers such as former California congressional candidate DeAnna Lorraine Tesoriero and QAnon influencer            Liz Crokin. This hoax was then picked up by mainstream right-wing media figures promoting COVID trutherism to a wider audience.
The FBI once called conspiracy theories spread by QAnon and others a "potential domestic terrorism threat." It's time to call the infodemic a public health threat.
Marc-Andr  Argentino, PhD candidate Individualized Program, 2020-2021 Public Scholar, Concordia University
This article is republished from The Conversation under a Creative Commons license.

Load-Date: April 13, 2020


End of Document




UC Health Launches Online Dashboard of COVID-19 Cases,Testing at its Hospitals
City News Service
April 6, 2020 Monday 6:07 PM PST


Copyright 2020 City News Service, Inc. All Rights Reserved
No City News Service material may be republished without the express written permission of the City News Service, Inc.
Length: 328 words
Body


SAN DIEGO (CNS) - University of California Health announced today that it will send out daily online updates regarding the number of positive COVID-19 tests and age distribution of confirmed cases from its five medical centers across the state, including UC San Diego Health.
The online dashboard, which can be viewed on the @UofCAHealth Twitter account, will display daily testing information from UCSD Health, UC Davis Health, UCI Health, UCLA Health and UCSF Health.
UC Health officials say the updates will help state and federal officials see the rates of increase and geographic spread of COVID-19 cases in near real-time.
"We're able to provide a near real-time picture of the current COVID-19 situation across our system, which because of its reach can be a proxy for the state conditions," said Atul Butte, chief data scientist and the director of the Center for Data-driven Insights and Innovation at UC Health. "Thanks to the architecture and functionality of the data system, users can see the information at the level they need whether that is a statewide overview or specifics by location."
To date, UC Health says more than 13,000 patients have been tested at its medical centers, with a positive rate of 6.64%.
The initial post on the Twitter page states that 2,766 patients were tested at UCSD, with 116 of those cases testing positive for COVID-19. Results are pending for 73 patients.
"Real-time data is critical to the practice of medicine today," said Dr. Carrie L. Byington, executive vice president of UC Health. "By drawing on our deep and knowledgeable data science experts across the university, we are able to rapidly build and launch this reporting tool. It is critical information to create transparency on key pandemic-related trends in the state.
"We're uniquely positioned with our academic health centers located across the state to give public health and elected officials the visibility they need for responding to this pandemic," she added.

Load-Date: April 7, 2020


End of Document




UC Health Launches Online Dashboard of COVID-19 Cases,Testing at its Hospitals
City News Service
April 6, 2020 Monday 6:19 PM PST


Copyright 2020 City News Service, Inc. All Rights Reserved
No City News Service material may be republished without the express written permission of the City News Service, Inc.
Length: 324 words
Body


LOS ANGELES (CNS) - University of California Health announced today that it will send out daily online updates regarding the number of positive COVID-19 tests and age distribution of confirmed cases from its five medical centers across the state, including UCLA and UCI.
The online dashboard, which can be viewed on the @UofCAHealth Twitter account, will also display daily testing information from UCSD Health, UC Davis Health and UCSF Health.
UC Health officials say the updates will help state and federal officials see the rates of increase and geographic spread of COVID-19 cases in near real-time.
"We're able to provide a near real-time picture of the current COVID-19 situation across our system, which because of its reach can be a proxy for the state conditions," said Atul Butte, chief data scientist and the director of the Center for Data-driven Insights and Innovation at UC Health. "Thanks to the architecture and functionality of the data system, users can see the information at the level they need whether that is a statewide overview or specifics by location."
To date, UC Health says more than 13,000 patients have been tested at its medical centers, with a positive rate of 6.64%.
The initial post on the Twitter page states that 2,766 patients were tested at UCSD, with 116 of those cases testing positive for COVID-19. Results are pending for 73 patients.
"Real-time data is critical to the practice of medicine today," said Dr. Carrie L. Byington, executive vice president of UC Health. "By drawing on our deep and knowledgeable data science experts across the university, we are able to rapidly build and launch this reporting tool. It is critical information to create transparency on key pandemic-related trends in the state.
"We're uniquely positioned with our academic health centers located across the state to give public health and elected officials the visibility they need for responding to this pandemic," she added.

Load-Date: April 7, 2020


End of Document




    
     
  Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved.   
  California HealthlineNo Safety Switch: How Lax Oversight Of Electronic Health Records Puts Patients At Risk
       California Healthline
November 21, 2019


  Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved.   


Length: 3343 words
Byline: Fred Schulte and Erika Fry, Fortune
Body

In fall 2009, several dozen of the best minds in health information technology huddled at a hotel outside Washington, D.C., to discuss potential dangers of an Obama White House plan to spend billions of tax dollars computerizing medical records.  
     
  The health data geeks trusted that transitioning from paper to electronic records would cut down on medical errors, help identify new cures for disease and give patients an easy way to track their health care histories.  
     
  But after two days of discussions, the group warned that few safeguards existed to protect the public from possible consequences of rolling out the new technology so quickly. Because this software tracks the medicines people take and their vital signs, even a tiny error or omission, or a doctor's inability to access the file quickly, can be a matter of life or death.  
     
  The experts at that September 2009 meeting, mainly members of the American Medical Informatics Association, or AMIA, agreed that safety should be a top priority as federal officials poured more than $ 30 billion into subsidies to wire up medical offices and hospitals nationwide.  
     
  The group envisioned creating a national databank to track reports of deaths, injuries and near misses linked to issues with the new technology.  
     
  It never happened.  
     
  Instead, plans for putting patient safety first -- and for building a comprehensive injury reporting and reviewing system -- have stalled for nearly a decade, because manufacturers of electronic health records (EHRs), health care providers, federal health care policy wonks, academics and Congress have either blocked the effort or fought over how to do it properly, an ongoing investigation by Fortune and Kaiser Health News shows.  
     
  Over the past 10 years, the parties have squabbled over how best to collect injury data, over who has the power to require it, over who should pay for it, and over whether to make public damning findings and the names of those responsible for safety problems.  
     
  In 2015, members of Congress derailed a long-planned EHR safety center, first by challenging the government's authority to create it and later by declining to fund it. A year later, Congress stripped the Food and Drug Administration of its power to regulate the industry or even to track malfunctions and injuries.  
     
  "A lot of people involved with patient safety and medical informatics were horrified," said Ross Koppel, a University of Pennsylvania sociologist and prominent EHR safety expert. Koppel said the industry won legal status as a "regulatory free zone" when it came to safety, an outcome he called a "scandal beyond belief."  
     
  The Electronic Health Record Association, a trade group that represents more than 30 vendors, declined to comment on the safety issue.  
     
  Meanwhile, patients remain at risk of harm. In March, Fortune and KHN revealed that thousands of injuries, deaths or near misses tied to software glitches, user errors, interoperability problems and other flaws have piled up in various government-sponsored and private repositories. One study uncovered more than 9,000 patient safety reports tied to EHR problems at three pediatric hospitals over a five-year period.  
     
  Allegations of EHR-related injuries or other flaws have surfaced in the courts. KHN/Fortune examined more than two dozen such cases, such as a California woman who mistakenly had most of her left leg amputated because the EHR sent another patient's pathology report indicating cancer to her medical file. A Vermont patient died after a doctor's order to scan her brain for an aneurysm never made it from the computer to the lab.  
     
  Despite such incidents, experts believe EHRs have made medicine safer by eliminating errors due to illegible handwriting and in some cases speeding up access to vital patient files. But they also acknowledge they have no idea how much safer, or how much the systems could still be improved because no one -- a decade after the federal government all but mandated their adoption -- is assessing the technology's overall safety record.  
     
  KHN and Fortune found that at least a dozen expert commissions, federal health IT panels and medical associations have echoed AMIA's early call to track EHR safety risks only to be thwarted by objections from the industry or its allies, or by simple bureaucratic inertia. Some critics see the situation as a dispiriting Washington tale of corporate "capture" of government, while others wonder why a warning system to alert health officials to dangers with certain software is even controversial.  
     
  "How is it in the public interest for medical records software to have flaws that lead to deaths?" said Joshua Sharfstein, who served as FDA deputy commissioner when the safety issue flared up during President Barack Obama's first term. These incidents "should be fully understood and investigated" and "not be able to be buried."  
     
  Support for computerizing medical records has spanned the political spectrum. The health IT industry's aversion to FDA oversight has won support at critical times both with liberals who embraced EHRs as a high-tech magic bullet for reforming the nation's costly health care system and with free-market conservatives skeptical of red tape and government interventions.  
     
  The vendors protested they were overburdened with technical requirements that their software had to meet to qualify for the government subsidy program. Those specifications included many relatively small-bore features, like including a check box indicating the doctor had asked about the patient's smoking status -- and other tasks likely to have little impact on safety.  
     
  Complicating things further, many safety advocates themselves have worried that heavy-handed oversight -- such as requiring approval of every software update -- could actually make the technology less safe, stalling urgent software updates (not to mention stifling innovation and slowing the marketing of vital new technology).  
     
  After a contentious process in which consumer advocacy group Public Citizen accused FDA officials of collaborating with the devices industry to weaken oversight, Congress passed the 21st Century Cures Act. A few sentences buried in the law, signed by Obama in late 2016, all but shut the door on FDA regulation of EHRs.  
     
  The bipartisan law, which speeds up approvals for some medical therapies, states flatly that electronic health records are not medical devices subject to FDA scrutiny. Some longtime EHR safety advocates say they have all but given up hope for consensus on any system that would investigate and share findings from adverse events, as happens in other industries, like transportation and aviation.  
     
  "We have nothing like that," said Justin Starren, director of the Center for Data Science and Informatics at Northwestern University. "We have the opposite ... with vendors saying that customers are explicitly forbidden from publicizing problems they encounter."  
     
  Starren noted that health care providers don't like to share safety failures either: "It's the liability fear. If an institution holds up its hand and says, 'Our EHR might be killing people,' the lawyers will be lining up outside the courthouse door."  
     
  Less Red Tape Unleashes Red Flags?  
     
  In the months before the 2009 AMIA meeting, concern was mounting at the FDA over the rapidly advancing EHR rollout.  
     
  Since the mid-1980s, however, the FDA had considered health IT to present a low risk of harm because a "learned intermediary," such as a doctor, was in charge. Most manufacturers agreed and insisted their products were not medical devices, but vehicles for processing and storing medical information.  
     
  The legal distinction is critical. While the FDA requires device makers to report adverse events, the policy in place gave EHR manufacturers a pass. At least one major vendor, Cerner Corp., has concluded that EHRs are, in fact, medical devices and has submitted some error reports to FDA's public MAUDE database. But most manufacturers disagree and have not reported data, leaving a sizable gap in the agency's grasp of possible hazards.  
     
  Within the FDA, some staffers urged the agency to rethink the hands-off stance given the rush by hundreds of health IT companies -- many of them new entrants -- to sell medical records software that tens of thousands of doctors, hospitals and patients would rely on.  
     
  On Sept. 22, 2009, FDA staff shared their views with deputy commissioner Sharfstein and his boss, commissioner Margaret Hamburg, at a "regulatory strategy" meeting. After hearing the pitch, Hamburg agreed the FDA "needs to be involved in the White House [EHR] initiative," according to an agency memo. Hamburg had no comment for this article.  
     
  One former FDA official recalls tension mounting as the agency became more assertive, saying: "It was a big train going down the tracks at 80 miles per hour, and there were concerns that FDA would slow it down."  
     
  The FDA sounded a public warning at a February 2010 hearing. The agency's chief devices regulator, Jeffrey Shuren, testified that even with limited surveillance, the FDA had tied six deaths and 44 reported injuries to health information technology failures.  
     
  In all, Shuren said, the FDA had logged 260 reports of "malfunctions with the potential for patient harm" over the previous two years. In one case, the software filed results from emergency lab tests to the wrong patient's electronic record.  
     
  Shuren described the reports as likely the "tip of the iceberg" and said they suggested "significant clinical implications and public safety issues." He laid out three options for FDA involvement, the least burdensome being registration of EHR software and mandatory reporting of dangerous incidents. Through an agency spokesperson, Shuren declined to be interviewed for this article.  
     
  Shuren's 2010 testimony did not appear to carry much weight with David Blumenthal, a Harvard physician chosen as the Obama administration's point man for the digital medical record rollout. Blumenthal declined to comment.  
     
  Many in Blumenthal's division, known as the Office of the National Coordinator for Health Information Technology, or ONC, sympathized with the industry's assertion that FDA regulation would discourage innovation, which, in turn, could cripple the president's plans to revolutionize health care and save money. Blumenthal, who was convinced EHRs would make medicine much safer, described the FDA injury reports as "anecdotal."  
     
  An obscure outpost of the Department of Health and Human Services in the second Bush administration, ONC under Blumenthal revved up as federal officials laid plans for distributing billions of stimulus dollars.  
     
  The stimulus law directed ONC to set up two diverse advisory panels so that no single faction of the health care sector could unduly influence policy. Yet it seemed clear, at least to skeptics, that ONC depended heavily on the goodwill, expertise and guidance of the technology community.  
     
  Steven Findlay, who served on one of the panels as a representative of Consumers Union, said industry witnesses often "commandeered" the discussions because they "had the technical knowledge to steer things in a direction that they wanted."  
     
  Safety "was not necessarily their first priority. They were building products to serve an industry and designing them to make money," Findlay said in a recent interview.  
     
  Dean Sittig, a medical informaticist at UTHealth in Houston and early researcher on EHR safety, said ONC was "trying to promote" digital records "and there wasn't a lot of interest in talking about things that could go wrong." That conflict persists, he said. "They gave out $ 36 billion. It's hard for them to say EHRs aren't safe."  
     
  The ONC did form a safety "working group." The panel suggested that doctors and hospitals be required to report "potential hazards" and "incidents" to a national database or risk forfeiting government subsidies for purchasing records software, according to minutes from its March 12, 2010, meeting.  
     
  That idea never got past the drafting stage, however.  
     
  Glitches In The Matrix  
     
  In a nod to safety, ONC asked the National Academy of Sciences' Institute of Medicine to weigh in, a move some at the FDA hoped would at the least lend support for nationwide collection of injury data.  
     
  When the 18-member expert panel held a public hearing in mid-December 2010, Shuren reappeared with updated FDA figures -- about 370 reports of "adverse events or near misses" involving health IT since January 2008. Once again, he called FDA's count a "small percentage of the actual [adverse] events that do occur."  
     
  Among the causes he cited: failure of the software to interface properly with other technologies, user errors, design flaws and inadequate pre-market testing.  
     
  Shuren suggested EHRs were medical devices over which the FDA "has exercised enforcement discretion; meaning it has not enforced existing requirements," an apparent reference to the hands-off policy. He called for "real-time collection, aggregation and analysis" of reports on the functioning of EHRs.  
     
  The Institute of Medicine panel in November 2011 called on HHS to make adverse incident reporting mandatory for vendors and voluntary for users. It also said HHS should ask Congress to approve a government-run injury monitoring system as rigorous as that used to promote airline safety that would both investigate and make its findings public. The FDA might not be the best-equipped agency to take on the task, the group noted.  
     
  The panel asserted that EHR vendors face "competing priorities, including maximizing profits and maintaining a competitive edge, which can limit shared learning and have adverse consequences for patient safety."  
     
  One member called for even stricter oversight. In an impassioned dissent, Richard Cook, a Chicago radiologist and safety expert, argued EHRs were medical devices that necessitated the scrutiny of the FDA.  
     
  "At least a few U.S. citizens -- perhaps more than a few -- have died or have been maimed because of health IT. The extent of the injuries generated by health IT is unknown because no one has bothered to look for them in a systematic fashion," Cook wrote in his dissent.  
     
  Backtracking On Oversight  
     
  In 2012, Congress required FDA, ONC and the Federal Communications Commission to propose "risk-based" oversight for health IT that "promotes innovation, protects patient safety, and avoids regulatory duplication."  
     
  Two years went by before the agencies did so. In April 2014, they promoted a "limited, narrowly tailored approach" to oversight led by the ONC as well as a "surveillance mechanism" to track adverse events and near misses.  
     
  ONC's budget for the 2015 and 2016 fiscal years proposed spending $ 5 million for such a center, which ONC said would begin "a robust collection and analysis of health IT-related adverse events."  
     
  But four House Republicans in June 2014 questioned whether ONC had the legal authority to set up the center.  
     
  Energy and Commerce Committee Chairman Fred Upton of Michigan, Vice Chairman Marsha Blackburn of Tennessee, health subcommittee Chairman Joseph Pitts of Pennsylvania and communications and technology subcommittee Chairman Greg Walden of Oregon argued that ONC had failed to satisfy their concerns over what Blackburn termed regulatory "mission creep." At a House hearing in July 2014, Blackburn repeated her worry about "a misguided system of regulation."  
     
  Former ONC director Karen DeSalvo said she was five months on the job and felt completely blindsided by the line of questioning -- despite the National Academy of Sciences report years earlier that had advised HHS to seek approval from Congress to expand ONC's oversight role. The center's prospects dimmed further when the Congressional Research Service issued a report on the matter in early 2015 that seemed to side with the Republicans.  
     
  DeSalvo's team later requested legislative authority to create the center, but the effort was not successful. ONC was granted legislative authority for other requests, however, empowering it to define interoperability and to crack down on vendors who improperly restrict access to medical records.  
     
  These days, many of the key players have conflicting opinions and recollections about what went wrong and why.  
     
  DeSalvo, now a professor of medicine and population health at Dell Medical School, said she really doesn't know if something sinister torpedoed the safety center or it was just a matter of not enough people caring. "It was really just kind of start and stop," she said. That's perhaps not surprising, considering ONC has had seven directors in its 15 years of existence -- and six since 2009, when the government made EHRs a national priority. (And that's not counting four interim directors who collectively helmed the outfit for 16 months.)  
     
  Doug Fridsma, who left his role as ONC's chief scientist in 2014, cited other factors that slowed the center's momentum. He said uncertainty over its mission didn't help gain the trust of the industry, while citing other thorny issues, such as who would foot the bill and whether its data might be used to discipline or otherwise harm vendors. Fridsma, now AMIA's chief executive, said that government-sponsored regional patient safety organizations aren't well equipped to conduct national oversight of EHR functions.  
     
  "It has resulted in a vacuum around health IT safety," said Fridsma. "Congress has failed to make it a priority."  
     
  Shifting Public Attention  
     
  Revisiting plans for a full-fledged EHR safety center holds little appeal to Don Rucker, the Trump administration's ONC chief.  
     
  Rucker said he sees little value in collecting data on incidents often "years and years" after they occurred. Rapidly evolving technologies are making computer errors easier to recognize and remedy. "We can catch these things a lot earlier," he said.  
     
  Rucker argues that the 21st Century Cures Act prohibits the industry from enforcing "gag" clauses that in the past have handcuffed hospitals and doctors from criticizing their EHRs. The Cures law includes fines of up to $ 1 million for "information blocking," including taking steps to discourage EHR users from reporting adverse events and other problems for review.  
     
  New freedom to sound off assures that doctors and hospitals will begin sharing EHR problems, mitigating any need for mandatory reporting, in Rucker's view. Rucker said he hopes to have the regulations in place by the end of the year.  
     
  The proposed ONC regulations cite a "strong public interest" in "open communication of information regarding health hazards, adverse events and unsafe conditions." But that information won't be shared with the public. ONC says all reports of problems are exempt from public release under the Freedom of Information Act. Congress gave these records the same legal status as income tax returns as part of the Cures law.  
     
  Jacob Reider, a former ONC interim director, said the government's failure to do more to promote public awareness of safety concerns is disappointing -- and even irresponsible -- given its zeal in bringing EHRs into the mainstream of medicine.  
     
  "I remember internal conversations where we talked about 'What is the equivalent of a plane crash that is going to get the attention of people?'" said Reider, who now practices family medicine in upstate New York. "'Is it going to be a congressperson's relative is harmed by health IT that causes the attention to shift?' I would offer that still hasn't happened yet, but someday it will. And gosh, wouldn't it be a horrible thing that we have to wait for that to happen?"


End of Document




Berkeley Coding Academy Launches 2020 Summer Camp in Machine Learning and A.I.
CaliforniaNewswire
March 10, 2020 Tuesday


Copyright 2020 CaliforniaNewswire, distributed by Contify.com All Rights Reserved
Length: 656 words
Byline: Christopher Simmons
Body


BERKELEY, Calif. /California Newswire/ - Berkeley Coding Academy, a new computer science team of credentialed teachers, is offering summer camps to empower youth in Python Programming, Data Analytics, Machine Learning, and Artificial Intelligence.
Machine Learning has not been an option for middle school and high school students until now. Machine Learning is the ability of computers to make predictions from data without being explicitly programmed. More importantly, Machine Learning allows systems to problem-solve and refine themselves again and again, making it the most valuable skill in the economy today.
Corey Wade, chair of the math department at Berkeley Independent Study and lead author of "The Python Workshop," developed BCA to bring Machine Learning to a wider and younger audience. A multiple grant award winner, Wade has run after school coding clubs with students to create iPhone Apps, design websites, and analyze big data. Wade realized that bringing Machine Learning to the high school curriculum, however, was a slow process. The urgency and potential of high school students to become acquainted with Machine Learning inspired Wade to start BCA right away.
BCA partnered with Hands-On!, Berkeley's first STEAM lab run by credentialed teacher Tracy Hollander. Hands-On! provides STEAM camps during summer for ages 5-11. BCA will provide Machine Learning & AI camps at the same location for ages 12-18. All instructors at BCA and Hands-On! are Berkeley credentialed teachers with deep experiences in the classroom.
According to Wade, "Many talented people understand Machine Learning, but very few teach Machine Learning to high school students. It's definitely a challenge. When I look at what my students accomplish in math, I know they can learn Machine Learning. Our students, given the right guidance and motivation, can do anything."
What makes BCA special is that Bay Area middle school and high school students now have the rare opportunity to learn Machine Learning & AI with an experienced and passionate teacher.
Here are 5 facts about Machine Learning & AI in the world today, from Finances Online. (https://financesonline.com/machine-learning-statistics/)
1. 80% of companies plan to adopt AI for customer service by 2020. (oracle.com)
2. 97% of mobile users use AI-powered voice assistants. (creativestrategies.com)
3. $28.5 billion - The total funding allocated to machine learning worldwide during the first quarter of 2019. (statista.com)
4. $13 trillion - The potential global economy that AI could deliver by 2030. (mckinsey.com)
5. 74% of companies say ML will transform jobs and industries. (memsql.com)
Here are 5 reasons why Bay Area students should enroll in a Machine Learning & AI summer camp with BCA.
1. Machine Learning is a game-changer - Students who build Machine Learning models will gain an edge. Machine Learning is usually reserved for upper division college students, graduates, and data science professionals. BCA changes the status quo by making Machine Learning available to teens.
2. Big data is bigger than ever - BCA students learn how to program in Python and analyze big data. Using Python to analyze big data is the work of a data analyst and typically commands a large salary.
3. Friendships and mentorships can transform lives - Learning from and working with like-minded peers can help solidify technical skills and forge lifelong friendships. Moreover, BCA encourages developing relationships with mentors, potentially leading to exciting opportunities that extend beyond summer.
4. Limitless opportunities for social impact - Working with big data and coding in Python is valuable to many people. Opportunities to help others in camp will arise, and opportunities to help local communities may surface.
5. BCA teaches professional coding skills through "real-world" datasets - The code that BCA teaches is the real deal. It's the same code used by professional data scientists.

Load-Date: March 11, 2020


End of Document




In YouTube "edutainment," minimal control for scientific accuracy
Salon.com
February 23, 2020 Sunday


Copyright 2020 Salon.com, LLC. All Rights Reserved
Length: 2001 words
Highlight: Misconduct by a self-described technology activist raises questions about science communication on YouTube


Body


Link to Image
Chalisa Thammapatanakul/EyeEm/Getty Images
Siraj Raval enthusiastically begins his YouTube videos by greeting his audience with the first two words any aspiring coder learns to produce: "Hello World!"
A self-described technology activist, Raval has built a YouTube following of almost 700,000 over the past four years with popular videos like "TensorFlow in 5 Minutes," which explains a popular software platform used in artificial intelligence research, and "How to Make Money as a Programmer in 2018." To keep his audience entertained and engaged with topics like machine learning and Bitcoin, Raval uses flashy graphics, memes, and even raps. The schtick is often referred to as edutainment - a cross between education and entertainment.
Edutainment aims to teach, "but it employs some of the strategies of entertainment in order to do so," says Gordon Carlson, an associate professor in the communications studies department at Fort Hays State University in Kansas. On YouTube, edutainment is both a popular and wildly diverse genre, encompassing videos about the mathematics of cake cutting and what            tattooing looks like in slow motion.            A 2018 Pew survey of nearly 5,000 adults in the United States found that about nine in 10 users of the site value it as a learning resource, while in            a similar study,            60 percent of Generation Z respondents preferred using YouTube to learn over books. Top YouTube channels like Raval's receive the majority of these views - one            2018 estimate found that the top 3 percent of channels take in 85 percent of all views.
While Raval's videos have been praised for their high production quality and accessibility, his work has recently been called into question. Last summer, for instance, his online course called "Make Money with Machine Learning" turned out to be effectively a scam, as The Register reported, and Raval was forced to refund hundreds of students. Around the same time, Raval published an            academic paper that was later revealed to be            plagiarized.
Backlash ensued. Critics found multiple instances where Raval failed to properly attribute code and questioned his lack of credentials. Though he claimed to be a data scientist, Raval never graduated from college and had little industry experience. Raval has since posted two video           apologies admitting that he'd made mistakes, but saying that he would remain committed to inspiring people to learn computer science. (Raval did not respond to multiple requests from Undark for comment.)
Researchers who study platforms like YouTube have been concerned about falsehoods - as well as other worrying trends, such as toxic ideologies - for years. "As one colleague recently said, 'Basically, YouTube is the Wild West'," says Joachim Allgaier, a sociologist who studies science communication at RWTH Aachen University in Germany. Raval's story, then, is unusual not for what he did, but for what he didn't do. None of his videos were "fake news" or pseudoscientific. He didn't spread conspiracy theories or hate. Instead, his hustle was entertaining and nominally educational videos.
There's no reason to think that Raval's specific misconduct is part of a pattern by YouTube edutainers, but the case raises questions about qualifications, substance, and accuracy on a platform that is the primary source of extra-scholastic science education for millions of people.
Drawing conclusions about the vast amount of scientific information, let alone edutainment specifically, on YouTube is difficult because it remains understudied, according to Asheley Landrum, an expert in science communication at Texas Tech University.
One exception is health information, which has been comparatively well-surveyed, with dozens of studies that examine the quality and accuracy of YouTube videos across topics such as anorexia, heart attacks, and smoking. There is no consensus, but in general, there appears to be plenty of misinformation. A 2011 survey of anorexia on YouTube found that 41 out of 140 videos were pro-anorexia. In 2015, researchers examined 200 videos about asthma and found that            38 percent promoted unsupported alternative treatments, including acupuncture and ingestion of live fish.
YouTube has taken at least some action against bogus health information. In early 2019, for example, the company demonetized anti-vaccination videos by removing ads, and            changed its recommendation algorithms to fight other            conspiracy theories. According to a YouTube spokesperson, videos like these, which the company considers "borderline content," are now watched 70 percent less often.
Allgaier remains unconvinced: "I mean, this is how YouTube works - to get as much traffic as possible."
Two converging trends further complicate the picture. First, thanks in part to advances in video production, it is now easier than ever to make and upload videos. "The credentials for traditional edutainment programming matched more closely the credentials for being a traditional educator or scholar," says Carlson. Today, anyone with the skills to be popular on YouTube can take that place. While the change isn't necessarily bad, Carlson adds, it opens the door to abuse. Second, YouTube is increasingly mainstream, with more of its biggest stars being backed by professional teams and organizations. In theory, this should add a level of quality control. In reality, the results aren't so clear.
Short, simple, and flashy is the recipe for success on YouTube, according to the user Coffee Break, a YouTuber who has produced videos critical of pop science. (In a phone interview with Undark, Coffee Break would only give his first name as Stephen, declining to provide more personally identifying information because he says his work can put him at legal risk.)
The recipe is also a problem for edutainment. "That isn't really the way people really learn. You can't learn TensorFlow in five minutes," he says, referring to Raval's videos. "Everyone would like to learn TensorFlow in five minutes. By promising that, you can gain an audience." Successful channels spawn imitators, and the result can be a race to the bottom: TensorFlow in four minutes, then TensorFlow in three minutes. The aim, Stephen says, is often to make people "feel smart."
Many of these videos fall into what Carlson calls "infotainment," where the goal isn't to teach, but to pass on information, even if it doesn't lead to understanding. "They're not saying, 'Here's how you would do the problem yourself,'" Stephen agrees. "They're just handing you a bunch of microwaved facts."
Exceptions are channels like Khan Academy, which creates educational videos on topics from geopolitics to geoscience, with step-by-step instruction - and, critically, on the basis of building up knowledge over time. Unlike many channels, Khan Academy doesn't promise each video will be accessible to everyone. And in 2019, YouTube launched "           learning playlists," which cover topics like            anatomy and physiology and move from beginner to advanced levels. Currently, the playlists also hide recommendations in an effort to keep viewers focused on learning.
But creating prerequisites and limitations for viewers may be a tough sell. Many popular videos promise to provide deep knowledge about complex topics like quantum mechanics and genetic engineering technology like Crispr in a few minutes. "To be charitable, a lot of them probably wish they could go into more detail," Stephen says. "It feels like if you want to really do your homework, and you really want to put a great video together, you're on the wrong platform."
YouTube isn't the only medium to offer edutainment. Before the platform's first video was uploaded in 2005 - an 18-second clip of one of the site's founders at the zoo, which now has more than 80 million views - television was responsible for the vast majority of the genre.
David Attenborough's "Life on Earth," Carl Sagan's "Cosmos," and kid-themed fare like "The Magic School Bus" captured generations of viewers with explorations of the universe and all the knowledge that science had to offer about it.
There were relatively few broadcasters, and in their educational programming they enforced an ethos that generally prized accuracy over entertainment, according to Carlson. But over the years that crumbled. From misleading depictions of sharks as lethal human-hunting machines during Discovery's Shark Week to pseudoscientific documentaries like History's Ancient Aliens, the scientific quality of the content declined.
Worries about a lack of gatekeeping fueled Andrew Keen's 2007 book "The Cult of the Amateur," one of the earliest broadsides leveled against user-generated content, from blogs to YouTube to Wikipedia. Keen blamed these upstarts for destroying professionally made media like the Encyclopedia Britannica and newspapers. Some aspects of the argument have aged better than others. While Wikipedia did mostly supplant Britannica and other professional encyclopedias,            studies have long suggested the crowd-sourced version is just as accurate. And since YouTubing is no longer only an activity for amateurs, there have been some improvements in style and substance.
"It would be too easy to say, 'YouTube is the key area where all the bad stuff happens and everything else is really nice and good,'" says Allgaier.
YouTube creators are experimenting with ways to improve educational videos. Last year, the YouTube channel Kurzgesagt, which explains scientific topics to its 10 million subscribers in short animated videos, published an unusual video titled "Can You Trust Kurzgesagt Videos?" This meta-video explains Kurzgesagt's relatively new process of contacting multiple experts and soliciting feedback. In years past, Kurzgesagt videos didn't go through this filter, and in the video about their new process the Munich-based team announced they were removing two videos, which they said no longer met their standards for quality.
It's an approach that's not so different from what traditional science publications are doing to build trust. For instance, in October 2019, Science News released a guide explaining their standards for journalism, answering similar questions about how they find and evaluate expert sources.
Another recent development on YouTube has been the use of the description box, located directly below the video. Typically used for copyright disclaimers or links to other social media accounts, many YouTubers have begun to treat it as a bibliography for citations. The lists are frequently comprehensive. When the popular German YouTuber Rezo posted an hour-long takedown of the German government's inaction on climate change, he prepared            a 13-page document with hundreds of references to academic papers, videos, websites, and popular science articles. Even the absurdist "           True Facts" nature documentary spoof has gotten serious about verifying its facts with scientists and providing sources in the description box.
It's unclear whether user-led sourcing efforts will have a large effect. Before his recent misconduct, Raval presented himself as an expert in data science by using others' code. In one of his apologies, he said that "All I did was was reupload someone else's code. It's not my code. The reason I did that is just selfishness and ego." Raval then went on to read a list of developers he'd failed to attribute code to.
Still, multiple experts Undark spoke with agreed that the efforts to clearly attribute credit were promising. "How the YouTubers deal with sources is actually more transparent," says Allgaier. "Journalists could learn from this as well."
Dan Garisto is a science journalist based in New York. He writes about physics and has been published in outlets including Scientific American, Symmetry, Science News, Hakai, and Nature News.
This article was originally published on Undark. Read the original article.

Load-Date: April 13, 2020


End of Document




The census goes digital - 3 things to know
Salon.com
February 29, 2020 Saturday


Copyright 2020 Salon.com, LLC. All Rights Reserved
Length: 982 words
Highlight: Collecting census data online creates new risks to the accuracy and integrity of the information


Body


Link to Image
Getty/liveslow
The U.S. Census Bureau is hoping that most people who live in the U.S. will use the internet to answer census questions, rather than filling out a paper form or providing those answers to a census taker in person, at their home.
That would be cheaper - a plus for a budget-strapped           Census Bureau - and could help ensure maximum turnout and accuracy of the count. For instance, databases could keep track of which homes have not yet responded to the survey, allowing census officials to target mailings and in-person visits to those locations, without needing to spend time chasing households that have already responded.
However, as some of my own work on digital platforms and electronic commerce shows, collecting data online carries some significant risks that are new to the census and may undermine the accuracy of the count and the public's trust in the process.
Cybersecurity risks
If everyone responds digitally, the census online system will have to handle nearly 130 million responses - one for each household in the country. Many of them may be using computers or smartphones that have been hacked or have            malicious software installed.
One potential problem this raises is that someone trying to respond to the census may find themself instead submitting their information to some other group, one that seeks to illegitimately harvest their personal data for profit.
Another possibility is that a person might be submitting their information to the actual census website, but the software running secretly on their computer could modify the data before it's recorded. That could result in inaccurate reporting - making it seem like more people live in a home than actually do, or fewer.
Because census data is used to determine congressional representation and            calculate who gets how much federal money, those changes could affect a community's political power and government services.
Bridging the digital divide
Of course, not everyone will complete their census survey online. In addition to people who don't have computers and smartphones, many homes aren't connected to the internet. Even in New York City, what appears to be a pinnacle of an interconnected urban area, about 29% of households don't have high-speed internet access.
To reach those people, and those in more suburban and rural areas who also don't have internet access, the Census Bureau will need to rely on phone and mail responses, along with the traditional method of visits by door-to-door census takers.
The data collection effort underway for the 2020 U.S. Census may end up disadvantaging the households without access to broadband internet access. Groups that are more likely to use the internet on their mobile phones - as opposed to a computer - may find it too hard to use their phones to respond to the online questionnaire. That could end up disproportionately reducing the response from African Americans, Latinos,            younger adults, low-income earners and people without a high school diploma.
The U.S. Census Bureau is aware of those concerns and is working to identify communities where a            lower online response is likely. The agency says it will            send paper questionnaires and even human census takers to households in those areas            at particular risk.
There is an opportunity for civic technology and citizen data science to help address people's difficulties using online surveys, too. For instance, the            Hard to Count map tracks households with poor internet access, and neighborhoods that are home to racial or ethnic minorities and people with lower income or education levels.            Nonprofit organizations and community groups are using the map to target efforts to encourage people to participate in the census.
Privacy concerns
Since the early days of the census, privacy has been a concern. In the 1850 census, the U.S. marshals assigned to collect data were instructed to consider all the responses to be confidential. By 1880, census workers - now trained survey-takers rather than law enforcement workers - were subject to            fines for violating their oaths of secrecy.
Over the decades, the Census Bureau has updated standards to keep up with changes in technology and societal expectations about privacy protection. The most recent set of concerns involves the potential for people to use computers to            match up census data with other data available publicly online. The U.S. Census Bureau's researchers found they could combine the 2010 census results with the contents of commercial databases and determine the            real identities of 52 million Americans. That could reveal private information, and violates the Census Bureau's obligation to protect respondents' identities.
In an attempt to prevent that from happening with the results of the 2020 census, the Census Bureau has adopted a statistical method called "differential privacy" in hopes of            obscuring sensitive personal information. The mathematics underlying technique are complicated, but in general the idea is that state-level counts will be accurate, but more detailed measurements - of populations of counties, towns and neighborhoods - will be altered to avoid revealing specific data that could be used to identify actual people.
However, researchers have voiced concerns that the data may not accurately represent the nation's population, and that            more specific details about the numbers of residents of states and towns may be misleading. Critics fear the effort to protect Americans' privacy may end up complicating planning that factors in population numbers, like disaster preparedness efforts.
Anjana Susarla, Associate Professor of Information Systems, Michigan State University
This article is republished from The Conversation under a Creative Commons license.

Load-Date: April 13, 2020


End of Document




200 years before Orwell, a German naturalist prophesied surveillance capitalism
Salon.com
September 29, 2019 Sunday


Copyright 2019 Salon.com, LLC. All Rights Reserved
Length: 550 words
Highlight: The current mania for metrics - from step-counting to Facebook likes - was foreshadowed by Alexander von Humboldt


Body


Link to Image
A hill whose elevation remains unknown is an insult to human reason. This statement, attributed to German naturalist and explorer Alexander von Humboldt (1769 - 1859), perfectly embodies the Enlightenment's thirst for knowledge, which led to a flourishing of the natural sciences in the nineteenth century. Some one hundred years later, in 1907, German sociologist Georg Simmel praised secrecy as "one of the greatest of mankind's intellectual achievements." Such words from the mouth of a sociologist are remarkable since it was his own discipline that sought to reveal the secrets of society. But perhaps Simmel was thinking of his colleagues in psychology and psychoanalysis, who were already setting about measuring the human soul.
A century further on, we today speak of the "metric I" and "transparent human beings." The virus carried by Humboldt's insistence of           measuring things has spread to all areas of social life, conceptually combining sociology with natural science to the extent that the new branch of research into measuring society is called "social physics." Digitalization has made us incapable of doing almost anything without            leaving behind a digital trail. Every Internet search is a statement of our interests, and every photo of a hill we post to our social networks reveals where we are. As early as 2010, Google boasted that it not only knew where we are and where we've been, but also what we think. And recent research suggests that 150 likes given by any person online says more about him or her than is known by their parents - 300 likes, more than is known by even his partner.
But we are mistaken if we immediately think of George Orwell. The better association would be Humboldt - since we are not the victims of, but rather the driving force behind civilization. We post updates on our lives, we publish likes  - we even measure our steps, our food and our sleep. The corresponding fan group is called "Quantified Self," and its slogan is "self-knowledge through numbers."
Of course, we don't do all of this by ourselves or for our own benefit. There is no shortage of service providers whose apps drive the measuring of social life forward. Five years ago, taxi service Uber drew up maps of one-night stands showing where rides had been taken to and from a given address on weekends between 10 PM and 4AM.
In this regard, it's perfectly appropriate to recall Orwell, but we shouldn't forget Humboldt as we do. The Uber example is less about surveillance in service of discipline as measuring as an end...to well, to what purpose? Even Uber itself couldn't say precisely. The company talked about a little "analytic game." But it could just as well have said that data that is collected but not collated is an insult to human reason.
Have human beings become victims of their own reason - and their capacity for satisfying it in almost every way? September 28 is the 101st anniversary of Simmel's death. That alone is reason enough to juxtapose his paean to secrecy with Humboldt's praise of measurement. Sometimes it's simply irrational to want to know everything. Diplomats and parents already realize this, as do successful husbands and wives, whereas it is a lesson the fanatic adherents of knowledge in data science still have to learn.

Load-Date: April 13, 2020


End of Document




Facebook CEO Mark Zuckerberg made staff recommendations to Pete Buttigieg's presidential campaign
Salon.com
October 21, 2019 Monday


Copyright 2019 Salon.com, LLC. All Rights Reserved
Length: 521 words
Highlight: And two of the people recommended by Zuckerberg and his wife, Priscilla Chan, were hired


Body


Link to Image
Facebook CEO Mark Zuckerberg and his wife Priscilla Chan sent Pete Buttigieg's 2020 presidential campaign staff recommendations earlier this year, Bloomberg News reported Monday.
The news of the recommandations come as Facebook faces mounting bipartisan scrutiny on issues including bias, election meddling, fake accounts, misinformation and voter suppression.
Zuckerberg reportedly sent multiple emails to Mike Schmuhl, Buttigieg's campaign manager, with names of individuals whom he thought would be a good fit for the team, campign spokesman Chris Meager confirmed to the news outlet. Chan also emailed various recommendations to Schmuhl, according to Meager.
Ultimately, two of the recommended people were hired. The campaign staffers are Eric Mayefsky, senior digital analytics adviser, and Nina Wornhoff, organizing data manager, according to the news outlet.
Mayefsky previously worked as the director of data science at Quora, a 10-year-old question-and-answer startup founded by former Facebook employees. Before that, he worked as a product manager at Facebook for more than three years, beginning in 2010, his LinkedIn profile shows.
Wornhoff previously worked as a machine learning engineer at the Chan Zuckerberg Initiative and in Democratic politics in Indiana, where Buttigieg is based, Bloomberg reported.
"From the CNN town hall in March to our launch a month later, we literally got 7,000 resumes," Meagher told the outlet. "I think that [Zuckerberg] thought Eric would be a good staff hire with a lot of experience and same with Nina and Priscilla."
Ben LaBolt, a spokesman for the Zuckerberg-Chan family, told Bloomberg that the recommended employees had asked Zuckerberg and Chan to pass along their names to the campaign.
"Having seen Mark's visit to South Bend in 2017 and Facebook Live with Mayor Buttigieg, colleagues later asked Mark and Priscilla to connect them with the Buttigieg campaign as they were interested in joining," LaBolt said.
He noted that Zuckerberg and Chan have not decided whom they will support for president.
It remains unclear if the pair made similar staff recommendations for others seeking to work on a presidential campaign.
Zuckerberg, 35, and Buttigieg, 37, attended Harvard University at the same time, and Buttigieg was friends with two of Zuckerberg's roommates, according to Bloomberg. The South Bend mayor was also among the first 300 people to register for a Facebook account. Zuckerberg and Buttigieg, however, were only introduced years later by a mutual college friend.
The Facebook CEO is set to testify Wednesday before the House Financial Services Committee in a hearing that will likely focus on the social media company's plans to launch a new digital currency called Libra.
Meanwhile, Buttigieg has surged into third place in Iowa behind former Vice President Joe Biden and Massachusetts Sen. Elizabeth Warren, according to a new poll released Monday. The Suffolk University/USA Today poll reveals that 13 percent of likely Democratic voters in the first-in-the-nation caucus state support Buttigieg, who trails Biden at 18 percent and Warren at 17 percent.

Load-Date: April 13, 2020


End of Document




Film Diversity Helps Drive Box Office Hits, Study Shows
The Hollywood Reporter
October 24, 2019 Thursday


Copyright 2019 Prometheus Global Media, LLC All Rights Reserved


Section: NEWS; TAG
Length: 618 words
Byline: Evan Real
Highlight: Considering cinematic hits like 'Us,' 'Crazy Rich Asians' and 'Black Panther,' Movio is examining how theatrical audience composition correlates to increased representation onscreen.


Body


Movio, a film industry marketing and data firm, on Thursday released a new study called "The Diversity Demand: Securing the Future of Moviegoing." In the white paper, Movio examines how theatrical audience composition correlates to increased representation in film - and if that suggests an opportunity to drive success at the box office by featuring more diversity onscreen.
The analysis draws a correlation between a minority group's representation onscreen and that group's audience turnout, with some groups attending in numbers at more than twice the usual rate. The research also shows that increased representation of minority groups onscreen can influence less engaged moviegoers within that demographic to come out to the theater.
As a cursory analysis, Movio chose several pairs of theatrically released films which would generally be considered "similar" in terms of both genre and budget. For each pair, one film featured an underrepresented lead or co-lead and the other did not. The audience composition of each title was then analyzed by Movio's data science team.
For example, Movio found that the audience for Pixar's animated feature  Coco was nearly 75 percent more Latinx than the audience for Pixar's other hit            Incredibles 2 ; and Jordan Peele's horror pic             Us brought out an audience that was nearly 100 percent more black than the audience that attended John Krasinski's similarly scary            A Quiet Place. However, Us' diverse cast  including stars Lupita Nyong'o and Winston Duke  also drew large numbers of non-black moviegoers and was widely successful at the box office, making eight times its production budget.
When analyzing a trio of similar romantic comedies, Movio's research found that Crazy Rich Asians (starring Constance Wu and Henry Golding) attracted an audience that was 186 percent more Asian and            What Men Want (Taraji P. Henson) attracted an audience that was 296 percent more black than the audience that attended            Isnt It Romantic (Rebel Wilson).
And in comparing the DC films Wonder Woman  (Gal Gadot) and            Aquaman (Jason Momoa), Movio's study shows that the female-fronted title did not significantly alter the demographic profile that typically makes its way to the theater for superhero flicks. Both titles attracted an audience that was 40-41 percent female. Despite Hollywood's long-standing skepticism about the potential success of a female-led superhero pic, the "absence of underperformance " (in this case, male moviegoers) seems to be just as meaningful as a film's overperformance with another.
In compiling the section of the study that analyzes mega blockbusters, Movio found that Black Panther , with a predominantly black cast, attracted an audience that was 38 percent more black than            Avengers: Infinity War , which features mostly white actors. This number is significant considering that, according to Movio, 40.7 percent of all black U.S. moviegoers attended Black Panther.
In a statement, Craig Jones, chief commercial officer and president of Movio Media, explained why understanding how to attract diverse audiences will soon be paramount to the success of filmmakers and marketers.
"Today's consumer has countless entertainment options, making it easier than ever for diverse audiences to find content that speaks to their tastes and experiences," said Jones, who will discuss Movio's findings on Tuesday during a New York Film Conference panel moderated by The Hollywood Reporter. "If cinema is to remain relevant and continue having a cultural impact, it must attract these audiences by delivering more representative content."
Read Movio's complete study here.
Link to Image

Load-Date: October 27, 2019


End of Document




Roku to Acquire Video Ad Platform dataxu for $150 Million
The Hollywood Reporter
October 22, 2019 Tuesday


Copyright 2019 Prometheus Global Media, LLC All Rights Reserved


Section: NEWS; TAG
Length: 321 words
Byline: Georg Szalai
Highlight: "TV advertising is shifting toward OTT and a data-driven model focused on business outcomes for brands," says Roku CEO Anthony Wood.


Body


Streaming media firm Roku said Tuesday that it has agreed to acquire Boston-based dataxu, a video advertising platform that enables marketers to plan and buy campaigns, for $150 million in cash and stock.
"TV advertising is shifting toward OTT and a data-driven model focused on business outcomes for brands," said Anthony Wood, CEO of Roku. "The acquisition of dataxu will accelerate our ad platform while also helping our content partners monetize their inventory even more effectively."
The acquisition "will complement Roku's industry-leading OTT advertising platform and enable Roku to provide marketers a single, data-driven software solution to plan, buy, and optimize their ad spend across TV and OTT providers," the company said. "dataxu brings an experienced team - including strong talent in software engineering, data science and analytics - to work with new and existing marketers on Roku's proven advertising platform."
Roku streams more ad-supported hours than any other streaming platform, according to a June comScore analysis. With more than 30.5 million active accounts as of June 30, the firm has been looking to grow its offerings of OTT ad services.
Dataxu provides marketers with an automated bidding and self-serve software solution to manage their ad campaigns programmatically across various digital platforms. It also advanced TV and OTT media planning tools.
"Advertisers today spend more than $70 billion on traditional TV. According to Magna Global, OTT accounts for 29 percent of TV viewing, but so far has only captured 3 percent of TV ad budgets," Roku said in its deal announcement. "As viewers continue to migrate to streaming, automated media buying solutions are expected to unlock more advertising investment into OTT."
The acquisition has been approved by both companies' boards and is expected to close later this quarter, subject to such closing conditions as regulatory approvals.
 
Link to Image

Load-Date: October 22, 2019


End of Document




CBS and Viacom Chiefs' Memos to Staff Hint at Changes Ahead
The Hollywood Reporter
August 13, 2019 Tuesday


Copyright 2019 Prometheus Global Media, LLC All Rights Reserved


Section: NEWS; TAG
Length: 1802 words
Byline: Alex Weprin
Highlight: Joseph Ianniello tells employees merger "will accelerate our global ambitions," while Bob Bakish says the combined company will "shape the future of the entertainment industry."


Body


In emails to their respective employees sent after the Viacom-CBS merger announcement, CBS CEO Joe Ianniello and Viacom CEO Bob Bakish touted the potential of the combined companies, while acknowledging that the move will result in staff changes.
Bob Bakish will be president and CEO of the combined company, to be called ViacomCBS, while Ianniello will be chairman and CEO of CBS after the merger is completed, reporting to Bakish. Viacom's brands include MTV Networks and Paramount Pictures, while CBS also owns CBS TV Studios and Showtime.
"As we all know, there is a race to create more of the best content. We are already leaders in this regard, and today's news will accelerate our global ambitions," Ianniello wrote in his memo to CBS staff.
"This merger comes with a lot of expectation, but it also comes with what I believe is a rare and exciting opportunity," Bakish wrote in his memo to Viacom employees. "Together, we have the opportunity to be one of the few companies positioned to shape the future of the entertainment industry."
The two CEOs also acknowledged that the combined entity will result in staff changes. The companies say they expect to have annual run-rate synergies of $500 million, with some of that coming from layoffs. 
"[T]he vast majority of you will continue to excel in your current roles. And while all of us will experience some change, including new challenges and opportunities, I want you to know that each and every one of you is a crucial component of our success, and we value your contributions every day," Ianniello wrote in his email. "It's also important to remember that the process of combining our two companies won't happen overnight, and the closing of this merger is likely several months away. I know that many of you will have questions, and they will be addressed in the upcoming weeks and months."
Bakish, meanwhile, noted in his memo that Viacom CFO Wade Davis will be leaving the company, as CBS CFO Christina Spade will become CFO of ViacomCBS.
"Combining our companies will be a joint effort," Bakish added. "Over time, we expect there will be opportunities to bring our teams together with our peers at CBS, so we can begin identifying ways to work together and learn from each other."
You can read the full memos from Ianniello and Bakish below.
Memo from CBS CEO Joe Ianniello:
Dear Colleagues -
CBS is entering a new era today, announcing a merger with Viacom that bolsters our premium content portfolio and positions us for an even better future. As we all know, there is a race to create more of the best content. We are already leaders in this regard, and today's news will accelerate our global ambitions.
We are merging at a time when the possibilities for premium content companies are greater than ever. Viacom owns terrific brands - Paramount, Nickelodeon, BET, MTV, Comedy Central and many others - that will complement ours and offer innovative ways to reach a whole new set of viewers. You can read more about the benefits of the deal in this press release.
When we finalize the merger, Viacom CEO Bob Bakish will become the President and CEO of the combined company, and I will become the Chairman and CEO of CBS. Bob and I will ensure a smooth and steady integration of our two great companies.
I am proud that I will continue to lead CBS, responsible for overseeing all of our CBS-branded businesses. I love CBS like you do, and I'm pleased to remain a steward of it along with our great team.
The new ViacomCBS will include corporate representation from both management teams, including our Chief Financial Officer, Chris Spade, who will become ViacomCBS' CFO.
Together, we will build upon our success in Entertainment, News and Sports with the #1 television network, a prolific studio that produces more and more hit programming for outlets across the industry, an interactive division that operates CBS All Access and is a top-10 digital property in the U.S., a syndication arm that has eight of the top 10 first-run shows on television, and a local media business that has 28 television stations in the country's major markets. As for our esteemed colleagues at Simon & Schuster and Showtime, your divisions will report to Bob when the deal closes, working under the inspired leadership of Carolyn Reidy and David Nevins - who will also continue to work with me overseeing entertainment programming for CBS, CBS TV Studios and CBS All Access.
CBS was founded on the principle of great content. This is something that will not change. In fact, our announcement today is structured in part to make certain that our tradition of excellence will remain an integral part of our future. And so in my role I'll be "keeping my Eye on the Eye"...working with you to ensure that the rich heritage of CBS remains central to everything we do...and that CBS continues to thrive as it has magnificently for more than 90 years.
Of course, our best and strongest asset is our people, and we will continue to prioritize investing in you. We've accomplished so much on that front recently, and I assure you that our ongoing commitment to a positive, diverse and inclusive workplace will remain a key priority.
It's important to note that the vast majority of you will continue to excel in your current roles. And while all of us will experience some change, including new challenges and opportunities, I want you to know that each and every one of you is a crucial component of our success, and we value your contributions every day.
It's also important to remember that the process of combining our two companies won't happen overnight, and the closing of this merger is likely several months away. I know that many of you will have questions, and they will be addressed in the upcoming weeks and months.
In the meantime, you should all have confidence that today's announcement will put us in a decidedly better competitive position to succeed in the years ahead. As always, you have my appreciation and respect for the creativity, dedication and loyalty that you bring to your job every day. You are CBS.
Joe
Memo from Viacom CEO Bob Bakish:
Team -
I'm writing today to share some big news. We just announced an agreement to merge with CBS, bringing together our two great companies to create a leading global, multiplatform, premium content powerhouse.
This merger comes with a lot of expectation, but it also comes with what I believe is a rare and exciting opportunity. Together, we have the opportunity to be one of the few companies positioned to shape the future of the entertainment industry.
With this agreement, we bring together the most storied studio in Hollywood, a portfolio of brands that have shaped culture for nearly four decades, a broadcast powerhouse rightfully called "The Tiffany Network," a major force in consumer publishing with Simon & Schuster, and Showtime, a premium brand that consistently pushes the boundaries of storytelling. Between us, we also boast one of the most innovative, diversified collections of digital assets in the industry. Make no mistake, together we aren't just bigger - we are much, much better.
Our combined company - which will be called ViacomCBS - will have a library of content with incredible breadth and depth, and a reinforced capability to produce premium and popular content at scale. Well have greater reach, strengthening our position with advertising and distribution partners. We will have an extended portfolio of direct-to-consumer products - both ad-supported and subscription-based - that will accelerate our growth. And we'll be able to build on our leadership positions in the US, UK, Australia, Argentina and India for continued global expansion.
Very importantly to me, CBS and Viacom are also a great fit. The CBS team is incredibly talented, with distinct expertise that has propelled the company's continued leadership in broadcast, D2C and beyond. And, both of our companies share a passion for creating premium content and a commitment to innovating through a rapidly shifting media landscape. I'm honored to say that I will be leading the combined company as President and CEO. Christa D'Alimonte will serve as EVP, General Counsel and Secretary. Christina Spade, who is currently EVP and CFO of CBS, will serve as EVP and CFO of the combined company. Joe Ianniello - currently President and Acting CEO of CBS - will serve as Chairman and CEO of CBS, overseeing the CBS-branded assets.
Combining our companies will be a joint effort. Over time, we expect there will be opportunities to bring our teams together with our peers at CBS, so we can begin identifying ways to work together and learn from each other.
In short, I'm very excited to begin working with CBS. Together, we can better maximize our business today, while ensuring we lead the industry tomorrow.
This is a big step forward for all of us, and I'm so grateful for all you've done to get us here. Despite changes in our company and the industry - as well as the continued speculation of a potential merger - you've focused. You've executed. And, you've delivered, which makes us a much stronger company than we were just a few years ago. I'm hoping you can maintain similar focus and determination in the months ahead as we work to close the transaction.
One change we already know will happen is that Wade Davis will depart in connection with the closing of the transaction, as we've determined there isn't a senior operating role at the corporate level of the merged company that would be consistent with the full breadth of his experience, expertise and the scope of his current role at Viacom.
Wade has been one of Viacom's most vocal and passionate champions, playing a critical role for the company in helping to develop and successfully execute our strategy to evolve Viacom for the future. In addition to managing our global financial functions, corporate development, investor relations, data science and technology services, over the past two years he has spearheaded important strategic growth initiatives, including most recently the creation of Advanced Marketing Solutions, which was the engine that retuned us to Domestic Ad Sales growth, as well as the acquisition, integration and management of Pluto TV, which helped us establish a leadership position in the DTC marketplace. We're so grateful for his many contributions, and that he'll be with us through the closing of the deal.
Throughout the process ahead, I promise to be as accessible and transparent as I can, starting with a Bob Live tomorrow, where I'll give you an overview of the merger and answer any questions you may have.
I can't wait to speak with you then. In the meantime, please check out the press release below for further details, and thank you again for all your hard work in getting us to this very important milestone.
Best, Bob
Link to Image

Load-Date: August 13, 2019


End of Document




Researchers Identifies Genes that Increase Autism Risk
City News Service
August 8, 2019 Thursday 9:29 AM PST


Copyright 2019 City News Service, Inc. All Rights Reserved
No City News Service material may be republished without the express written permission of the City News Service, Inc.
Length: 555 words
Body


LOS ANGELES (CNS) - A UCLA-led research team has identified dozens of genes, including 16 new genes, that increase the risk of autism spectrum disorder, according to a UCLA study published today.
The findings, published in the journal Cell, were based on a study of families with at least two children with autism, UCLA said in a statement.
Researchers from UCLA, Stanford University and three other institutions used a technique called whole genome sequencing to map the DNA of 2,300 people from nearly 500 families, it said. They found 69 genes that increase the risk for autism spectrum disorder. Sixteen of those genes were not previously suspected to be associated with a risk for autism.
Researchers also identified several hundred genes they suspect may increase the risk of autism based on their proximity to genes that were previously identified to carry an increased risk. The study further revealed several new biological pathways that had not previously been identified in studies of autism.
The findings highlight the importance of learning how genetic variants or mutations -- the differences that make each person's genome unique -- are passed from parents to children affected with autism, said the study's co-lead author, Elizabeth Ruzzo, a UCLA postdoctoral scholar. Former UCLA postdoctoral scholar Laura Perez-Cano is the study's other co-lead author.
"When we look at parents of autistic children and compare them to individuals without autism, we find that those parents carry significantly more, rare and highly damaging gene variants," Ruzzo said. "Interestingly, these variants are frequently passed from the parents to all of the affected children but none of the unaffected children, which tells us that they are significantly increasing the risk of autism."
Of the children in the study, 960 have autism and 217 children do not. That enabled researchers to analyze the genetic differences between children with and without autism across different families.
"Studying families with multiple children affected with autism increased our ability to detect inherited mutations in autism spectrum disorder," said Dr. Daniel Geschwind, a senior author of the study and the Gordon and Virginia MacDonald Distinguished Professor of Human Genetics, Neurology and Psychiatry at the David Geffen School of Medicine at UCLA.
"We show a substantial difference between the types of mutations that occur in different types of families, such as those that have more than one affected child versus those having only one child with ASD," said Geschwind, who also is director of the UCLA Center for Autism Research and Treatment and director of the Institute of Precision Health at UCLA.
The research also found that the 16 genes newly determined to be associated with an increased risk for autism form a network with previously identified genes that are associated with a risk for autism spectrum disorder. The way they interact with one another further heightens the risk, said Dennis Wall, the study's co-senior author, a Stanford University School of Medicine associate professor of pediatrics and of biomedical data science.
"They associate with each other more tightly than we'd expect by chance," he said. "These genes are talking to each other, and those interactions appear to be an important link to autism spectrum disorder."

Load-Date: August 9, 2019


End of Document




You'd be better off lighting your money on fire than giving it to a politician to spend on TV ads
Salon.com
September 1, 2019 Sunday


Copyright 2019 Salon.com, LLC. All Rights Reserved
Length: 800 words
Highlight: Hillary Clinton may have lost to Donald Trump because she bought the wrong kind of ads


Body


Link to Image
Alright, you want to make this country a better place for yourself, your children and the many generations to come. So you make a donation to a political candidate you believe will fight for a better country.
But, in reality, you are wasting your money. Here's why.
Television has long been the golden goose of political advertising. The conventional wisdom is that the candidate who can spend the most on it will most likely win.
With the exception of Donald Trump, almost every person elected president since 1960 has raised and spent more money than their opponent. That includes Reagan, George H.W. Bush, Clinton and Obama - with a significant amount of that money being used to buy expensive television advertisements.
In 2016, Hillary Clinton raised over US$1.1 billion, as opposed to Trump's grand total of less than $650 million. She outspent Trump almost            three times over on television advertising.
So how is it that a presidential candidate won with less money raised and spent?
Spending where it counts
Some have attributed this to free media Trump received from television networks hungry for ratings. But, in many ways, that argument doesn't hold water, so consider a different answer: digital advertising.
While he was outspent on TV, Trump spent four times the amount Hillary Clinton did on digital ads, which are any ad on a computer rather than the typical campaign ads on TV, mail or billboards.
Why would this be the answer?
As of 2016, a new era of politics has been established (arguably initiated by Obama in 2008), dominated by digital advertising. And no one has done it better than Donald Trump.
The wasted dollars of TV advertising
A typical House candidate will spend 65% to 70% of their entire political budget on TV and U.S. mail advertising.
When one of them advertises on TV, almost 80% of the money spent on the ads is spent broadcasting those ads to people who don't vote or live in that candidate's district. That's because TV does not allow you to target your audience to the same precise level as digital can. This is true from major metro TV markets to rural states.
So if you give to a political campaign, then over 50% of your money is
 being spent on TV ads that do not reach people who can vote for your candidate.
What's more, if you take into account what is spent on further advertising, it turns out that for every dollar you give, only 10 cents actually goes to engaging voters.
In effect, television advertising is the worst thing you can support in terms of impact for your money.
But, if you give to campaigns, both district-level and presidential, that advertise digitally, it is an entirely different story.
Digital advertising targets better
When politicians advertise digitally, their advertising can get smarter and more targeted. That's because the digital advertising acquires more information on individuals and better learns what policies and causes the donor cares about.
For example, much of Donald Trump's current Facebook advertising doesn't even ask for money, it asks for information about you, such as which issues you are interested in and whether you favor building 'The Wall.' Here's a screenshot:
Link to Image
Screenshot from Trump campaign Facebook advertisement.
Facebook
And here are screenshots from a campaign website that the Facebook ad takes you to, which includes an "Official Secure The Border Survey."
Link to Image
Link to Image
Screenshots, Trump campaign website.
Facebook
Trump's digital ads not only ask for your opinion on a variety of topics, they also assign you a survey number and ask for all the data necessary (name, email, ZIP code, phone number) to target you individually for future voting and fund raising.
This is even more valuable than the advertisement itself, because individuals can continually be targeted on topics they specifically care about.
Trump spent 44% of his massive 2016 election media budget on digital advertising. Commercial companies spend 54% of their advertising budgets on digital advertising. But U.S. Senate campaigns only spent 4% to 7% on digital advertising in 2016.
Who do you think is spending more money on figuring out how people are responding to different forms of advertisement?
Now that he's campaigning for re-election, President Trump is currently running thousands of ads per day on Facebook alone. That's consistently more than the 23 Democratic candidates challenging Trump combined.
If this trend continues into the general election, it is pretty clear to me who most likely will win.
It seems that the winners will be those who use digital wisely - the losers will be the ones who stick with TV. 
Liberty Vittert, Professor of the Practice of Data Science, Washington University in St Louis
This article is republished from The Conversation under a Creative Commons license.

Load-Date: April 13, 2020


End of Document





 
Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. 
California HealthlineWalmart Charts New Course By Steering Workers To High-Quality Imaging Centers
 California Healthline
May 15, 2019


Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. 


Byline: Phil Galewitz
Body

Walmart Inc., the nation's largest private employer, is worried that too many of its workers are having health conditions misdiagnosed, leading to unnecessary surgery and wasted health spending.
 
The issue crystallized for Walmart officials when they discovered about half of the company's workers who went to the Mayo Clinic and other specialized hospitals for back surgery in the past few years turned out not to need those operations. They were either misdiagnosed by their doctor or needed only non-surgical treatment.
 
A key issue: Their diagnostic imaging, such as CT scans and MRIs, had high error rates, said Lisa Woods, senior director of benefits design for Walmart.
 
So the company, whose health plans cover 1.1 million U.S. employees and dependents, has recommended since March that workers use one of 800 imaging centers identified as providing high-quality care. That list was developed for Walmart by Covera Health, a New York City-based health analytics company that uses data to help spot facilities likely to provide accurate imaging for a wide variety of conditions, from cancer to torn knee ligaments.
 
Although Walmart and other large employers in recent years have been steering workers to medical centers with proven track records for specific procedures such as transplants, the retail giant is believed to be the first to prod workers to use specific imaging providers based on diagnostic accuracy -- not price, said employer health experts.
 
"A quality MRI or CT scan can improve the accuracy of diagnoses early in the care journey, helping create the correct treatment plan with the best opportunity for recovery," said Woods. "The goal is to give associates the best chance to get better, and that starts with the right diagnosis."
 
Walmart employees are not required to use those 800 centers, but if they don't use one that is available near them, they will have to pay additional cost sharing. Company officials advise workers that they could have more accurate results if they opt for the specified centers.
 
Studies show a 3% to 5% error rate each workday in a typical radiology practice, but some academic research has found mistakes on advanced images such as CT scans and MRIs can reach up to 30% of diagnoses. Although not every mistake affects patient care, with millions of CT scans and MRIs done each year in the United States, such mistakes can have a significant impact.
 
"There's no question that there are a lot of errors that occur," said Dr. Vijay Rao, chairwoman of radiology at the Thomas Jefferson University Hospital in Philadelphia.
 
Errors at imaging centers can happen for many reasons, including the radiologist not devoting enough time to reading each image, Rao said. The average radiologist typically has only seconds to read each image, she said. "It's just a lot of data that crosses your eye and there is human fatigue, interruptions, and errors are bound to happen," she added.
 
Other pitfalls: the technician not positioning the patient correctly in the imaging machine or a radiologist not having sufficient expertise or experience, Rao said.
 
Employers and insurers typically do little to help patients identify which radiology practices provide the most accurate results. Instead, employers have been focused on the cost of imaging tests. Some employers or insurers require plan members to use free-standing outpatient centers rather than those based in hospitals, which tend to be more expensive.
 
Woods said Walmart found that deficiencies and variation in imaging services affected employees nationwide. "Unfortunately, it is all over the country. It's everywhere," she said.
 
Walmart's new imaging strategy is aligned with its efforts over the past decade to direct employees to select hospitals for high-cost health procedures. Since 2013, Walmart has been sending workers and their dependents to select hospitals across the country where it believes they can get better results for spine surgery, heart surgery, joint replacement, weight loss surgery, transplants and certain cancers.
 
As part of its "Centers of Excellence" program, the Bentonville, Ark.-based retail giant picks up the tab for the surgeries and all related travel expenses for patients on the company's health insurance plan, including a caregiver.
 
Sampling Imaging Centers' Work
 
Covera has collected information on thousands of hospital-based and outpatient imaging facilities starting with its previous business work in the workers' compensation field.
 
"Our primary interest is understanding which radiologist or radiology practices are achieving the highest level of diagnostic accuracy for their patients," said Dan Elgort, Covera's chief data science officer.
 
Covera has independent radiologists evaluate a sampling of patient care data on imaging centers to determine facilities' error rates. It uses statistical modeling along with information on each center's equipment, physicians and use of industry-accepted patient protocols to determine the facilities' rates of accuracy.
 
Covera expects to have about 1,500 imaging centers in the program by year's end, said CEO Ron Vianu.
 
There are about 4,000 outpatient imaging centers in the United States, not counting thousands of hospital-based facilities, he estimated.
 
As a condition for participating in the program, each of the imaging centers has agreed to routinely send a sampling of their patients' images and reports to Covera.
 
Vianu said studies have shown that radiologists frequently offer different diagnoses based on the same image taken during an MRI or CT scan. Among explanations are that some radiologists are better at analyzing certain types of images -- like those of the brain or bones -- and sometimes radiologists read images from exams they have less experience with, he said.
 
Vianu noted that most consumers give little thought to where to get an MRI or CT scan, and usually go where their doctors send them, the closest facility or, increasingly, the one that offers the lowest price. "Most people think of diagnostic imaging as a commodity, and that's a mistake," he said.
 
Rao applauded the effort by Walmart and Covera to identify imaging facilities likely to provide the most accurate reports. "I am sure centers that are worried about their quality will not be happy, but most quality operations would welcome something like this," she said.
 
Few Guides For Consumers
 
Consumers have little way to distinguish the quality of care from one imaging center to the next. The American College of Radiology has an accreditation program but does not evaluate diagnostic quality.
 
"We would love to have more robust  measurements" about the outcomes of patient care than what is currently available, said Dr. Geraldine McGinty, chair of the college's board of chancellors.
 
Facilities typically conduct peer reviews of their radiologists' patient reports, but there is no public reporting of such results, she said.
 
Covera officials said they have worked with Walmart for nearly two years to demonstrate they could improve the quality of diagnostic care its employees receive. Part of the process has included reviewing a sample of Walmart employees' health records to see where changes in imaging services could have caught potential problems.
 
Covera said the centers in its network were chosen based on quality, and price was not a factor.
 
In an effort to curtail unnecessary tests, Walmart, like many large employers and insurers, requires its insured members to get authorization before getting CT scans and MRIs.
 
"Walmart is on the leading edge of focusing on quality of diagnostic imaging," said Suzanne Delbanco, executive director of the Catalyst for Payment Reform, an employer-led health care think tank and advocacy group.
 
But Mark Stolper, executive vice president of Los Angeles-based RadNet, which owns 335 imaging centers nationally, questions how Covera has enough data to compare facilities. "This would be the first time," he said, "I have seen or heard of a company trying to narrow a network of imaging centers that is based on quality instead of price."
 
Woods said that even though the new imaging strategy is not based on financial concerns, it could pay dividends down the road.
 
"It's been demonstrated time and time again that high quality ends up being more economical in the long run because inappropriate care is avoided, and patients do better," she said.
 
This story was produced by Kaiser Health News, an editorially independent program of the Kaiser Family Foundation.
 
[Clarification: This story was updated on May 15 at 7:50 a.m. PT to make clear that Dr. Geraldine McGinty's comments were about measuring outcomes of patient care.]
 
Phil Galewitz: pgalewitz@kff.org, @philgalewitz  


End of Document




Garcetti Heads to New Jersey to Promote Nonprofit Partnership
City News Service
May 21, 2019 Tuesday 6:22 PM PST


Copyright 2019 City News Service, Inc. All Rights Reserved
No City News Service material may be republished without the express written permission of the City News Service, Inc.
Length: 225 words
Body


LOS ANGELES (CNS) - A non-profit organization Los Angeles Mayor Eric Garcetti helped establish will enter a partnership to invest in distressed communities across the nation, it was announced today.
The Mastercard Center, a philanthropic subsidiary of the credit card giant, along with Garcetti's Accelerator for America are partnering together to fund an $850,000 philanthropic grant from the Mastercard Impact Fund to help Accelerator assist 50 city leaders and their communities maximize the potential of the federal Opportunity Zone incentive.
The Mastercard Center said it will give "in-kind support" in the form of data science expertise, economic development tools and research to help city leaders make evidence-based decisions.
Garcetti made the announcement from Newark, New Jersey during the Forbes Opportunity Zones Summit. This was Garcetti's second trip in as many weeks out of state, following a nearly weeklong trip to Israel last week. He is set to return on Wednesday.
Yusef Robb, a political spokesperson for Garcetti, did not respond to questions regarding details of the nonprofit's funding, but said that Accelerator for America pays the mayor's expenses for when he speaks and travels at their events.
Accelerator for America is a nonprofit consortium of mayors, labor and business leaders, and urban and economic development experts.

Load-Date: May 22, 2019


End of Document





 
Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. 
California HealthlineExtreme Temperatures May Pose Risks To Some Mail-Order Meds
 California Healthline
January 10, 2019


Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. 


Byline: Alex Smith, KCUR
Body

Take a look at your prescription bottles. Most say "Store at room temperature" or "Keep refrigerated."
 
But what happens when drugs are delivered by mail? Were those instructions followed as the medicine wended its way from the pharmacy to your doorstep?
 
Those questions haunt Loretta Boesing, who lives in Park Hills, a small town in the hills of eastern Missouri, where the weather varies dramatically from season to season.
 
"It's crazy," Boesing said. "We sometimes experience temperatures like they would feel in Arizona. Sometimes we experience temperatures like they would feel up north."
 
In 2012, when son Wesley was 2 years old, he got so sick from the flu that he needed a liver transplant.
 
The transplant surgery went well, but just a few months later, lab tests showed Wesley's body appeared to be rejecting the organ.
 
Boesing felt both devastated and guilty.
 
"I feel the extra duty of not just protecting his life, but the life that lives on inside him," she said.
 
Wesley didn't lose his new liver, but during his weeks in the hospital, Boesing's mind raced, thinking about what might have gone wrong.
 
She remembered that when his anti-rejection medications were last delivered to their house, the box had been left outside by the garage, where it sat for hours.
 
Temperatures that day were well over 100 degrees, well beyond the safe temperature range listed on the drug's guidelines.
 
At the time, she hadn't worried about it.
 
"Even though I see plainly on the bottle that it says, 'Store at room temperature,'" Boesing said, "I still thought, 'Ah, someone's making sure it's safe.'"
 
But after Wesley's setback, Boesing swore off mail-order pharmacy altogether, and this year she started a Facebook group for patients who share her concerns about how extreme temperatures during shipping could affect the prescription drugs that many people receive by mail.
 
As of 2016, prescriptions fulfilled by mail accounted for nearly a quarter of total U.S. spending on prescriptions (before rebates and discounts), according to a report from IQVIA's Institute for Human Data Science.
 
Health insurers typically contract with companies known as pharmacy benefit managers to handle the complex process of getting medicine to patients. PBMs negotiate with drugmakers on prices and rebates, help insurers decide which drugs to cover and handle mail-order shipping.
 
Mail order is a money saver for PBMs, and, in turn, they've touted the potential advantages for patients -- such as 90-day refills for the cost of a 30-day copay, and the added convenience, especially for rural or housebound patients.
 
But Boesing wants insurers and their PBMs to reconsider these incentives and their practices in light of temperature concerns. She says they must ensure that their patients have easy access to retail pharmacies -- unless the mail-order services can prove that drugs are getting to patients at the right temperatures.
 
The three biggest PBMs are Express Scripts, CVS Caremark and OptumRX. They insist they've got mail-order drug shipment down to a science.
 
Inside an enormous OptumRX warehouse in a Kansas City suburb, lines of orange prescription bottles fly along conveyor belts, while pharmacists scan bar codes and technicians refill bins of pills.
 
Lead pharmacist Alysia Heller explains that this shipping behemoth, which sends out as many as 100,000 prescriptions a day, includes a system to account for weather.
 
"If there's an extreme heat situation where a product is going into 100-plus-degree weather, the system will tell the technician to add an extra ice pack," Heller said, "because we've monitored the ZIP code and the weather in that area."
 
But at OptumRX and across the industry, that level of temperature-controlled shipping is usually reserved only for a relatively small number of drugs -- such as certain types of insulin, or hepatitis C drugs that have specific refrigeration requirements.
 
Standard, room-temperature medications (like most drugs for blood pressure or cholesterol, which make up the vast majority of prescriptions shipped) are typically sent in bubble mailers without any temperature monitors.
 
Stephen Eckel, a pharmacy professor at the University of North Carolina at Chapel Hill, said those practices can lead to some drugs being damaged.
 
"A lot of people enjoy the convenience of mail order, but there are some risks they've got to understand," said Eckel. He said it's possible that drugs in liquid form, such as the one Wesley was taking, could potentially be damaged by exposure to extreme heat or cold.
 
He predicts it's just a matter of time before mail-order pharmacies will expand their use of temperature controls and add individual temperature monitors to all packages, so customers can see whether their medications got too hot or too cold in transit.
 
But Adam Fein, a consultant on pharmaceutical economics and drug distribution, called the temperature concerns overblown. He pointed out that many states already require insurance companies and/or PBMs to offer access to retail pharmacies if customers prefer.
 
"We have literally billions and billions of prescriptions that have been dispensed by mail over many years without evidence of widespread harm," Fein said.
 
The Pharmaceutical Care Management Association is a national trade industry group for PBMs. In response to questions about temperature concerns and the safety of mail-order drugs, the association wrote in a statement: "Mail-service pharmacies adhere to all Food and Drug Administration rules, ship those prescription medications that may be adversely affected by extreme heat in refrigerated packaging, and notify patients to make sure those packages have been delivered properly."
 
Some room-temperature drugs are approved to spend up to 24 hours in temperatures from as low as the upper 50s to as high as 104 degrees. But scientists just don't know how a number of medications respond to more extreme temperatures -- such as they might experience on a freezing porch or in the back of a sweltering truck.
 
A few studies suggest that some inhalers or antibiotics can lose potency over time.
 
Many industry experts think mail-order pharmacy is on the cusp of a boom driven by the development of new specialty drugs, especially biologics. Many of those often come with a hefty price tag and are generally not handled by retail pharmacies. These specialty drugs, many of which are injected, can be more vulnerable to temperature swings.
 
Competition in the mail-order drug industry is heating up, with Amazon's acquisition last summer of online pharmacy PillPack, and the announcement in December that Walgreens would work with FedEx to offer next-day medication delivery.
 
Fein said more temperature controls and monitoring would do little more than drive up costs in an industry that's been successful in large part because of its low operating costs.
 
But after collecting more than 76,000 signatures for an online petition on the issue, Loretta Boesing said she's convinced a larger health problem is being shrugged off.
 
In Missouri, the Board of Pharmacy has decided to review its mail-order prescription policies and invited Boesing to testify.
 
Her son still needs prescriptions, but Boesing has stopped using Walgreens' Specialty Pharmacy, which was shipping the drugs. She obtained a waiver that lets her fill Wesley's prescriptions at a specialized pharmacy affiliated with a children's hospital in St. Louis. She makes the two-hour round-trip drive every month to pick up the medicine.
 
After connecting with patients all over the country, she said, her advocacy is no longer just about keeping Wesley safe.
 
"I don't want my son to have to receive special treatment," Boesing said. "I want everyone to have access to safe medications."
 
This story is part of a partnership that includes KCUR, NPR and Kaiser Health News.
 
This story was produced by Kaiser Health News, an editorially independent program of the Kaiser Family Foundation.


End of Document





 
Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. 
California HealthlineExtreme Temperatures May Pose Risks To Some Mail-Order Meds
 California Healthline
January 10, 2019


Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. 


Byline: Alex Smith, KCUR
Body

Take a look at your prescription bottles. Most say "Store at room temperature" or "Keep refrigerated."
 
But what happens when drugs are delivered by mail? Were those instructions followed as the medicine wended its way from the pharmacy to your doorstep?
 
Those questions haunt Loretta Boesing, who lives in Park Hills, a small town in the hills of eastern Missouri, where the weather varies dramatically from season to season.
 
"It's crazy," Boesing said. "We sometimes experience temperatures like they would feel in Arizona. Sometimes we experience temperatures like they would feel up north."
 
In 2012, when son Wesley was 2 years old, he got so sick from the flu that he needed a liver transplant.
 
The transplant surgery went well, but just a few months later, lab tests showed Wesley's body appeared to be rejecting the organ.
 
Boesing felt both devastated and guilty.
 
"I feel the extra duty of not just protecting his life, but the life that lives on inside him," she said.
 
Wesley didn't lose his new liver, but during his weeks in the hospital, Boesing's mind raced, thinking about what might have gone wrong.
 
She remembered that when his anti-rejection medications were last delivered to their house, the box had been left outside by the garage, where it sat for hours.
 
Temperatures that day were well over 100 degrees, well beyond the safe temperature range listed on the drug's guidelines.
 
At the time, she hadn't worried about it.
 
"Even though I see plainly on the bottle that it says, 'Store at room temperature,'" Boesing said, "I still thought, 'Ah, someone's making sure it's safe.'"
 
But after Wesley's setback, Boesing swore off mail-order pharmacy altogether, and this year she started a Facebook group for patients who share her concerns about how extreme temperatures during shipping could affect the prescription drugs that many people receive by mail.
 
As of 2016, prescriptions fulfilled by mail accounted for nearly a quarter of total U.S. spending on prescriptions (before rebates and discounts), according to a report from IQVIA's Institute for Human Data Science.
 
Health insurers typically contract with companies known as pharmacy benefit managers to handle the complex process of getting medicine to patients. PBMs negotiate with drugmakers on prices and rebates, help insurers decide which drugs to cover and handle mail-order shipping.
 
Mail order is a money saver for PBMs, and, in turn, they've touted the potential advantages for patients -- such as 90-day refills for the cost of a 30-day copay, and the added convenience, especially for rural or housebound patients.
 
But Boesing wants insurers and their PBMs to reconsider these incentives and their practices in light of temperature concerns. She says they must ensure that their patients have easy access to retail pharmacies -- unless the mail-order services can prove that drugs are getting to patients at the right temperatures.
 
The three biggest PBMs are Express Scripts, CVS Caremark and OptumRX. They insist they've got mail-order drug shipment down to a science.
 
Inside an enormous OptumRX warehouse in a Kansas City suburb, lines of orange prescription bottles fly along conveyor belts, while pharmacists scan bar codes and technicians refill bins of pills.
 
Lead pharmacist Alysia Heller explains that this shipping behemoth, which sends out as many as 100,000 prescriptions a day, includes a system to account for weather.
 
"If there's an extreme heat situation where a product is going into 100-plus-degree weather, the system will tell the technician to add an extra ice pack," Heller said, "because we've monitored the ZIP code and the weather in that area."
 
But at OptumRX and across the industry, that level of temperature-controlled shipping is usually reserved only for a relatively small number of drugs -- such as certain types of insulin, or hepatitis C drugs that have specific refrigeration requirements.
 
Standard, room-temperature medications (like most drugs for blood pressure or cholesterol, which make up the vast majority of prescriptions shipped) are typically sent in bubble mailers without any temperature monitors.
 
Stephen Eckel, a pharmacy professor at the University of North Carolina at Chapel Hill, said those practices can lead to some drugs being damaged.
 
"A lot of people enjoy the convenience of mail order, but there are some risks they've got to understand," said Eckel. He said it's possible that drugs in liquid form, such as the one Wesley was taking, could potentially be damaged by exposure to extreme heat or cold.
 
He predicts it's just a matter of time before mail-order pharmacies will expand their use of temperature controls and add individual temperature monitors to all packages, so customers can see whether their medications got too hot or too cold in transit.
 
But Adam Fein, a consultant on pharmaceutical economics and drug distribution, called the temperature concerns overblown. He pointed out that many states already require insurance companies and/or PBMs to offer access to retail pharmacies if customers prefer.
 
"We have literally billions and billions of prescriptions that have been dispensed by mail over many years without evidence of widespread harm," Fein said.
 
The Pharmaceutical Care Management Association is a national trade industry group for PBMs. In response to questions about temperature concerns and the safety of mail-order drugs, the association wrote in a statement: "Mail-service pharmacies adhere to all Food and Drug Administration rules, ship those prescription medications that may be adversely affected by extreme heat in refrigerated packaging, and notify patients to make sure those packages have been delivered properly."
 
Some room-temperature drugs are approved to spend up to 24 hours in temperatures from as low as the upper 50s to as high as 104 degrees. But scientists just don't know how a number of medications respond to more extreme temperatures -- such as they might experience on a freezing porch or in the back of a sweltering truck.
 
A few studies suggest that some inhalers or antibiotics can lose potency over time.
 
Many industry experts think mail-order pharmacy is on the cusp of a boom driven by the development of new specialty drugs, especially biologics. Many of those often come with a hefty price tag and are generally not handled by retail pharmacies. These specialty drugs, many of which are injected, can be more vulnerable to temperature swings.
 
Competition in the mail-order drug industry is heating up, with Amazon's acquisition last summer of online pharmacy PillPack, and the announcement in December that Walgreens would work with FedEx to offer next-day medication delivery.
 
Fein said more temperature controls and monitoring would do little more than drive up costs in an industry that's been successful in large part because of its low operating costs.
 
But after collecting more than 76,000 signatures for an online petition on the issue, Loretta Boesing said she's convinced a larger health problem is being shrugged off.
 
In Missouri, the Board of Pharmacy has decided to review its mail-order prescription policies and invited Boesing to testify.
 
Her son still needs prescriptions, but Boesing has stopped using Walgreens' Specialty Pharmacy, which was shipping the drugs. She obtained a waiver that lets her fill Wesley's prescriptions at a specialized pharmacy affiliated with a children's hospital in St. Louis. She makes the two-hour round-trip drive every month to pick up the medicine.
 
After connecting with patients all over the country, she said, her advocacy is no longer just about keeping Wesley safe.
 
"I don't want my son to have to receive special treatment," Boesing said. "I want everyone to have access to safe medications."
 
This story is part of a partnership that includes KCUR, NPR and Kaiser Health News.
 
This story was produced by Kaiser Health News, an editorially independent program of the Kaiser Family Foundation.


End of Document





 
Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. 
California HealthlineWhy The U.S. Remains The World's Most Expensive Market For 'Biologic' Drugs
 California Healthline
January 28, 2019


Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. 


Byline: Sarah Jane Tribble
Body

Biologic drugs, made from living organisms, are big moneymakers partly because they have little competition from "biosimilars." It's a very different story in Europe.
 
Europeans have found the secret to making some of the world's costliest medicines much more affordable, as much as 80 percent cheaper than in the U.S.
 
Governments in Europe have compelled drugmakers to bend on prices and have thrown open the market for so-called biosimilars, which are cheaper copies of biologic drugs made from living organisms. The brand-name products -- ranging from Humira for rheumatoid arthritis to Avastin for cancer -- are high-priced drugs that account for 40 percent of U.S. pharmaceutical sales.
 
European patients can choose from dozens of biosimilars, 50 in all, which have stoked competition and driven prices lower. Europe approved the growth hormone Omnitrope as its first biosimilar in 2006, but the U.S. didn't follow suit until 2015 with cancer-treatment drug Zarxio.
 
Now, the U.S. government stops short of negotiating and drugmakers with brand-name biologics have used a variety of strategies -- from special contracting deals to overlapping patents known as 'patent thickets'-- to block copycat versions of their drugs from entering the U.S. or gaining market share.
 
As a result, only six biosimilars are available for U.S. consumers.
 
European countries don't generally allow price increases after a drug launches and, in some cases, the national health authority requires patients to switch to less expensive biosimilars once the copycat product is proven safe and effective, said Michael Kleinrock, research director for IQVIA Institute for Human Data Science.
 
If Susie Christoff, a 59-year-old who suffers from debilitating psoriatic arthritis, lived in Italy, the cost of her preferred medicine would be less than a quarter of what it is in the U.S., according to data gathered by GlobalData, a research firm.
 
Christoff tried a series of expensive biologics before discovering that a once-a-month injection of Cosentyx, manufactured by Swiss drugmaker Novartis, worked the best.
 
Without the medicine, Christoff said, her fingers can swell to the size of sausages.
 
'It's 24/7 constant pain in, like, the ankles and feet,' said Christoff, who lives in Fairfax, Va. 'I can't sleep, [and] I can't sit still. I cry. I throw pillows. It's just  awful.'
 
At first, Christoff's copay for Cosentyx was just $ 50 a month. But when a disability led her to switch to a Medicare Advantage plan, her out-of-pocket costs ballooned to nearly $ 1,300 a month -- more than three times her monthly car loan.
 
Christoff, with the help of her rheumatologist, Dr. Angus Worthing, tried Enbrel, Humira and other drugs before finding Cosentyx, the only drug that provides relief.
 
Christoff's case is 'heartbreaking,' Worthing said.
 
Novartis declined to respond to questions about Cosentyx's price. Instead, like other pharmaceutical companies, Novartis says it offers patient-assistance programs for those who can't afford the drug. Christoff said she doesn't qualify for financial assistance.
 
Like other biologics, Cosentyx costs thousands of dollars per month. The annual cost of Christoff's treatment runs about $ 65,000 in the U.S. In Italy, where competition and price negotiations play a bigger role, it would run about $ 15,000, according to GlobalData.
 
In England, Dr. Christopher Griffiths, a lead researcher at the National Institute for Health Research who treats patients with Cosentyx, said the National Health Service would pay about 10,000 pounds, or less than $ 13,000.
 
And those drastic price differences are true even though there is no biosimilar version of Cosentyx yet available in Europe, and might not be for years.
 
The cost of the drug is taking a toll on Christoff. This past summer, her progressive disease made it difficult to enjoy the annual family vacation with her three grown children and their kids in Virginia Beach, Va.
 
'I can't get down on the sand to play with my kids without help. I can't get up without help,' Christoff recalled. 'I'm not ready to stop trying. But I'm also not ready to go through my entire retirement fund to walk.'
 
Unlike Cosentyx, rival drugs -- Humira, Enbrel and Remicade -- all face biosimilar competition in Europe. Only Remicade has competition from a lower-cost biosimilar in the U.S., and Humira isn't expected to have a copycat competitor in the U.S. market until 2023. Humira, made by AbbVie, is the world's top-selling drug.
 
In late October, Wall Street analyst Ronny Gal at Sanford C. Bernstein & Co. noted that AbbVie agreed to drop Humira's price by 80 percent in one Nordic country to combat biosimilar competition. During the company's quarterly conference call, AbbVie chief executive Richard Gonzalez said the drug's discount was as low as 10 percent and as high as 80 percent across the continent, with the highest discounts in Nordic countries.
 
'These are markets where it's 'winner takes all' across the entire  category, so includes Remicade and Enbrel as well,' Gonzalez said in November, adding that Nordic countries represent about 4 to 5 percent of overall revenue in AbbVie's international business.
 
Concerned about how much biologics cost the U.S. health system and patients, Food and Drug Administration Commissioner Scott Gottlieb announced an 'action plan' last summer that included tapping the Federal Trade Commission for help, saying he was 'worried' about the biosimilar market.
 
'The branded drug industry didn't build its success by being business naive; they are smart competitors,' Gottlieb told an audience full of advocates, industry insiders and researchers at the Washington, D.C.-based Brookings Institution in July. 'But that doesn't mean we need to embrace all of these business tactics or agree with them and think they are appropriate.'
 
One of these business tactics involves so-called rebate traps, in which financial deals are cut to make sure patients can get only a biologic, not a biosimilar. International drugmaker Pfizer alleged in a September 2017 lawsuit that exclusionary contracts created by Johnson & Johnson prevented use of its biosimilar by health insurers, hospitals and clinics.
 
Johnson & Johnson's wildly successful biologic Remicade, the brand-name version of infliximab, produced $ 6.3 billion in worldwide revenue in 2017. Pfizer launched its copycat drug, Inflectra, in the U.S. in October 2016, noting in the announcement that it would price the drug at a 15 percent discount to Remicade's wholesale price.
 
Still, health systems such as Geisinger Health, based in Pennsylvania, say they have had difficulty switching to the less expensive alternative.
 
'J&J has done a really good job of entrenching themselves in the market,' said Jason Howay, manager of formulary services at Geisinger.
 
The health system ultimately decided it wanted to switch all adults to Pfizer's biosimilar, saying it provided the same quality of treatment. But Johnson & Johnson had 'bundled' the prices of other drugs with Remicade. So if Geisinger stopped using Remicade on adult patients, J&J could stop providing discounts on other drugs, such as those used for cardiology, Howay explained. 'It weaves a very tangled web.'
 
A spokeswoman for Janssen, Johnson & Johnson's main pharmaceutical subsidiary, says the drugmaker does offer 'more attractive contract terms' to buyers who use a wider range of J&J medicines. 'Our contracting approach has always prioritized access for patients and their providers,' Meredith Sharp said.
 
Geisinger negotiated with biosimilar maker Pfizer and won still lower prices to make up for lost savings on the other J&J drugs. It's now transitioning all adult patients to the less expensive biosimilar.
 
Another business tactic is creating patent thickets, in which drugmakers win numerous patents to block competitors entirely. Humira, which treats arthritis and Crohn's disease, has more than 100 patents protecting it in the U.S. By contrast, cheaper biosimilar versions of the AbbVie drug rolled out in October in Europe, where patent laws are less likely to offer such protection.
 
Such tactics are stifling competitors, according to Bruce Leicher, a former senior vice president and general counsel for Momenta Pharmaceuticals, a Cambridge, Mass., biotechnology company focused on rare-disease drug development. Smaller companies are 'struggling now' to plan launches of new drugs. Momenta cut half its staff in October, including Leicher, and announced plans to shift away from five biosimilar development programs.
 
Leicher said the FDA's biosimilar action plan under Gottlieb is a good first step but 'would have been wonderful to see five years ago.'
 
Other lingering regulatory burdens have hampered biosimilar adoption as well. Last summer, Medicare began reimbursing doctors and hospitals differently for biosimilars. Before that change, patients could pay more out-of-pocket for less expensive rheumatoid arthritis biosimilars than for the brand-name biologics.
 
And, unlike more chemical compound generics, like aspirin, a pharmacist cannot automatically replace a brand-name biologic with the biosimilar. More than 40 states have passed laws around how and when doctors and pharmacists can substitute a biosimilar for a biologic. Federal guidelines are still not established.
 
In addition, the FDA requires each biosimilar name to include a random suffix, ostensibly to differentiate it from the biologic drug. That can be confusing and could cause patients and physicians to believe the products are significantly different in quality, said Christine Simmon, who promotes biosimilars for the trade group Association for Accessible Medicines.
 
She said the naming convention 'fuels the fire' of a broader debate about whether to trust biosimilars, adding, 'Making sure doctors and patients are comfortable with the products is integral to uptake.'
 
This story was produced by Kaiser Health News, an editorially independent program of the Kaiser Family Foundation.
 
Sarah Jane Tribble: sjtribble@kff.org, @SJTribble  


End of Document




UCSD Professor Elected to the National Academy of Medicine
City News Service
October 15, 2018 Monday 10:33 AM PST


Copyright 2018 City News Service, Inc. All Rights Reserved
No City News Service material may be republished without the express written permission of the City News Service, Inc.
Length: 388 words
Body


SAN DIEGO (CNS) - A UC San Diego Health professor is one of 85 members in the health and medicine field elected to this year's class of the National Academy of Medicine, UCSD announced today.
Dr. Lucila Ohno-Machado is the chair and founder of UCSD's Department of Biomedical Informatics.
"Bioinformatics and big data are increasingly a foundation and driver of modern medicine. Lucila is an undisputed expert and pioneer in both," said Dr. David Brenner, UCSD's vice chancellor of health sciences. "Her leadership and dedication to scientific excellence make her an ideal NAM member. We are proud of the many research and education programs she has built here, helping UC San Diego remain a leader in these critical fields."
The 2018 academy class includes 75 regular members and 10 international members. Current members elect new members each year, focusing on medical professionals who have made significant contributions in the medical science, health care and public health fields. Academy members elected Ohno-Machado for her work creating an algorithm that allows medical professionals to share clinical data while maintaining a patient's privacy, according to the NAM.
Ohno-Machado is an informatics and technology professor at the School of Medicine and a founding member of the UCSD Halicioglu Data Science Institute. In addition to her work at UCSD, Ohno-Machado is a research health scientist for San Diego's Veterans Affairs healthcare system, co-leads the California Precision Medicine Consortium and has procured grants totaling more than $100 million for UCSD. She is also the editor-in-chief of the Journal of the American Medical Informatics Association.
"I'm proud of building a completely new department from the ground up -- it's been a big challenge, but it has come with big rewards," Ohno-Machado said. "I'm grateful to be able to do this work at UC San Diego, where we have the great advantage of being able to put our innovations into practice in our own health system, and I'm honored to receive this recognition for our work."
Ohno-Machado received her doctorate in Medical Information Sciences and Computer Science from Stanford University, her medical degree from the University of Sao Paulo and a master's degree in Healthcare Administration at Escola de Asministracao, Fundacao Getulio Vargas in Sao Paulo.

Load-Date: October 16, 2018


End of Document




Cloudvirga Earns a 2018 Benzinga FinTech Award
CaliforniaNewswire
May 29, 2018 Tuesday


Copyright 2018 CaliforniaNewswire, distributed by Contify.com All Rights Reserved
Length: 329 words
Byline: Valerie Gotten
Body


IRVINE, Calif. /California Newswire/ - Cloudvirga(TM), a leading provider of digital mortgage point-of-sale (POS) software whose enterprise technology is powered by the intelligent Mortgage Platform(R), was honored as one of the world's top fintech firms at last week's Benzinga Global Fintech Awards.
After advancing to the competition's final round in April, Cloudvirga was one of just three companies recognized in the Best Digital Mortgage Solution category during the awards ceremony on May 15 in New York. The following morning, Cloudvirga announced it had completed a $50-million Series C funding round led by private-equity firm Riverwood Capital.
Founded in 2015, the Benzinga Global Fintech Awards recognizes leading innovators in finance and fintech. Hundreds of companies vied for recognition in nearly 30 categories this year.
Cloudvirga is proud to be recognized as a global leader in digital mortgage solutions, said Cloudvirga CEO Michael Schreck. Our intelligent Mortgage Platform combines decades of real-world mortgage experience with the latest innovations in process automation, data science and machine learning to produce a world-class borrower experience while radically reducing the cost of lending for mortgage companies. We're redefining what the digital mortgage experience means for consumers, loan officers and back-office teams.
About Cloudvirga(TM):
Cloudvirga's digital mortgage point-of-sale (POS) software, powered by the intelligent Mortgage Platform(R), uniquely combines a world-class borrower experience with a truly automated lender workflow that radically cuts overall loan costs, increases transparency and reduces the time to close a loan. Founded by top fintech veterans with a track record of building successful mortgage technologies, Cloudvirga's customer base includes eight of the top 40 non-bank mortgage originators. To date, Cloudvirga has raised over $77 million from some of the country's top lenders and private-equity firms.

Load-Date: May 30, 2018


End of Document




Has the Opioid Epidemic Begun to Turn a Corner?
Creators Syndicate
April 27, 2018 Friday


Copyright 2018 Creators Syndicate, Inc. All Rights Reserved
Section: HAS THE OPIOID EPIDEMIC BEGUN TO TURN A CORNER?
Length: 1007 words
Byline: C Force
Body


Wouldn't you know? No sooner do I come out questioning the effects of the recent commitment by policymakers and the medical establishment to upping the ante in addressing the nation's opioid epidemic than a new report appears pointing toward signs of success. This report, authored by IQVIA Institute for Human Data Science, suggests the various measures undertaken to fight the opioid epidemic appear to be having an impact. The report says that in 2017, retail opioid prescriptions declined by 10.2 percent. In addition, the number of patients who received opioid prescriptions for the first time fell by 8.7 percent last year. On average, prescription opioid volume has decreased every year over the last five years in all 50 states.
Before we get too carried away with the news, it should be noted that at the same time, the number of new monthly prescriptions for medications that treat opioid addiction nearly doubled over the past two years. While these medications may help temper cravings, they do not necessarily lead to kicking the habit. As pointed out in a recent New York Times report, "although the number of people taking medications to combat addiction is rising, it remains a small fraction of the roughly 2.6 million people believed to suffer from 'opioid use disorder,' or addiction." There is also no way of knowing from the data whether the prescriptions came with behavioral therapy and other support.
"Unfortunately, we're ... still ... massively overprescribing," Dr. Andrew Kolodny, head of the Opioid Policy Research Collaborative at Brandeis University and executive director of the advocacy group Physicians for Responsible Opioid Prescribing, told STAT News. While Dr. Kolodny is pleased with the news that we are "moving in the right direction," he believes that many Americans are still going to become addicted until prescribing doctors become much more cautious.
As noted by The Times: "The declines come amid a flurry of new insurance company policies and state laws setting limits on opioid prescribing. States have also been tracking opioid prescriptions more closely through electronic databases and requiring more doctors to check the databases for signs of 'doctor shopping' or misuse before giving a patient opioids."
The IQVIA Institute report also underscores questions as to whether some pain patients are now being undertreated, as well as whether tightened prescribing over the last few years has contributed to the surge in overdose deaths from heroin, especially fentanyl. The nation's opioid epidemic continues to be responsible for the death of more than 115 people every day in this country.
It is hoped that pressure being applied by policymakers is also starting to persuade the medical establishment to begin taking alternative treatment approaches more seriously.
"The issue here is there are startling gaps in quality of care for people receiving medication-assisted treatment," Dr. G. Caleb Alexander, co-director of the Johns Hopkins Center for Drug Safety and Effectiveness, explains to The New York Times. "So it's really important that we do a better job of improving it and building out systems of care that can deliver it en masse."
Meanwhile, in scattered places such as Lafayette, Colorado, the practice of incorporating alternative medicine into mainstream medical care is paying dividends. In an eight-week course available to Colorado Kaiser Permanente members for a fee of $100, high-risk opioid patients are being educated on an integrated program on pain management.
The key element of Kaiser Permanente's Integrated Pain Service program is it is just that: truly integrated. For patient care, a doctor, two mental health therapists, a clinical pharmacist, a physical therapist and a nurse are available, all on one floor. Patients have the option to meet with this support team all at once or in groups. There is no need for referrals or making doctor appointments at different facilities. This approach appears to be working.
Kaiser researchers tracked more than 80 patients over the course of a year and found the group's emergency room visits decreased 25 percent. Inpatient admissions dropped 40 percent, and patients' opioid use was dramatically down.
While some patients need to go to the chemical dependency unit for medication-assisted treatment, they also have access to alternatives to drugs like exercise, acupuncture, meditation and mindfulness. Similar projects to Kaiser Permanente's Integrated Pain Service program in states like California have been equally successful.
"The future of health care is integrated and unfortunately, our history is very fragmented and we're just now catching up to developing a system of care that meets the needs of people," Benjamin Miller, an expert on integrated care with the national foundation Well Being Trust, reminds NPR.
But programs such as this have proved to be difficult to implement universally. According to NPR, one challenge is scale. Big systems like Kaiser have resources to afford to run programs like this. Another challenge is payment. Some insurers will not pay for alternative treatments, while others have separate payment streams for different kinds of care. Often medical health and behavioral health are paid for by entirely different systems.
Even in Colorado, there remains a severe shortage of treatment options. In December of last year, the state's largest substance abuse treatment provider, Arapahoe House, announced it was closing its doors.
Clearly, there remains lots of work to be done to stem the tide of the opioid epidemic in this country.
Write to Chuck Norris (info@creators.com) with your questions about health and fitness. Follow Chuck Norris through his official social media sites, on Twitter @chucknorris and Facebook's "Official Chuck Norris Page." He blogs at http://chucknorrisnews.blogspot.com. To find out more about Chuck Norris and read features by other Creators Syndicate writers and cartoonists, visit the Creators Syndicate Web page at            www.creators.com.
COPYRIGHT 2018 CREATORS.COM

Load-Date: April 27, 2018


End of Document




The Whistle-Blower Behind the Facebook Data Scandal Has a Surprising Fashion Background
hollywoodreporter.com
March 19, 2018 Monday


Copyright 2018 Prometheus Global Media, LLC All Rights Reserved


Section: NEWS; TAG
Length: 674 words
Byline: Sam Reed
Body


The unlikely common denominator between Robert Mueller`s Russia investigation, a sphere of data science and the fashion industry is a pink-haired 28-year-old Canadian vegan named Christopher Wylie.
Over the weekend, The New York Timesand            The Guardianboth published articles revealing that U.K.-based data firm Cambridge Analytica - the company Wylie and Steve Bannon built with the help of a $15 million investment from Republican billionaire donor Robert Mercer - had unethically obtained and exploited the personal data of more than 50 million Facebook users without their consent. This data was then analyzed to identify connections between personality traits and political leanings, becoming the foundation for techniques later used by the Trump presidential campaign. 
The lengthy articles detail cover-ups orchestrated by parties involved in the ordeal, but what was perhaps most intriguing was Wylie`s own backstory. Despite leaving school at age 16, Wylie had no trouble finding work. When he was 19, he taught himself to code, and at age 20 he enrolled at the London School of Economics to study law. At the time he helped launch Cambridge Analytica at age 24, he was working toward a PhD in fashion trend forecasting. 
Yes, while the young data scientist was coming up with the algorithm that would help channel the harvested data into what he later described as "Steve Bannon `s psychological warfare mindfuck tool," he was in the midst of learning how to predict fashion trends - a bit of a distant cousin to the kinds of psychological trends he then analyzed for the former Breitbart editor.
In an interview with The Guardian`s Carole Cadwalladr, Wylie used his background as a helpful metaphor to illustrate how Bannon used the data for his own politically motivated purposes. "[Bannon] believes in the whole Andrew Breitbart doctrine that politics is downstream from culture, so to change politics, you need to change culture," Wylie explained. "Fashion trends are a useful proxy for that. Trump is like a pair of Uggs, or Crocs, basically. So how do you get from people thinking 'Ugh. Totally ugly' to the moment when everyone is wearing them? That was the inflection point he was looking for."
Whether he was aware of it or not (and given his background, we`d like to think that he was definitely aware of it), both Crocs and Uggs were having a major moment in fashion while he was carrying out his work for Cambridge Analytica. In 2016, Christopher Kane sent bedazzled Crocs down the runway. A year later, Balenciaga gave Mario Batali`s preferred footwear the official high-fashion stamp of approval, sending platform versions of the sandal/sneaker hybrid down the catwalk. 
Meanwhile, Uggs has been attempting to bolster its own cool factor with various designer collaborations, including a collection by Moschino creative director Jeremy Scott, as well as a selection by Phillip Lim. Yes, both fugly footwear specimens did become cool again, and Trump won the presidential election, perhaps thanks to the help of Cambridge Analytica and Wylie`s analytics. So perhaps trend forecasting and political forecasting aren`t that different after all? 
Bannon`s disheveled (to put it nicely) appearance and propensity for cargo jackets aside - believe it or not, the former White House strategist has championed fashion before. During his tenure at Breitbart, the 64-year-old brought on fashion critic John Binder, 25, who regularly assesses the first lady`s style for the conservative outlet. The coverage, predictably, is rarely critical.
Another unexpected revelation from Wylie`s characterization of Bannon - the man who preaches xenophobia - is his affinity for "the gays." Wylie, who is openly gay, noted in the profile that Bannon "loved the gays.... He saw us as early adopters. He figured, if you can get the gays on board, everyone else will follow."

Load-Date: May 3, 2018


End of Document




Tribeca: 'Enhanced' Directors Discuss Sports Science's Complicated Future
hollywoodreporter.com
April 27, 2018 Friday


Copyright 2018 Prometheus Global Media, LLC All Rights Reserved


Section: NEWS; TAG
Length: 520 words
Byline: Zoe Haylock
Body


Sports are riddled with superstitions, traditions and rules that keep the game fair and sacred for its fans. But as technology and science progress, the boundaries will be pushed farther and farther.
This is what the six-part television series Enhanced on ESPN, which premiered at Tribeca Film Festival on Thursday (April 26), will tackle. After a screening of part one, "Knowledge," directors Chai Vasarhelyi, Alison Klayman and Jesse Sweet and executive producer Alex Gibney joined The Hollywood Reporter's East Coast TV editor Marisa Guthrie for a conversation about the documentary series. 
The first episode deals with the future of data science. Gone are the days of Moneyball or, even sabermetrics, the application of statistical analysis to baseball records. With new technology, like Nike's wearables, sports data has reached new heights. Later episodes deal with steroids, gene manipulation, brain mapping and other scientific frontiers. Beyond the capability of each type of enhancement, the real question, which Guthrie posed, is: Are these enhancements helping or hurting the games?
"I think both things are true," Sweet said Thursday night. "It`s a tool and it can give you what you want. There`s the players` perspective, the fans` perspective and the ownership`s perspective. It offers every one of them a new benefit, but also a drawback."
One major drawback when it comes to players' data is privacy. Once a player's strength, speed and even sleeping habits are tracked, who owns that information?
"If Cambridge Analytica can look at how many cat videos I like and know whether I`m going to vote for a Libertarian or not, what can they tell by tracking your heart rate over 40 years and how much [you]'re sleeping?" Sweet continued. "No one`s really thinking about this, I think, because the players are so hyper-competitive. It`s just 'How can I use this to get on a team?'"
While some technology offers more precise measurements of athletes` bodies, other sports insiders are using science to find ways to keep athletes` bodies intact. For example, Dallas Mavericks owner Mark Cuban got permission tofund a studyabout human growth hormones` effects in recovery from ACL surgery. 
"When you talk about what the needs of players are, anything that can help you recover faster is a great thing," Klayman explains.
Her episode, "Recovery," deals with the nuances behind human limits. 
Vasarhelyi's episode on "Power" sheds a new light on the technologies that do the pushing, like steroids. 
"It`s not to say that performance enhancing drugs aren`t bad for you," the director explains. "But by banning them, they`re essentially pushed to the underground so there`s no real medical research and no way to do that medical research, hence Mark Cuban finances it privately." 
Steroids and data are just the tip of the sports science iceberg, and Enhanced will tackle these issues and more throughout its six-part series.

Load-Date: May 3, 2018


End of Document




Hillary Clinton -- Endless Sore Loser
Creators Syndicate
June 2, 2017 Friday


Copyright 2017 Creators Syndicate, Inc. All Rights Reserved
Section: HILLARY CLINTON -- ENDLESS SORE LOSER
Length: 1005 words
Byline: David Limbaugh
Body


Attacks on President Donald Trump are commonplace   many depict him as this wildly bizarre, classless person occupying the Oval Office   but have critics fairly considered what a horror show a Hillary Clinton presidency would have been?
Why is this relevant, you ask? Well, because the liberal media are permanently afflicted with Trump derangement syndrome and won't quit feeding Clinton's narcissistic obsession with her defeat. Did the media fixate on Mitt Romney's defeat to Barack Obama and forever question him about it?
Clinton has been muttering about her loss since she recovered from the initial election-night shock, and it has been ugly. In early April   and probably earlier   she attributed her loss, in part, to misogyny. "It is fair to say ... certainly, misogyny played a role," she lamented at the Women in the World Summit in New York. "I mean, that just has to be admitted. ... Some people, women included, had real problems" with "the first woman president."
In early May, Clinton said she takes "personal responsibility" but then quickly contradicted herself by shifting blame to Russian interference in the election and then-FBI Director James Comey's release of a letter concerning the investigation into her emails.
Late in May, Clinton resurfaced at the Code Conference, denying she or her organization made any significant mistakes in the campaign and blaming many others and other factors for her loss. She said the Russian government orchestrated a vast disinformation campaign to discredit her, and she also blamed WikiLeaks' release of campaign chairman John Podesta's emails and speculated that Trump had colluded with Russians to disseminate this information. She lambasted the media for covering her email chicanery as if it were Pearl Harbor, calling it "the biggest nothingburger ever." And for good measure, she further blamed sexism, saying that criticism of her six-figure speeches to various groups was gender-driven.
She introduced a new twist, however, in pointing her finger at her formerly beloved Democratic Party. "I get the nomination. ... I inherit nothing from the Democratic Party," she huffed. The Democratic National Committee "was bankrupt. It was on the verge of insolvency. Its data was mediocre to poor, nonexistent, wrong," she continued. "I had to inject money into it to keep it going." She also threw her fellow Democrats under the bus, saying that they "are not good historically at building institutions," adding, "We've got to get a lot better." Clinton said her campaign was further crippled by the widespread assumption that she was going to win.
Her attack on the DNC won her no friends in the party. Andrew Therriault, former data science director for the DNC, tweeted (and later deleted) profanity at Clinton's convenient narrative: "DNC data folks: today's accusations are f -ing bull  , and I hope you understand the good you did despite that nonsense." He added, "Private mode be damned, this is too important. I'm not willing to let my people be thrown under the bus without a fight."
Nor did her attacks on the media sit well with certain media mavens. MSNBC host Andrea Mitchell said Clinton's claim that Americans colluded with Russia to "weaponize information" against her is "drawing a conspiracy theory" against the Trump campaign without evidence.
Isn't it interesting that it took an insult from Clinton to get a liberal media person to admit there's no evidence of Trump collusion with Russia?
Clinton's right about one thing: The Democratic Party is to blame. That's because it nominated her to run for president and even colluded with her against Bernie Sanders to ensure it happened.
But the main takeaway from Clinton's pathetically endless election post-mortem is that while Trump critics dwell on his alleged instability and lack of class, Clinton has further proved herself to be worse in reality than Trump is perceived to be.
I'm not talking about policy matters, albeit Clinton would have been an unspeakable disaster in that realm as president, too. I mean on a personal level, where this woman who has held herself out as a public servant all these years is a self-absorbed political animal and shows little grace and even less class. I have no doubt that the stories we've read about how she mistreats people are true.
Cursory inspection of her many scapegoats reveals that even if any of her claims have merit, she is the primary reason for every one of them. She was the virtual head of the DNC she castigates. Her own gross negligence (and criminality, truth be told) led to the FBI investigation, without which Comey wouldn't have made any statement. She bought into and perpetuated the narrative that she was the prohibitive favorite in the campaign. She offered no change from, much less any explanation for, Obama's horrendous record. Her campaign platform was simply, "Never Trump." She arrogantly refused to campaign in Wisconsin, a blue state that ultimately swung to Trump, and she didn't devote nearly enough resources to the other blue states of Michigan and Pennsylvania. She invites attention to gender in constantly whining about it on the one hand while lecturing us for considering it on the other. And can we all just please admit that she forfeited standing to complain about mistreatment of women long ago when she enabled her world-class womanizing husband's serially decadent dalliances? Despite her reputation for brilliance and experience, she couldn't defeat political novice Donald Trump in their debates.
Hillary Clinton was a disastrous candidate and would have been a worse president. How long must we endure these public postelection couch sessions?
David Limbaugh is a writer, author and attorney. His latest book is "The True Jesus: Uncovering the Divinity of Christ in the Gospels." Follow him on Twitter @davidlimbaugh and his website at www.davidlimbaugh.com. To read features by other Creators Syndicate writers and cartoonists, visit the Creators Syndicate webpage at            www.creators.com.
COPYRIGHT 2017 CREATORS.COM

Load-Date: June 2, 2017


End of Document




No Headline In Original
City News Service
November 10, 2017 Friday 3:53 PM PST


Copyright 2017 City News Service, Inc. All Rights Reserved
No City News Service material may be republished without the express written permission of the City News Service, Inc.
Length: 244 words
Dateline: LOS ANGELES
Body


For the second consecutive year, Los Angeles has been named America's top digital city by a national advisory institute focused on how governments can best serve constituents through information technology.
The City of Angels took first place among cities with a population over 500,000 in the 2017 Digital Cities Survey conducted by the Center for Digital Government, Mayor Eric Garcetti's office announced Friday.
L.A. also won the honor in 2016.
"Special congratulations go out to Los Angeles, which has been recognized with multiple awards (from the Center) this year for a broad range of technology-enhanced efforts," said Teri Takai, executive director of the Center. "We integrate technology into everything we do to serve our communities better and create new opportunities in the lives of Angelenos," Garcetti said. "L.A. is a center of innovation -- and our commitment to digital technology and open data is bringing more democracy, more efficiency and better service to neighborhoods across the city."
The programs for which Los Angeles was recognized include:
-- Free devices and WiFi to assist the most vulnerable residents through OurCycle LA;
-- LA Cyberlab, a public/private partnership to thwart cyber criminals;
-- A partnership with the U.S. Geological Survey to deploy the Earthquake Early Warning System;
-- The Data Science Federation, a partnership with colleges and universities to tackle tough challenges and expand data-driven decisions at City Hall.

Load-Date: November 11, 2017


End of Document




Opinion: Take the Dictator Quiz
CaliforniaNewswire
February 24, 2017 Friday


Copyright 2017 CaliforniaNewswire, distributed by Contify.com All Rights Reserved
Length: 576 words
Byline: John Scott G
Body


Lots of people dream of having unlimited power but that requires a hatred of democracy. Take this test to see if you are ignorant and biased enough to try holding the reins of an embarrassed nation. 
1. The three branches of U.S. government are:
a) Executive, Legislative, Judicial.
b) President, President, President.
2. A primary goal of the leader of the free world should be:
a) Upholding the Constitution.
b) Making as much money as possible.
3. To ensure that future generations have a hospitable planet:
a) The U.S. should lead the world in combatting climate change.
b) The question is moot because "The concept of global warming was created by and for the Chinese in order to make U.S. manufacturing non-competitive."
4. Helping assure transparency in government is one reason for:
a) The free press.
b) Forget that. Instead let's go with the bleating of Sean "Melissa" Spicer and Kellyanne "Alternative Integrity" Conway.
5. Justice and equality are best served by:
a) Invoking and upholding the rule of law.
b) Appointing bigot Jefferson Beauregard Sessions III as Attorney General.
6. To ensure the world's confidence in the U.S., the POTUS should:
a) Act in a sober and responsible manner.
b) Confidence schmonfidence. Let's insult allies, attack NATO, and terrify all sentient beings by suggesting that more nations obtain nuclear weapons.
7. Emphasizing facts requires:
a) Reading and comprehending multiple news sources.
b) That takes work. It's more fun to quote FoxFakeNews, NotSoBrightBart, WorldNutDaily, InfoWhores, Heritage Confoundation, Lame-o Institute, and DailyStormTrooper.
8. As leader of the free world, the POTUS should set an example by:
a) Emphasizing facts, data, science, and reality.
b) Making up crap.
9. A responsible chief executive will:
a) Gather a team of responsible thinkers.
b) Take advice from nihilist Steve Bannon and rightwing nutjob Stephen Miller.
10. To remove any potential conflicts of interest:
a) The POTUS must divest his financial holdings.
b) Hey, no conflict, just the bottom line: grab the money grab the money grab the money lie about it grab more money grab still more money grab the money grab the money grab the money lie about it grab more money grab still more money make the deal make the deal make murica hate again grab the money grab the money grab the money lie about it grab more money grab still more money grab the money grab the money make the deal grab the money and screw you for not getting in on the deal.
Scoring
10 points for every (a) answer and zero points for every (b) answer. 
100 points = Excellent choice for POTUS
90 points = Good choice for POTUS
80 points = Okay choice for POTUS
70 points = You are Millard Fillmore.
60 points = You are a staffer for Richard Nixon.
50 points = You are Richard Nixon.
40 points = You are Caligula.
30 points = You are Idi Amin.
20 points = You are Mitch McConnell.
10 points = You are Paul Ryan.
0 points = You are Donald Trump.
# # #
Editorial Note: Be aware this series of political articles may contain intentional satire by author John Scott G and is not fully based on fact (aside from some of the stupider things, which are sadly 100% real). Hopefully you can tell the difference?
This opinion piece is Copr. 2017 by John Scott G and originally published on CaliforniaNewswire.com - a publication of The Neotrope News Network - all commercial and reprint rights reserved. Opinions expressed are solely those of the author. Editorial collage image by and Copr. John Scott G.

Load-Date: February 25, 2017


End of Document





 
Copyright © Advisory Board Company & California Healthcare Foundation 2014, All Rights Reserved. 
iHealthBeatCan the U.S. Health Care System Realize the Promise of Digital Health?
 iHealthBeat
November 20, 2014


Copyright © Advisory Board Company & California Healthcare Foundation 2014, All Rights Reserved. 


Section: Insight
Byline: Kate Ackerman, iHealthBeat Editor in Chief
Body

NEW YORK -- Embracing digital health is key to curbing out-of-control health care costs, increasing access to care, improving care quality and encouraging patient engagement. But the current regulatory and policy landscape could get in the way of the U.S. realizing the full potential of health IT.
 
That was the message from speakers and attendees at the New York eHealth Collaborative's fourth annual Digital Health Conference in New York City this week.
 
Why the U.S. Health Care System Is Ripe for a Digital Health Intervention
 
Ezekiel Emanuel -- vice provost for Global Initiatives and chair of the Department of Medical Ethics and Health Policy at the University of Pennsylvania -- said that the most important number in health care is $ 3.05 trillion. That's the amount of money the U.S. is set to spend on health care this year. For comparison, Emanuel, a former adviser to President Obama, noted that France's gross domestic product is $ 2.73 trillion, meaning the U.S.' health care system is actually the fifth-largest economy in the world.
 
He explained that chronic disease care is a huge contributor to health care spending. According to Emanuel, half of the U.S. population accounts for just 2.7% of all health care spending, while 10% of the population is responsible for 65.3% of spending.
 
Health IT -- including electronic health records, telehealth and mobile health tools -- could be leveraged to improve preventive care, boost care coordination, increase adherence to clinical guidelines and lower costs when treating individuals with multiple chronic diseases, Emanuel argued.
 
Further, health systems could tap "health data analytics to identify next year's high-cost patients in advance," he said.
 
Eric Topol, director of the Scripps Translational Science Institute, said medical care is currently provided at the population level, which is "incredibly wasteful, imprecise and, in many ways, harmful."
 
Topol said that providing the same prescription to all patients with a certain disease doesn't make sense, noting that the top three prescription drugs in the U.S. work for only 35% of patients. Instead of the current one-size-fits-all approach, Topol said patients should be analyzed to determine which treatments will work best for them.
 
Because of the availability of new tools, the U.S. health care system has an opportunity to shift toward individualized medicine where there is a "Google Map of each individual" that helps determine the best course of treatment, Topol said.
 
Topol predicted that the U.S. will move toward a "doctorless patient model." While physicians won't be replaced and, in fact, "their importance will be magnified," digital health will democratize health care and create a partnership between patients and physicians, Topol asserted.
 
Emanuel offered his own predictions for the future of U.S. health care, highlighting six "megatrends":
 - Diffusion of VIP care for the chronically and mentally ill;
 - Expansion of digital medicine and closure of hospitals;
 - End of insurance companies as we know them;
 - End of employer-sponsored health insurance;
 - End of health care inflation; and
 - Evolution of academic health centers.
 
Speakers throughout the two-day conference echoed the sentiment that digital health holds the promise to help transform care.
 
John Brownstein -- an associate professor at Harvard Medical School and director of the Computational Epidemiology Group at Boston Children's Hospital -- discussed how social media can quantify population health in a way that traditional public health interventions cannot. He cited several areas where social media can help, including:
 - Infectious disease;
 - Chronic disease;
 - Prescription drug safety;
 - Patient experience; and
 - Drug diversion and abuse.
 
Martin Coulter, CEO of PatientsLikeMe, discussed how social networking sites can "elevat[e] the patient voice to the level of medical evidence."
 
Based on PatientsLikeMe's experience, he said:
 - Members are engaged and want to participate;
 - Sensors, such as wearable devices, help increase patient engagement, activation and data donation;
 - Members share sensor data with doctors;
 - Objectively captured sensor data correlates with measures of functional disability as reported subjectively by patients; and
 - The pharmaceutical industry is systematically learning from patients and applying those lessons to business needs.
 
Will the Health Care System Be Able To Realize Promise of Digital Health?
 
Despite all the optimism surrounding the promise of digital health, there are a host of regulatory and policy issues that could impede progress.
 
Privacy and Consent
 
Throughout the conference, privacy was continuously cited as a barrier to greater adoption of health information exchange, mobile health and data analytics.
 
Health information organizations have struggled with how to acquire consent from patients to share their health information.
 
Deven McGraw -- a partner in the health care practice of Manatt, Phelps & Phillips and a member of the Health IT Policy Committee -- said patients have a dual interest of wanting to have their data shared to improve the care they receive and protecting their privacy. She said that an "opt-in" consent approach might feel more patient-centered, but it's very expensive and ends up emphasizes patients' privacy interests over patients' data-sharing interests. Instead, she said federal work groups came down on the side of "meaningful opt-out" as the best consent approach.
 
McGraw added that offering patients a portal where they can access their own data will help them understand the value in health data sharing and help the "needle move."
 
Meanwhile, Ann Waldo, a partner at Wittie, Letsche & Waldo, said HIPAA's "data culture" is "highly resistant to sharing, even with patients." She added that "uncertainty among innovators as to the rules of the road outside HIPAA" is holding the market back.
 
Waldo said that extending HIPAA to consumer health data would be a "terrible idea" because its complexity would add additional burdens and hurt innovation. Instead, she suggested developing a self-regulatory framework of best practices for consumer health data. According to Waldo, organizations that comply with the framework could put a seal to alert consumers on their website or applications.
 
Even after consumers are on board with mobile health apps or devices, there can be controversy over how collected data are used.
 
For example, Andrew Rosenthal, group manager of wellness and platform at Jawbone, discussed how after an earthquake in the San Francisco area in late August, the company's data science team looked at sleep data of Jawbone users across the region and was able to accurately identify the epicenter of the earthquake and to quantify the amount of sleep lost because of the earthquake.
 
Although the analysis used anonymized and aggregate data, some critics said the company overstepped in its use of the data.
 
Julia Bernstein -- strategy and sales operations lead at Ginger.io, a health data platform -- said vendors should be very clear from the beginning about the value of their product and the trustworthiness of the system.
 
Arthur Levin -- co-founder and director of the Center for Medical Consumers -- added that firms can build trust by being transparent and delivering on what they say they're going to do.
 
Data Usability and Interoperability
 
Other commonly cited barriers are limitations surrounding health data and interoperability.
 
Andrew Kasarskis -- co-director of the Icahn Institute for Genomics & Multiscale Biology and an associate professor at the Icahn School of Medicine at Mt. Sinai -- noted that EHRs have a data limitation because people do not actually spend much time in hospitals or with doctors. He added that learning how to incorporate the data individuals generate every day into their health records is essential.
 
McGraw noted that some patients still intentionally withhold sensitive health information because they are concerned about it getting into the wrong hands. She called it "a myth" that EHRs hold complete records.
 
Further, Jonathan Hirsch -- founder and president of Syapse, a software company that allows health care providers to deploy precision medicine programs -- noted that because EHRs were built for billing and compliance, the systems are not able to handle genomic information or other big data.
 
Health care lags other industries when it comes to data analytics and interoperability, according to experts.
 
Kasarskis noted that he can use his smartphone to find out the menu and prices of all the restaurants within 20 miles, but "if a patient comes into Mount Sinai from another hospital, we may not know anything about them except what they've told us."
 
Citing his experience with a financial spinoff company, Colin Hill, CEO of GNS Healthcare, said he's seen big differences between the finance and health care industries. For example, he noted that financial data are "much cleaner" than health care data and that it is much faster and easier to validate or invalidate financial data.
 
Hill said there are a lot of regulatory forces and structures holding the health care industry back.
 
Misaligned Incentives
 
From keeping patients out of the hospitals to improving medication adherence to personalizing treatment, digital health has the potential to reduce costs, improve care quality and increase efficiency. However, under the current payment structure, health care providers are not incentivized to embrace digital health. In fact, doing so could hurt their bottom line.
 
Kasarskis said the progress made in other industries offers a "glimmer of hope" that the current barriers can be overcome and digital tools can be embraced with the "proper incentives."
 
Emanuel said that technology "can't fix everything," arguing for payment reform. He said the U.S. needs to move from a fee-for-service system to:
 - Bundled payments;
 - Two-sided risk; and
 - Capitation.
 
He said, "If we really want the digital medicine of the future, we're going to have to push hard on payment reform."


End of Document




Lauren Zalaznick to Leave NBCUniversal
hollywoodreporter.com
September 26, 2013 Thursday


Copyright 2013 VNU Business Media, Inc. All Rights Reserved


Section: NEWS; TAG
Length: 709 words
Byline: Lacey Rose
Body


Lauren Zalaznick is out at NBCUniversal.
The 12-year company veteran was a top cable operator at NBCU, overseeing a female-friendly portfolio of networks including Bravo, Oxygen and now-defunct Style until February of this year. At that time, her channels were absorbed by fellow cable exec Bonnie Hammer, and Zalaznick was moved into more of a strategic role. With NBCU executive vp as her title, she was charged with accelerating growth for the company portfolio by identifying and executing on new business opportunities.
STORY: Style Network to Take Over Style, Not G4
Many of her current NBCU responsibilities will be assumed by new hire Cesar Conde,who joined the company in mid-September as an executive VP reporting to Burke. (Exceptions include Fandango, which will be shifted to a film group overseen by recently promoted Jeff Shell.) Zalaznick will stay on to help transition her current portfolio and then will remain affiliated with the company as a consultant on digital media content and technology marketplace trends for a period of time.
In making his early 2013 announcement, which made Zalaznick more of a behind-the-scenes suit than the high-profile programmer she had been, Burke touted her as an exec who "loves ideas and new ways of doing things. She also is innovative, energetic and focused on growth." Over the past six months, her gig has entailed such things as establishing the company's first data science team and forming the NBCU Digital Council to focus on the impact of emerging global trends. 
Driven at least in part by fear of losing Zalaznick or Hammer following the Comcast/NBCU merger, Burke had elevated both execs to chairmen and split the cable portfolio between them. Zalaznick, who was often credited with cementing Bravo as a destination for upscale females, with juggernaut franchises including The Real Housewives and Top Chef, added Style, Telemundo and Mun2 to her purview.
She joined the company in 2004, after the acquisition of Vivendi Universal Entertainment, where she had been president of the TRIO network since February 2002. Zalaznick launched her TV career at VH1 in 1994, where her on-air credits include included Pop Up Video, Divas: Live and the VH1 Fashion Awards, after a stint in the independent film world.
STORY           : Jeff Shell Moving to Universal Studios, Adam Fogelson Out
Zalaznick's departure comes some three weeks after NBCUniversal made a series of high-profile executive changes. Universal Studios chairman Adam Fogelson was ousted as Comcast vet named Shell vice chairman, with former co-chair Donna Langley upped to chair and Ron Meyer named vice chairman of NBCU. Over on the TV side, the Style network got a surprise reboot (it's now the Esquire network), and the cable group reshuffled its execs. Jeff Wachtel and Frances Berwick added new networks to their portfolio, while former Oxygen president Jason Klarman was let go.
Below is the text of the memo Burke sent the staff.
I am writing to let you know that Lauren Zalaznick will be leaving NBCUniversal after 12 successful years working in a variety of leadership roles here.Those of you who know Lauren well know she is one of those rare executives that combine great creative instincts with a true business orientation. Her contributions to our company have been significant and far-reaching, from cable entertainment to Spanish-language broadcast, from digital initiatives to new marketing campaigns. Lauren also has been an important champion for many of our company-wide initiatives.Moving forward, most of Lauren's current group including Lori Conkling, Tony Cardinale, Nick Lehman and Lenore Moritz will report to Cesar Conde. Russell Hampton (Consumer Products) and Paul Yanover (Fandango) will report to Jeff Shell. John Shea and the Integrated Media group will report solely to Linda Yaccarino.Lauren will remain at the company to transition her current portfolio, and then consult with NBCUniversal on digital media content and technology marketplace trends.I want to thank Lauren for her years of service. Her drive to innovate, her energy and her intelligence have made our company a better place. She has been an important member of our team and we wish her well in the future.
E-mail: Lacey.Rose@THR.com Twitter: @LaceyVRose

Load-Date: October 11, 2013


End of Document




No Headline In Original
City News Service
December 10, 2014 Wednesday 2:41 PM PST


Copyright 2014 City News Service, Inc. All Rights Reserved
No City News Service material may be republished without the express written permission of the City News Service, Inc.
Length: 199 words
Dateline: SAN DIEGO
Body


A UC San Diego professor who uses music to understand the human brain won a $300,000 grant today from the University of California.
Professor Scott Makeig was one of five winners of the first President's Research Catalyst Awards, chosen from a pool of almost 200 proposals.
"The President's Research Catalyst Awards will spur UC research and offer our faculty and students new opportunities for cross-campus, multi- disciplinary collaboration," UC system President Janet Napolitano said. "We want to support research endeavors that have real-world impact in areas with critical needs."
Makeig's UC Music Experience Research Community Initiative brings together experts on music listening, performance, neuroscience, brain imaging and data science to understand the potential of music for health and cognition.
Other grantees will research how California's ecosystems will be affected by climate change, how to address a healthcare crisis in prison, use of quantum emulation to study the smallest matter in the universe, and tapping big data to look at questions of health, poverty and social justice.
UC officials said the application period for the next round of grants will begin during the winter.

Load-Date: December 11, 2014


End of Document




Producer Michael Shamberg Wants to "Invent the Future" With BuzzFeed Motion Pictures
hollywoodreporter.com
August 13, 2014 Wednesday


Copyright 2014 VNU Business Media, Inc. All Rights Reserved


Section: NEWS; TAG
Length: 1757 words
Byline: Natalie Jarvey
Body


How do GIFs and cat videos translate into feature-length films?
Michael Shamberg has joined BuzzFeed to figure that out. The viral content publisher has tapped the Pulp Fiction producer to advise its newly-created BuzzFeed Motion Pictureson projects that could eventually make their way to a television or big screen.
In an exclusive sit down with The Hollywood Reporter, he explains that it's the ability to test new kinds of content that has him excited about the role. "You work in film and TV and the business model is chiseled in stone," he adds. "Here, it's all an experiment to find out what the future is. And what's more exiting than feeling like you actually have a chance to invent the future?"
BFMP is part of a larger experiment at New York-based BuzzFeed to expand beyond its listicles into a media powerhouse worth $850 million, according to a report in the New York Times. Spurred on by a            $50 million cash infusion, CEO Jonah Peretti announced Aug. 11 that he would not only ramp up BuzzFeed's video operations but also grow its editorial team, branded content division and international presence.
BFMP will consist of three divisions: BuzzFeed Video for short-form content, BuzzFeed Live Development for mid-length serialized projects and Future of Fiction, for developing long-form film, television and transmedia efforts. The company plans to use its short- and mid-length videos to create characters, develop plots and test casting with online audiences. Projects with potential could move into full-length series or films.
"I don't think there's ever been a Hollywood R&D model like we have here," says Shamberg, who has been spending about two days a week at BuzzFeed's 100-plus person Hollywood campus.
BFMP is hiring in-house writers and building a sound stage with fixed sets to keep development costs down. But the studio also plans to invest in projects with established Hollywood players. "That's where Michael comes in," notes BFMP president Ze Frank.
But Shamberg will be more involved than just acting as a liaison to Hollywood. "We sit on opposite sides of the business, but it immediately became apparent that the way that Michael thinks about media overall is very experimental," says Frank, adding that Shamberg "has so much depth and insight into the traditional business but is also really interested in the ways that both development and distribution are changing right now."
Shamberg, whose credits include Django Unchained and Erin Brockovich, and Frank, a vlogging pioneer who joined BuzzFeed in 2012, sat down with The Hollywood Reporter on Monday to discuss the opportunity they see in digital video and explain why Hollywood is more risky than a startup.
Where did the idea for BuzzFeed Motion Pictures come from?
Frank: BuzzFeed Motion Pictures is a pretty multi-faceted entity. I think the best way to explain it is that the motion picture business is consumed differently than other web content, so it made sense to encapsulate that business. Michael has taken a very forward role, both in the conceptualization of this and its execution. As we start to use what we've learned in this content ecosystem, it really makes sense for us to collaborate with the very best people. That's where Michael comes in. He can broker relationships to people that have really perfected the craft of dialogue and character development, who spend their entire life thinking about genre and its implication.
Shamberg: When we develop stuff here, then we can upstream it to movies and TV. It's a real opportunity for talent and it's a real opportunity to complement the studio system.
Why would a creator take a project to BuzzFeed?
Shamberg: Talent is always looking to express themselves. The most successful writers, for example, maybe 5 percent of their scripts get produced. We're not going to be hearing pitches and saying "yes" or "no." Anybody who comes in, they'll get to try something. Right now, if you're writing scripts or you're looking at TV shows, yes you can make a living off of that. But for a fraction of your time, you can come in and experiment and have fun and discover new ways of doing stuff. And if that stuff becomes worth something, then you share it with us, you don't get down below somebody else's profit definition. You can have a director who's always had an idea come in and we can try it out. I don't think there's ever been a Hollywood R&D model like we have here.
What excites you creatively about the online video space?
Shamberg: I've had the most success in my career where something was both fresh and commercial. Here I think we can aim for both. We're going to do stuff that nobody has done before, but it's also going to be commercial.
The BuzzFeed Motion Pictures campus in Hollywood
What's the appeal for BuzzFeed about going long-form?
Frank: The different forms of content, whether it be length or format or genre, imply different areas to learn and grow. It's a different way of thinking about how you make that content and how you release it. We've grown to a size where there's a lot of really amazing possibilities to really advance on some of these forms. In the beginning, focusing on short-form was great because it taught us so much, but it's time to really look at the big picture.
What does a BuzzFeed movie or TV show look like?
Shamberg: There are a lot of things that you can incubate using this system. The stuff BuzzFeed's doing now with these really relatable characters is based on traits that people relate to. But that's a question you should be asking in three to six months.
Frank: We're not saying that we're going to reinvent the wheel in terms of the final end product. We're definitely going to be making some serious advancements in the process by which those end products are made, but I think it can really be anything. That's what I'm really excited about. We're committed to this kind of fixed model - putting everything under one roof, having production space here, having people in-house that are talented in post-production, editing and writing. We're going to have living sets. So some of the content is going to grow into that space and come out of that space. Some of it is going come out of the folks that we collaborate with and what their interests are.
What types of projects are you working on?
Frank: I don't think that we're really prepared to talk about it. I can tell you that we have already been doing a good amount of testing and learning in the short-form business, so a lot of the videos that people really love right now - including the ones around the Creep and moving back home with your parents - are part of a broader test. So we're doing casting within those, we're testing some initial premises, all in the service of fleshing out more complex series.
Shamberg: I'm now reaching out to filmmakers I've been lucky enough to work with and inviting them in and seeing what we can come up with. So that's kind of my starting point. I know great stuff's going to come in.
What's your budget for these projects?
Frank: Well the budgets are really determined by the business model. One of the things that's shifting right now, and one of the areas that we're really interested in working on, is that movies and television shows are almost like mini-startups at this point, and they usually have to manufacture most of their marketing in the opening launch. The development cycles are currently wasteful. We believe that we can help make some gains in bringing the overall cost down in development and in distribution, because obviously we have a big distribution. 
Shamberg: I think the cost structure will be pretty economical at the beginning, but with production value. And then if stuff's working we'll start to spend more for it. But the lower you keep the costs down, the more stuff you can try.
More than 100 employees work in the converted yoga studio
Venture backed startups are risky businesses, are you worried this may not be a successful business in a year or two?
Shamberg: Every time I make a movie I believe it's going to be a hit. I've been right and I've been wrong. But you go into it with the passion thinking it's going to work.
Frank: I don't think there's a riskier venture backed business than Hollywood. Hollywood puts us to shame. People's careers live or die by a single decision. Michael's been through the ringer.
Shamberg: Opportunity and risk go hand-in-hand. So if you're not willing to take the risk then you really have to forego the opportunity. Frankly, at this point in my career it's not even something I think about. I don't like to lose money for studios. I love making money for them and myself, but you have to go into it feeling creatively that you've done your best.
How do you envision these Hollywood collaborations working?
Shamberg: We'd agree that we're going to make some version [of what they write.] They wouldn't be writing it on spec. I want to get out of the pitching/rejection model. At this point in our growth if we're just rejectors we're going to shut ourselves down. We're willing to take chances. Anybody who wants to do an idea we've agreed upon, or they'll bring us an idea, we're pretty much saying we'll do what you want.
Frank: In addition to that we're going to be bringing a good amount of writers on staff. A lot of times experimentation can sound like it has a degree of randomness to it. But the way that we work here is that we really move down lines of assumption and keep testing it. The same thing has to be true as we start thinking about character development and genre development. That part of it is particularly exciting to me.
How does BuzzFeed Motion Pictures fit into the larger BuzzFeed organization?
Frank: BuzzFeed has a very robust data science development products team. The products that our producers use to understand the success of their video and what people are saying about their content, that's all in very tight collaboration with our technology teams both here in the office but also in New York. I mentioned the outward-facing residency but we also have an internal-facing residency. So our editors from lifestyle and entertainment and news come here and do residencies within our teams and we learn a ton from the existing company.
How much autonomy do you have?
Frank: It is pretty autonomous. We run these like startups. Jonah has been very committed early on to try to make sure that people can grow, grow quickly and can make decisions on their own and don't get too mired in bureaucratic decision making.
Email: Natalie.Jarvey@THR.comTwitter:            @NatJarv

Load-Date: August 13, 2014


End of Document




WME | IMG Launches "Internet of Things" Joint Venture With AGT International
hollywoodreporter.com
October 22, 2015 Thursday


Copyright 2015 Prometheus Global Media, LLC All Rights Reserved


Section: NEWS; TAG
Length: 537 words
Byline: Rebecca Sun
Body


WME | IMG has formed a joint venture with AGT International, a company that specializes in developing "Internet of Things" technology and delivering big data analytics, The Hollywood Reporter has learned.
The Internet of Things, or IoT, is technology that connects household and everyday objects online in order to collect data and enhance user experience. Perhaps the most well-known current application of IoT technology is Nest Labs' smart thermostats, smoke alarms and home security cameras, which are outfitted with an array of sensors that can be accessed remotely via mobile phone.
Read MoreCES: Samsung CEO Heralds the Rise of the Internet of Things
AGT will help develop an IoT platform for WME | IMG's vast portfolio of more than 800 events, which include concerts as well as Wimbledon and Fashion Weeks around the world. The joint venture launched this week at Toronto Fashion Week with a small test sampling of volunteer models, designers and WME | IMG employees. The models are wearing biometric sensors and cameras that allow users to experience the catwalk from a first-person perspective. Meanwhile, employees in the audience are outfitted with their own biometric sensors that send real-time data to the designers about their physical reactions to each piece in their collections.
After a few more test runs at select events in the U.S. and Europe over the next couple of months, the joint venture will embark on a more comprehensive rollout next year. Soon, "smart" venues will be able to communicate with attendees to tell them the location of the nearest bathroom with the shortest line, or to send photos taken on venue-equipped cameras directly to attendees via a permission-based event app.
For WME | IMG clients, event organizers will be able to monitor user experience in real-time, allowing them to respond and adapt quickly as needs arise. And with enough attendee opt-ins, artists and organizers alike will have access to deeper and more detailed consumer feedback.
"Consumer and entertainment companies have been collecting insights for years, but very few of our clients and partners have had access to analytics that allow them to act on those insights to deliver relevant content and better experiences for their audiences," WME | IMG co-CEO Ari Emanuel said in a statement. "Our partnership with AGT marks a paradigm shift, recognizing that IoT technology and data science will fundamentally change the way people experience live events, as well as how they consumer and share content that matters to them."
The joint venture will be headquartered in New York, which also will host a research and data center in addition to the Switzerland-based AGT's existing R&D sites in Silicon Valley, Germany and Israel.
"This exciting new venture is centered on the consumer experience, creating true Social IoT," AGT CEO Mati Kochavi said in a statement. "There is no greater single source of sports and entertainment content and live events than WME | IMG. Layering our technology across their impressive event portfolio will produce a new level of engagement among event-goers, talent, brands and even fans at home. It's a meaningful connection that is meant to start before an event and extend well past it."

Load-Date: October 22, 2015


End of Document







| About LexisNexis | Privacy Policy | Terms & Conditions | Copyright © 2021 LexisNexis 






Page  of Can vaccination and infection rates add up to reach COVID herd immunity?


 


 



Page  of USC Pharmacy School Receives $5 Million To Improve Prescription Safety


 


 



Page  of  Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. iHealthBeatSeizures and Mosquitoes: The Rewards of Working Sma....


 


 



Page  of LOWER FEES, BETTER TECH?; NEW VOICES ARE VYING FOR SPACE IN THE FOOD-DELIVERY WORLD


 


 



Page  of How to erase the digital divide


 


 



Page  of CORONAVIRUS IN CALIFORNIA; We don't know the half of it; Did 50% of L.A. residents catch COVID-19? It all depends.


 


 



Page  of UCSD Researchers Use Map To Reveal How Resources Are Distributed In The Brain


 


 



Page  of CITY BEAT; A tail-wagging tale of man and corgi


 


 



Page  of D.A.'s New Investigative Chief Sworn In


 


 



Page  of Is it time to kill calculus?


 


 



Page  of Is it time to kill calculus?


 


 



Page  of Is it time to kill calculus?


 


 



Page  of In the West, wildfire smoke accounts for more pollution; Air quality declines again after years of steady gains, study finds


 


 



Page  of County Reports 56 Deaths, Hospitalizations Surge As COVID-19 Variant Spreads


 


 



Page  of SD County Reports 3,815 new COVID-19 Infections As Hospitalizations Increase


 


 



Page  of San Diego Foundation Announced $750,000 in STEM Grants for Minority Students


 


 



Page  of Nielsen Sets Timetable for Cross-Platform Media Measurement


 


 



Page  of Learn to expertly work with data and earn an in-demand job


 


 



Page  of Back to the drawing board, maps


 


 



Page  of Garcetti Announces Deputy Mayor to Lead Office of Budget and Innovation


 


 



Page  of Asian Americans split on affirmative action at UC; Proposition 16 divides university system's most overrepresented group of students.


 


 



Page  of            Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved.     California Healthline'It's Science, Stupid': A S....


 


 



Page  of Studies Link Viewership of Fox News to Reduced Pandemic Precaution


 


 



Page  of NASA Awards Los Angeles $1.3 Million for Air Quality Monitoring Program


 


 



Page  of Super Aggregator ScreenHits TV Closes $2 Million in Funding


 


 



Page  of There is hard data that shows "Bernie Bros" are a myth


 


 



Page  of Biden campaign adds more staff in Texas


 


 



Page  of CDC softened school reopening guidelines criticized in fringe group's letter to President Trump


 


 



Page  of CDC softened school reopening guidelines criticized in fringe group's letter to President Trump


 


 



Page  of Student debt and the end of the liberal arts dream


 


 



Page  of Student debt and the end of the liberal arts dream


 


 



Page  of L.A Times Endorses Biden


 


 



Page  of New Measurement of Home Condition Without Inspections - Pomar Lane Completes Condition Scores for 90,000 Homes


 


 



Page  of            Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved.     California HealthlineAs Georgia Reopened, Offici....


 


 



Page  of Andrew Cuomo saw COVID-19's threat to nursing homes - yet he still risked adding to it


 


 



Page  of Mathematicians urge peers to stop working on racist "predictive policing" technology


 


 



Page  of            Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved.     California HealthlineCoronavirus Tests The Value....


 


 



Page  of Cal State LA researchers use data visualization, AI in fight against COVID-19


 


 



Page  of Statistic of the decade: The massive deforestation of the Amazon


 


 



Page  of Uber's data revealed nearly 6,000 sexual assaults. Does that mean it's not safe?


 


 



Page  of QAnon conspiracy theories about the coronavirus pandemic are a public health threat


 


 



Page  of UC Health Launches Online Dashboard of COVID-19 Cases,Testing at its Hospitals


 


 



Page  of UC Health Launches Online Dashboard of COVID-19 Cases,Testing at its Hospitals


 


 



Page  of            Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved.     California HealthlineNo Safety Switch: How Lax O....


 


 



Page  of Berkeley Coding Academy Launches 2020 Summer Camp in Machine Learning and A.I.


 


 



Page  of In YouTube "edutainment," minimal control for scientific accuracy


 


 



Page  of The census goes digital - 3 things to know


 


 



Page  of 200 years before Orwell, a German naturalist prophesied surveillance capitalism


 


 



Page  of Facebook CEO Mark Zuckerberg made staff recommendations to Pete Buttigieg's presidential campaign


 


 



Page  of Film Diversity Helps Drive Box Office Hits, Study Shows


 


 



Page  of Roku to Acquire Video Ad Platform dataxu for $150 Million


 


 



Page  of CBS and Viacom Chiefs' Memos to Staff Hint at Changes Ahead


 


 



Page  of Researchers Identifies Genes that Increase Autism Risk


 


 



Page  of You'd be better off lighting your money on fire than giving it to a politician to spend on TV ads


 


 



Page  of  Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. California HealthlineWalmart Charts New Course By Steering Wor....


 


 



Page  of Garcetti Heads to New Jersey to Promote Nonprofit Partnership


 


 



Page  of  Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. California HealthlineExtreme Temperatures May Pose Risks To So....


 


 



Page  of  Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. California HealthlineExtreme Temperatures May Pose Risks To So....


 


 



Page  of  Copyright © Advisory Board Company & California Healthcare Foundation 2015, All Rights Reserved. California HealthlineWhy The U.S. Remains The World's Most Exp....


 


 



Page  of UCSD Professor Elected to the National Academy of Medicine


 


 



Page  of Cloudvirga Earns a 2018 Benzinga FinTech Award


 


 



Page  of Has the Opioid Epidemic Begun to Turn a Corner?


 


 



Page  of The Whistle-Blower Behind the Facebook Data Scandal Has a Surprising Fashion Background


 


 



Page  of Tribeca: 'Enhanced' Directors Discuss Sports Science's Complicated Future


 


 



Page  of Hillary Clinton -- Endless Sore Loser


 


 



Page  of No Headline In Original


 


 



Page  of Opinion: Take the Dictator Quiz


 


 



Page  of  Copyright © Advisory Board Company & California Healthcare Foundation 2014, All Rights Reserved. iHealthBeatCan the U.S. Health Care System Realize the Promise....


 


 



Page  of Lauren Zalaznick to Leave NBCUniversal


 


 



Page  of No Headline In Original


 


 



Page  of Producer Michael Shamberg Wants to "Invent the Future" With BuzzFeed Motion Pictures


 


 



Page  of WME | IMG Launches "Internet of Things" Joint Venture With AGT International


 Artificial Intelligence May Help Achieve UN's Sustainable Development Goals
Eurasia Review
February 20, 2021 Saturday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 805 words
Body


Scientists from the Andalusian Research Institute in Data Science and Computational Intelligence, or DaSCI (University of Granada), together with the private company Ferrovial and the Spanish Royal Academy of Engineering, highlight the need for unified, accessible, and open data in developing projects to address many of the challenges of the UN's Sustainable Development Goals
Scientists from the Andalusian Research Institute in Data Science and Computational Intelligence, or DaSCI (University of Granada), together with the private company Ferrovial and the Spanish Royal Academy of Engineering (RAI), have conducted a study to analyse how engineering and technological solutions strongly linked to artificial intelligence (AI) can positively contribute to the 17 Sustainable Development Goals (SDGs) set by the United Nations (UN).
To protect the planet and ensure prosperity for all people, the UN established these 17 SDGs as part of its 2030 Agenda, which constitute a paradigm shift for companies and governments in the design of new business models and public policies based on sustainability. Governments, the private sector, and civil society all play an important role in achieving the goals.
The project, entitled "Engineering as a Facilitator of SDGs: Artificial intelligence and disruptive digital technologies", began in March 2020, focusing specifically on the study of AI and digital technologies and how these might be applied to further progress toward the 17 SDGs. The research is organised into three facets that broadly correspond to (i) an introduction to AI and digital technologies, (ii) analysis of their application to the SDGs, and (iii) recommendations for action that can help develop projects and support the achievement of associated goals. As part of this research, the authors reviewed the specialised scientific literature, including over a thousand bibliographical references relevant to the 169 targets that have been set to achieve the SDGs.
This work makes an important contribution to our understanding of the analytical capacity of engineering--under the umbrella of AI and digitalisation in support of the SDGs--and to addressing the challenges faced by the world economy and society in the 21st Century. It also provides insights into the three dimensions that characterise sustainability: the economic (including the different areas of life and economic and technological development); the social (including social development and equality); and the environmental (including resources and the environment).
One of the conclusions of this work is that data constitute the common element on which AI and digital technologies are based. Here, it is important to highlight the need for data that are unified, accessible, and open, as this supports the development of projects designed to address many of the global challenges. Governments and companies must converge toward this objective by generating and sharing data that allow them to successfully take on projects and design solutions to address the SDGs.
The researchers point out that it is imperative to strengthen the links between science and engineering, industry and governments, to reinforce dialogue and expand the different avenues toward achieving high-quality data.
Global targets
"The SDGs set targets to be achieved at the global level, but not all countries and regions of the world are currently in the same position in this race to achieve them. So the application of AI and digital technologies must obviously be adapted to the situation of each country and targeted at the most pressing SDGs," the authors explain.
Digital technologies are advancing at a rapid pace, which means it is important to look for alternative ways to measure the achievement of the SDGs--ways that are adapted to this accelerated pace of progress and the emergence of new digital paradigms. This is especially important given the current world scenario caused by the COVID-19 pandemic, which has undoubtedly had a profound impact on all dimensions of the SDGs, far beyond the strictly health-related aspect.
AI and digital technologies are fundamental tools for travelling the path we have to navigate during this decade, as we carry heavy moral and ethical responsibility toward today's world. Working toward the 17 SDGs is both a great opportunity and a major challenge.
A new book about the study, featuring original illustrations by Pablo García-Moral, is now available, which explores the latest thinking on this issue from those involved in the project. It was written by a team of 16 authors, coordinated by Rosana Montes (UGR), Francisco Herrera (UGR and RAI), Javier Pérez de Vargas (RAI), and Rosario Marchena (Ferrovial).
The article Artificial Intelligence May Help Achieve UN's Sustainable Development Goals appeared first on Eurasia Review.

Load-Date: February 19, 2021


End of Document




Study Claims Reparations For Slavery Could Have Reduced COVID-19 Infections And Deaths In US
Eurasia Review
February 12, 2021 Friday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 2526 words
Body


New study suggests monetary reparations for Black descendants of people enslaved in the United States could have cut SARS-CoV-2 transmission and COVID-19 rates both among Black individuals and the population at large.
Researchers modeled the impact of structural racism on viral transmission and disease impact in the state of Louisiana.
The higher burden of SARS-CoV-2 infection among Black people also amplified the virus's spread in the wider population.
Reparations could have reduced SARS-CoV-2 transmission in the overall population by as much as 68 percent.
Compared with white people, Black individuals in the United States are more likely to be infected with SARS-CoV-2, more likely to end up in the hospital with COVID-19, and more likely to die from the disease.
Civil rights activists have long called for monetary reparations to the Black descendants of Africans enslaved in the United States as a financial, moral, and ethical form of restitution for the injustices of slavery.
Now, a study led by Harvard Medical School researchers suggests reparations could also have surprising public health benefits for Black individuals and the entire nation.
To estimate the impact of structural inequities between Black and white individuals, the researchers set out to capture the effect of reparation payments on the Black-white wealth gap in the state of Louisiana.
Their analysis, published online inSocial Science & Medicine, suggests that if reparations had been made before the COVID-19 pandemic, transmission of SARS-CoV-2 in the state's overall population could have been reduced by anywhere from 31 percent to 68 percent.
The work was done in collaboration with the Lancet Commission on Reparations and Redistributive Justice.
"While there are compelling moral and historical arguments for racial-injustice interventions such as reparations, our study demonstrates that repairing the damage caused by the legacy of slavery and Jim Crow racism would have enormous benefits to the entire population of the United States," said study senior author Eugene Richardson, assistant professor of global health and social medicine in the Blavatnik Institute at Harvard Medical School.
The disproportionate effects of COVID-19 on racial minorities--Black individuals in particular--have been well documented. Black people get COVID-19 at a rate nearly one and a half times higher than that of white people, are hospitalized at a rate nearly four times higher, and are three times as likely to die from the disease, according to the latest estimates from the U.S. Centers for Disease Control.
The greater disease burden among Black people has caused tremendous loss of life and unspeakable suffering across these already vulnerable and disadvantaged communities. Notably, these effects have also spilled over and are driving transmission rates of the virus in the overall population, the study authors said.
Addressing the structural inequalities at the roots of this disparity through monetary reparations would not only radically decrease the impact of COVID-19 among the people who received reparations, the authors said, but would reduce the overall toll of the disease on a broader scale, benefiting the entire population.
The findings, the researchers said, powerfully underscores the truly global nature of the pandemic and the notion that a society is only as strong as its most vulnerable members.
"If we extrapolate these results to the entire United States, we can imagine that tens or hundreds of thousands of lives would have been spared, and the entire nation would have been saved much of the hardship it has endured in the last year," said Richardson, who is also the chair of the Lancet Commission on Reparations and Redistributive Justice.
For their analysis, the researchers paired sophisticated data analytics and computational tools with commonly used epidemiologic modeling methods to calculate the impact of structural racism on infection rates among Black and white populations in Louisiana. They chose Louisiana as an exemplar of the impacts of structural racism in the U.S. because it was one of the few states that reported infection rates by race in the early stages of the pandemic. For a control group, the researchers chose the relatively egalitarian population of South Korea.
The researchers noted that although modeling is used to understand many factors in the spread of an infectious disease, such as differences in infection risk based on whether passengers on a train sit with windows open or closed or individual variations in mask-wearing habits, it has rarely been used to capture the effects of social factors that can create vast disparities between populations, such as those seen between Blacks and whites in the U.S.
Richardson's recent book Epidemic Illusions explores the ways conventional epidemiology is constrained from proposing solutions that address the root causes of health disparities derived from the combined weight of centuries of racism, imperialism, neoliberal politics, and economic exploitation. One of the goals of the paper is to challenge the narrow ways people who work in medicine and public health measure and think about problems and solutions and to broaden the public imagination, thus opening new conversations about what challenges and opportunities are worth considering in global health and social science, Richardson said.
The study examined the initial period of the outbreak, before infection control measures were implemented, so any differences in infection rates between populations at that time would have been driven mainly by differences in the social structures, the researchers said.
For example, Louisiana has a population heavily segregated by race, with Black people having higher levels of overcrowded housing and working jobs that are more likely to expose them to SARS-CoV-2 than white people. In comparison, South Korea has a more homogenous population with far less segregation.
To probe how such structural inequities impact transmission of SARS-CoV-2, the researchers examined infection rates over time for the first two months of the epidemic in each location. During the initial phase of the outbreak in Louisiana, each infected person spread the virus to1.3 to 2.5 more people than an infected individual during the same phase of the outbreak in South Korea, the analysis showed. The study also showed it took Louisiana more than twice as long to bring the early wave of the epidemic under control as South Korea.
Next, the researchers used next-generation matrices to gauge how overcrowding, segregation, and the wealth gap between Blacks and whites in Louisiana could have driven higher infection rates and how monetary reparations would affect viral transmission.
The model showed that greater equity between Blacks and whites might have reduced infection transmission rates by anywhere from 31 percent to 68 percent for every person in the state.
This research comes at a time when many Americans are already thinking about the larger societal costs of structural racism, the researchers said. They noted, for example, that the nationwide movement to protest police brutality against Black people has been fueled by many of the inequitable outcomes exemplified so painfully by the coronavirus pandemic in the U.S.
"This moment has made it possible for a lot of people who had no reason to think about these inequalities to be very aware of them," said study co-author and Lancet reparations commissioner Kirsten Mullen, who was a member of concept development team for the National Museum of African American History and Culture.
Anti-racism in action
Richardson said that the research was designed to explore how reparations payments might have altered the trajectory of the coronavirus pandemic in the U.S. and how a different response to the disease could have helped mitigate the disparities fueled by social conditions that are vestiges of slavery. Such conditions, Richardson noted, include ongoing discrimination and structural racism in the form of redlining, overcrowding, over-incarceration, and the heightened use of lethal force in policing experienced by Black people.
Richardson said that historian and anti-racist scholar Ibram X. Kendi's description of the differences between racism and anti-racism were helpful in designing the study. According to Kendi, a racist policy is any policy that produces or sustains inequality or promotes the power of one racial group over another, whereas an anti-racist policy is any measure that produces or sustains equity between racial groups.
Richardson said that one important goal of the project was to attempt to harness the power of mathematical modeling for an anti-racist response to the coronavirus and beyond.
"When you look at a formula for transmissibility, it looks like an objective calculation," he said. "But where is lethal policing in that formula?"
Richardson noted that it was important to call attention to the systemic and structural elements of racism that can get lost in simplified models of disease.
What are reparations?
Mullen and study co-author William Darity, who recently published a book on reparations and have written in the press about the case for using reparation payments to fight COVID-19, defined reparations as a program of acknowledgement, redress, and closure for a grievous injustice. In this case, Mullen said, the atrocities are associated with periods of enslavement, legal segregation and white terrorism during the Jim Crow era, and racial strife and violence of the post-Civil Rights Act era, including ongoing inequities in the form of over-policing, police executions of unarmed Black people, ongoing discrimination in regard to incarceration, access to housing, and, possibly most important, the Black-white gulf in wealth.
Successful reparations programs include three elements: admission of culpability on behalf of the perpetrators of the atrocity; redress, in the form of an act of restitution; and closure, wherein the victims agree that the debt is paid and no further claims are to be made unless new harms are inflicted.
In this case, Mullen said, reparations would take the form of financial restitution for living Black individuals who can show that they are descended from at least one ancestor who was enslaved in the U.S. and that they self-identified as Black on a legal document at some point during the 12 years prior.
The financial restitution is designed to help close the Black-white wealth gap. Darity noted that it is important to distinguish wealth from income. Wealth is how much you own, and income is how much you earn. Greater wealth translates to greater stability for individuals and families across time. Greater wealth is also more strongly associated with greater well-being than greater income, Darity said, and disparities in wealth manifest as health disparities.
"Wealth is more strongly associated with familial or individual well-being," said Darity, who is the Samuel DuBois Cook Distinguished Professor of Public Policy at Duke University and a Lancet reparations commissioner. He noted that, according to the Federal Reserve Board 2016 Survey of Consumer Finances, the average Black household had a net worth $800,000 lower than the average white household, and that Black people, who represent 13 percent of the U.S. population, only own 3 percent of the nation's wealth.
"This dramatically restricts the ability of Black Americans to survive and thrive," Darity said.
To assess the effect of reparation payments on the trajectory of the pandemic, the researchers based their calculations on a model that would pay $250,000 per person or $800,000 per household to descendants of enslaved individuals--one of several proposed reparation models.
Every transmission is a social transmission
"Every transmission has a social cause," said study co-author and Lancet reparations commissioner James Jones, associate professor of Earth System Science and a senior fellow at the Woods Institute for the Environment at Stanford University.
For a brief moment when AIDS was in the spotlight during the late 80s and early 90s, people interested in social behavior became interested in mathematical modeling of disease, Jones said. While that interest largely waned, the COVID-19 crisis has highlighted the need to think about social science, inequality, social structure, behavior patterns, and behavior change, as well as how they fit together with how we understand and respond to epidemics, Jones said.
Even the simplest model must account for a rudimentary social structure, Jones said. At its most basic, this can be represented with a generalized estimate of how likely an infected person is to come into contact with a susceptible person. He explained that this number, R0 or "R-naught," is the average number of people an infected individual transmits the virus to. When R0 is less than one, no epidemic is possible because the number of people infected decreases. When R0 is greater than 1 an epidemic is possible. R0 also determines the total number of people who could potentially become infected or how many people would need to be vaccinated to end the epidemic. It can also be used to calculate the so-called endemic equilibrium--which determines whether a disease will continue to exist within a population, simmering constantly in the background or bubbling up seasonally, like influenza.
"That's the theory of infectious disease control in a single parameter," Jones said.
That seeming simplicity can make it hard to focus on the complex ways that infectious diseases move through the real world, the researchers said.
"It's important to highlight that R0 is not simply a function of the pathogen," Jones said. "It's a function of the society." Social and environmental factors like mobility, segregation, and the nature of the built environment help determine rates of infection, he said.
This is one important reason that diseases don't hit all people the same. Global R0 is an average of very different R0s for different groups of people. Some groups are more likely to interact only with members of their own group, some groups are more likely to come in contact with infected people, and some are more susceptible to the disease for other reasons, Jones said.
In this case, the researchers used mathematical models to help understand the differences in R0 for Black people and white people in Louisiana and to help think about how things would change if racism were less prevalent in America.
Absent those interventions, the researchers noted that Black Americans remain at an elevated and inequitable risk of becoming infected and dying during the COVID-19 pandemic and that this inequity will continue to fuel the pandemic for all Americans.
"Increasing equality would have huge benefits on infection rates for everyone," said co-author Momin Malik, who was a data science postdoctoral fellow at the Berkman Klein Center for Internet & Society at Harvard University at the time the study was conducted.
The article Study Claims Reparations For Slavery Could Have Reduced COVID-19 Infections And Deaths In US appeared first on Eurasia Review.

Load-Date: February 12, 2021


End of Document




Global Ice Loss Increases At Record Rate
Eurasia Review
January 26, 2021 Tuesday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 862 words
Body


The rate at which ice is disappearing across the planet is speeding up, according to new research.
And the findings also reveal that the Earth lost 28 trillion tonnes of ice between 1994 and 2017 - equivalent to a sheet of ice 100 metres thick covering the whole of the UK.
The figures have been published by a research team which is the first to carry out a survey of global ice loss using satellite data.
The team, led by the University of Leeds, found that the rate of ice loss from the Earth has increased markedly within the past three decades, from 0.8 trillion tons per year in the 1990s to 1.3 trillion tons per year by 2017.
Ice melt across the globe raises sea levels, increases the risk of flooding to coastal communities, and threatens to wipe out natural habitats which wildlife depend on.
The findings of the research team, which includes the University of Edinburgh, University College London and data science specialists Earthwave, are published in European Geosciences Union's journalThe Cryosphere.
The research, funded by UK Natural Environment Research Council, shows that overall, there has been a 65 % increase in the rate of ice loss over the 23-year survey. This has been mainly driven by steep rises in losses from the polar ice sheets in Antarctica and Greenland.
Lead author Dr Thomas Slater, a Research Fellow at Leeds' Centre for Polar Observation and Modelling , said: "Although every region we studied lost ice, losses from the Antarctic and Greenland ice sheets have accelerated the most.
"The ice sheets are now following the worst-case climate warming scenarios set out by the Intergovernmental Panel on Climate Change. Sea-level rise on this scale will have very serious impacts on coastal communities this century."
Dr Slater said the study was the first of its kind to examine all the ice that is disappearing on Earth, using satellite observations .
He added: "Over the past three decades there's been a huge international effort to understand what's happening to individual components in Earth's ice system, revolutionised by satellites which allow us to routinely monitor the vast and inhospitable regions where ice can be found.
"Our study is the first to combine these efforts and look at all the ice that is being lost from the entire planet."
The increase in ice loss has been triggered by warming of the atmosphere and oceans, which have warmed by 0.26°C and 0.12°C per decade since the 1980, respectively. The majority of all ice loss was driven by atmospheric melting (68 %), with the remaining losses (32%) being driven by oceanic melting.
The survey covers 215,000 mountain glaciers spread around the planet, the polar ice sheets in Greenland and Antarctica, the ice shelves floating around Antarctica, and sea ice drifting in the Arctic and Southern Oceans.
Rising atmospheric temperatures have been the main driver of the decline in Arctic sea ice and mountain glaciers across the globe, while rising ocean temperatures have increased the melting of the Antarctic ice sheet. For the Greenland ice sheet and Antarctic ice shelves, ice losses have been triggered by a combination of rising ocean and atmospheric temperatures.
During the survey period, every category lost ice, but the biggest losses were from Arctic Sea ice (7.6 trillion tons) and Antarctic ice shelves (6.5 trillion tons), both of which float on the polar oceans.
Dr Isobel Lawrence, a Research Fellow at Leeds' Centre for Polar Observation and Modelling, said: "Sea ice loss doesn't contribute directly to sea level rise but it does have an indirect influence. One of the key roles of Arctic sea ice is to reflect solar radiation back into space which helps keep the Arctic cool.
"As the sea ice shrinks, more solar energy is being absorbed by the oceans and atmosphere, causing the Arctic to warm faster than anywhere else on the planet.
"Not only is this speeding up sea ice melt, it's also exacerbating the melting of glaciers and ice sheets which causes sea levels to rise."
Half of all losses were from ice on land - including 6.1 trillion tons from mountain glaciers, 3.8 trillion tons from the Greenland ice sheet, and 2.5 trillion tons from the Antarctic ice sheet. These losses have raised global sea levels by 35 millimetres.
It is estimated that for every centimetre of sea level rise, approximately a million people are in danger of being displaced from low-lying homelands.
Despite storing only 1 % of the Earth's total ice volume, glaciers have contributed to almost a quarter of the global ice losses over the study period, with all glacier regions around the world losing ice.
Report co-author and PhD researcher Inès Otosaka, also from Leeds' Centre for Polar Observation and Modelling, said: "As well as contributing to global mean sea level rise, mountain glaciers are also critical as a freshwater resource for local communities.
"The retreat of glaciers around the world is therefore of crucial importance at both local and global scales."
Just over half (58 %) of the ice loss was from the northern hemisphere, and the remainder (42 %) was from the southern hemisphere.
The article Global Ice Loss Increases At Record Rate appeared first on Eurasia Review.

Load-Date: January 26, 2021


End of Document




Forecasting Coastal Water Quality
Eurasia Review
January 24, 2021 Sunday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 806 words
Body


Less than two days of water quality sampling at local beaches may be all that's needed to reduce illnesses among millions of beachgoers every year due to contaminated water, according to new Stanford research.
The study, published inEnvironmental Science & Technology, presents a modeling framework that dependably predicts water quality at beaches after only a day or two of frequent water sampling. The approach, tested in California, could be used to keep tabs on otherwise unmonitored coastal areas, which is key to protecting the well-being of beachgoers and thriving ocean economies worldwide.
"This work combines knowledge of microbiology, coastal processes and data science to produce a tool to effectively manage one of our most precious resources and protect human health," said senior author Alexandria Boehm, a Stanford professor of civil and environmental engineering.
Measuring concentrations of fecal indicator bacteria (FIB) - which denote the presence of fecal matter and can lead to unsafe water conditions - at beaches ensures the health and safety of the public. While all ocean water contains some degree of pathogens, such as bacteria or viruses, they're typically diluted to harmless concentrations. However, changes in rainfall, water temperature, wind, runoff, boating waste, storm sewer overflow, proximity to waste treatment plants, animals and waterfowl can lead to an influx of water contamination. Exposure to these contaminants can cause many ailments, including respiratory diseases and gastrointestinal illnesses, along with skin, eye and ear infections to swimmers.
Protecting coastal waters and the people that use them remains essential for much of California's 840 miles of coastline. Over 150 million people swim, surf, dive and play at one of the state's 450 beaches annually, generating over $10 billion in revenue. According to the California State Water Resources Control Board, health agencies across 17 counties, publicly owned sewage treatment plants, environmental groups and several citizen-science groups perform water sampling across the state. However, not all waters are routinely checked due to accessibility issues, budget resource constraints or the season, despite their use by the public.
Another obstacle to safeguarding public health lies in the lag time between sampling and results - up to two-days - leading beach managers to make decisions reflecting past water quality conditions. When monitored waters contain high levels of bacteria and pose a health risk, beach managers post warning signs or close beaches. The delay in current testing methods could unknowingly expose swimmers to unhealthy waters.
To overcome these limitations, the researchers combined water sampling and environmental data with machine learning methods to accurately forecast water quality. While predictive water quality models aren't new, they have generally required historical data spanning several years to be developed.
The team used water samples collected at 10-minute intervals over a relatively brief timeframe of one to two days at beaches in Santa Cruz, Monterey and Huntington Beach. Among the three sites, 244 samples were measured for FIB concentrations and marked as above or below the acceptable level deemed safe by the state. The researchers then collected meteorological data such as air temperature, solar radiation and wind speed along with oceanographic data including tide level, wave heights and water temperature (all factors influencing FIB concentrations) over the same timeframe.
Using the high-frequency water quality data and machine learning methods, they trained computer models to accurately predict FIB concentrations at all three beaches. The researchers found hourly water sampling for 24 hours straight - capturing an entire tidal and solar cycle - proved enough for reliable results. Feeding the framework meteorological and tidal data from longer time periods resulted in future water quality predictions that were dependable for at least an entire season.
"These results are really empowering for communities who want to know what's going on with water quality at their beach," Searcy said. "With some resources to get started and a day of sampling, these communities could collect the data needed to initiate their own water quality modeling systems."
The framework code, which is publicly accessible, could also be developed for accurate predictions of other contaminants such as harmful algae, metals and nutrients known to wreak havoc on local waters. The researchers point out that more analysis is needed to better determine the exact timeframe these models remain accurate and note that continually assessing and retraining the models remains a best practice for accurate predictions.
The article Forecasting Coastal Water Quality appeared first on Eurasia Review.

Load-Date: January 24, 2021


End of Document




Climate Change To Alter Position Of Earth's Tropical Rain Belt
Eurasia Review
January 19, 2021 Tuesday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 651 words
Body


Future climate change will cause a regionally uneven shifting of the tropical rain belt - a narrow band of heavy precipitation near the equator - according to researchers at the University of California, Irvine and other institutions. This development may threaten food security for billions of people.
In a study published in Nature Climate Change, the interdisciplinary team of environmental engineers, Earth system scientists and data science experts stressed that not all parts of the tropics will be affected equally. For instance, the rain belt will move north in parts of the Eastern Hemisphere but will move south in areas in the Western Hemisphere.
According to the study, a northward shift of the tropical rain belt over the eastern Africa and the Indian Ocean will result in future increases of drought stress in southeastern Africa and Madagascar, in addition to intensified flooding in southern India. A southward creeping of the rain belt over the eastern Pacific Ocean and Atlantic Ocean will cause greater drought stress in Central America.
"Our work shows that climate change will cause the position of Earth's tropical rain belt to move in opposite directions in two longitudinal sectors that cover almost two thirds of the globe, a process that will have cascading effects on water availability and food production around the world," said lead author Antonios Mamalakis, who recently received a Ph.D. in civil & environmental engineering in the Henry Samueli School of Engineering at UCI and is currently a postdoctoral fellow in the Department of Atmospheric Science at Colorado State University.
The team made the assessment by examining computer simulations from 27 state-of-the-art climate models and measuring the tropical rain belt's response to a future scenario in which greenhouse gas emissions continue to rise through the end of the current century.
Mamalakis said the sweeping shift detected in his work was disguised in previous modelling studies that provided a global average of the influence of climate change on the tropical rain belt. Only by isolating the response in the Eastern and Western Hemisphere zones was his team able to highlight the drastic alterations to come over future decades.
Co-author James Randerson, UCI's Ralph J. & Carol M. Cicerone Chair in Earth System Science, explained that climate change causes the atmosphere to heat up by different amounts over Asia and the North Atlantic Ocean.
"In Asia, projected reductions in aerosol emissions, glacier melting in the Himalayas and loss of snow cover in northern areas brought on by climate change will cause the atmosphere to heat up faster than in other regions," he said. "We know that the rain belt shifts toward this heating, and that its northward movement in the Eastern Hemisphere is consistent with these expected impacts of climate change."
He added that the weakening of the Gulf Stream current and deep-water formation in the North Atlantic is likely to have the opposite effect, causing a southward shift in the tropical rain belt across the Western Hemisphere.
"The complexity of the Earth system is daunting, with dependencies and feedback loops across many processes and scales," said corresponding author Efi Foufoula-Georgiou, UCI Distinguished Professor of Civil & Environmental Engineering and the Henry Samueli Endowed Chair in Engineering. "This study combines the engineering approach of system's thinking with data analytics and climate science to reveal subtle and previously unrecognized manifestations of global warming on regional precipitation dynamics and extremes."
Foufoula-Georgiou said that a next step is to translate those changes to impacts on the ground, in terms of flooding, droughts, infrastructure and ecosystem change to guide adaptation, policy and management.
The article Climate Change To Alter Position Of Earth's Tropical Rain Belt appeared first on Eurasia Review.

Load-Date: January 18, 2021


End of Document




Big Data To Analyze The Mystery Of Beethoven's Metronome
Eurasia Review
January 4, 2021 Monday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 878 words
Body


Data science and physics research at the Universidad Carlos III de Madrid and UNED has analysed a centuries-old controversy over Beethoven's annotations about the tempo (the playing speed) of his works, which is considered to be too fast based on these marks. In this study, published in the PLOS ONE journal, it is noted that this deviation could be explained by the composer reading the metronome incorrectly when using it to measure the beat of his symphonies.
Ludwig van Beethoven (1770-1827) was one of the first composers to start using a metronome, a device patented by Johann Nepomuk Maelzel in 1815. At that time, he started to edit his works with numerical marks with metronome indications. Doubts about the validity of these marks date back to the 19th century and during the 20th century many musicological analyses were carried out, some of which already pointed to the hypothesis that the metronome was broken, an assumption that could never be verified.
In any case, most orchestra conductors have omitted these marks as they consider them to be too fast (Romanticism), whereas since the 1980s, other conductors (Historicism) have used them to play Beethoven. However, music critics and the public described these concerts as frantic and even unpleasant.
Previous scientific research, such as Sture Forsén's study in 2013, has pointed to several defects that may have affected the metronome, causing it to function slower, which would have led the composer from Bonn to choose faster marks than those actually proposed. In order to validate this explanation, researchers from the UC3M and UNED have systematically compared the metronomic marks with contemporary interpretations. This requires physical skills to model the metronome mathematically, analyse data, computing, usability, and, of course, music skills. Overall, they have analysed the tempo and its variations for each movement of 36 symphonies interpreted by 36 different conductors, a total of 169 hours of music.
"Our study has revealed that conductors tend to play slower than Beethoven indicated. Even those who aim to follow his directions to the letter! The tempi indicated by the composer are, in general, too fast, to the point that, collectively, musicians tend to slow them down," says Iñaki Ucar, one of the authors of this research, data scientist at the UC3M's Big Data Institute, and clarinetist. This slowing down follows, on average, a systematic deviation, so it is not random, but conductors tend to play consistently below Beethoven's marks.
"This deviation could be explained by the composer reading the scale of the apparatus in the wrong place, for example, under the weight instead of above. Ultimately, this would be a problem caused by using new technology," says Almudena Martín Castro, the other author of the study, user experience designer and pianist, who carried out this research within the framework of her Bachelor Thesis for her Degree in Physics at UNED.
In this study, researchers have developed a mathematical model for the metronome based on a double pendulum, perfected with three types of corrections which take the amplitude of its oscillation, the friction of its mechanism, the impulse force, and the mass of its rod, an aspect that had not been considered in previous work, into account.
"With the help of this model, we developed a methodology for estimating the original parameters of Beethoven's metronome from photographs that are available and the patent outline," the work explains. In addition to this, they dismantled a modern metronome to measure it and use it to validate both the mathematical model and methodology.
The researchers tried to identify a "break" in the metronome that gave rise to the slow tempi usually followed by musicians. They tried to change the metronome's mass (it may have been damaged and a piece may have fallen off), move it onto the rod, increase the friction (the metronome may have been poorly lubricated) and even testing the assumption that the apparatus may have been misplaced, leaning over the piano while the composer was creating his music.
"None of the hypotheses matched what the data told us, which is a homogeneous slowdown in the tempi on the entire scale. Finally, we considered the fact that the deviation matches the size of the metronome's weight exactly, and we also found the annotation '108 or 120' on the first page of the manuscript for his ninth symphony, which indicates that the composer doubted where he was reading at least once. Suddenly, it all made sense: Beethoven was able to write down a lot of these marks by reading the tempo in the wrong place," they explain.
This methodology could be applied when investigating the work of other classical composers, as they are able to extract the tempo from a musical recording and clean up the data so they can be compared.
"Studying the relationship between the tempo played and marks from other composers would be very interesting, or even looking for the 'correct tempo' for composers who did not leave any metronomic marks. Is it possible that there is an average tempo at which people usually interpret Bach's fugues, for example?" they ask.
The article Big Data To Analyze The Mystery Of Beethoven's Metronome appeared first on Eurasia Review.

Load-Date: January 4, 2021


End of Document




Understanding How Birds Respond To Extreme Weather Can Inform Conservation Efforts
Eurasia Review
August 24, 2020 Monday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 783 words
Body


When it comes to climate change, University of Wisconsin¬-Madison forest and wildlife ecology Professor Ben Zuckerberg says birds are the proverbial canary in the coal mine. They are both responsive and sensitive to changes in the environment, including the extreme weather events associated with a warming planet.
However, not all birds are the same, and not all weather events have the same impact. How do different bird species respond to extreme weather events that occur for different amounts of time, ranging from weekly events like heat waves to seasonal events like drought? And how do traits unique to different species -- for example, how far they migrate or how commonly they occur -- predict their vulnerability to extreme weather?
To answer these questions, ecologists would traditionally observe a small number of bird species at a few sites over a few years, and then draw general conclusions. However, Zuckerberg and UW-Madison postdoctoral researcher Jeremy Cohen, along with Daniel Fink of the Cornell Lab of Ornithology, had more ambitious goals: they looked at 109 species across eastern North America over a 15-year period, and integrated this information with fine-scale satellite temperature and precipitation data.
In a study recently published in the journalGlobal Change Biology, the researchers show that not all birds are equally vulnerable to the effects of extreme weather resulting from climate change. As the planet warms, some species will adapt while others may struggle without conservation measures. The results of this study could help conservationists target their efforts to vulnerable species, as well as locations where extreme weather events are predicted.
The researchers used data from eBird, a global citizen-science initiative where bird enthusiasts submit checklists of bird sightings online. These checklists include which species were seen, how many, the location and time, and other observations.
The researchers compiled more than 830,000 of these checklists and integrated each one with weather data summarized over the week, month and three months before the observation was recorded. They relied on advanced computing to manage this large amount of information.
"The study we did would not have been remotely possible without data science," says Cohen. The emerging field of data science involves the study, development or application of methods that reveal new insights from data.
Zuckerberg points out that the combination of citizen science and data science makes research possible at a scale that was previously unimaginable for ecologists. However, citizen science has its limitations. Researchers have less control over the scientific process, and data quality can vary.
"Someone can go out for five minutes or two hours and submit eBird data. They can submit a checklist for 10 species or 40 species," says Zuckerberg. "We've adopted data science methods for working with large, unstructured data sets."
After controlling for this noisy data, the researchers observed that some species are less sensitive to extreme weather, and populations are not equally exposed to its effects because some geographic areas are warming faster than others.
When it comes to heat waves, Cohen notes, "long-distance migrants were not super affected by really hot periods. They winter in tropical environments and should be tolerant of heat."
However, resident birds and short-distance migrants such as robins and red-winged blackbirds responded negatively to heat waves, with their numbers sometimes declining 10% to 30% over several weeks.
As for drought, commonly occurring species like crows were more resilient than rare birds, particularly if the drought was severe and long-lasting.
"Rarer species have more specialized habitat and food requirements -- this is a general rule in ecology," says Cohen. "More common species usually have more options. If habitat quality declines due to drought, a generalist can go somewhere else."
Cohen says this is the first large-scale study, spanning half a continent, to look at how birds respond immediately after weather events. Because of the scope of the project, conservationists can better understand how many different bird species are likely to be affected by climate change, and mitigate some of the negative effects.
"If birds are truly winged sentinels of climate change, the greater likelihood of drought, flooding and extreme temperature conditions like heat waves will have significant consequences," says Zuckerberg. "We need to think about how we help species adapt to climate extremes."
The article Understanding How Birds Respond To Extreme Weather Can Inform Conservation Efforts appeared first on Eurasia Review.

Load-Date: August 25, 2020


End of Document




The Danger Of Weaponizing Trade For The Environment  Analysis
Eurasia Review
November 30, 2020 Monday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 984 words
Body


By Ken Heydon
Pressure around the world is growing to apply penalty tariffs on imports from perceived environmental free riders. But such policies are a threat to trade and are unlikely to help the environment. Fortunately, there are better policy alternatives to deal with trade and environment linkages, including tackling fossil fuel subsidies.
Prominent economists such as William Nordhaus and Thomas Piketty are advocating the imposition of carbon border taxes on imports from polluting countries. These calls are founded on the fear that levies on carbon-intensive production simply push production to countries where it is not taxed. Those imposing such border taxes might also claim legitimacy under General Agreement on Tariffs and Trade (GATT) Article XX which allows measures necessary to protect human, animal or plant life.
There is howeverno evidence of growthof widespread pollution havens. The International Energy Agency reports that by 2019 global energy-related carbon dioxide (CO2) emissions had flattened, withstrong renewables growthin China and India. China, Japan and South Korea have each recently set target dates for zero net carbon emissions.
Trade sanctions carry the risk of protectionist capture and of being a brake on the very economic development needed to fund the transition to cleaner energy and, now, to tackle theeconomic disruption from COVID-19and the associated acceleration of digitisation.
Carbon border adjustments tax levied on imports from countries without carbon pricing mechanisms  are becoming an integral part of EU trade policy, and US president-elect Joe Biden has also expressed support for them.
These sanctions may take the form of unilateral action by Europe and the United States or be applied through EU and US preferential trade agreements (PTAs) in the Asia Pacific and elsewhere. The EUJapan agreement, for example, contains commitments that the European Union could invoke to promote a more aggressive approach to trade and the environment, including that parties shall cooperate to promote the contribution of trade to the transition to low greenhouse emissions (Article 16.4). Similar risks are inherent in negotiation of the AustraliaEU PTA.
As for the United States, Joe Biden has made it clear that any consideration of US re-engagement in the Trans-Pacific Partnership will depend on stronger commitments being made on the environment and labour. Such commitments could involve trade penalties.
Advocacy of these measures should be rebutted at every opportunity, but it is not enough just to say 'no' to trade sanctions. The energy transition is not assured and more needs to be done. Despite their movement in the right direction, China, Japan and South Korea still fund the majority of new coal-fired power plants. Fortunately, there are other trade-related measures that can be taken to serve environmental ends and which, importantly, involve reducing rather than increasing distortions to trade.
Two such measures have been on the World Trade Organization's (WTO) agenda for years but are proving frustratingly difficult to advance: attempts to reduce fishing subsidies and negotiations to liberalise trade in environmental goods and services. While this work should be maintained and accelerated  within a hopefully revitalised WTO  two other measures might yield more immediate results.
The first is action in the WTO to reduce fossil fuel subsidies. The elimination of fossil fuel subsidies would, according to the International Monetary Fund, reduce global CO2 emissions by up to 23 per cent. Some WTO disputes have targeted government support for renewable energy, giving grounds to also target policies supporting fossil fuel-based energy.
Two implementation challenges would need tackling. First, the link betweendomesticsubsidies and trade needs to be demonstrated. This can be done by invoking the Anti-Dumping Agreement to determine that energy-subsidised exports constitute exporting at less than normal value and so are open to retaliation. Second, to avoid social disruption in fossil-fuel-dependent developing countries, mitigating development assistance policies need to be implemented, coordinated by a body such as the G20.
Global fossil fuel subsidy reductions could spur reform in Australia. Support to fossil fuel consumption has increased significantly in Australia over the past decade, with revenue forgone equivalent to over 40 per cent of the energy-related tax take, a high share byOECD standards.
A second necessary area of action  again with implications for Australia  is to ensure that the ongoing USChina tech war and ill-advised pursuit of decoupling does not bring further collateral damage to vital cooperation with China on renewable energy. Australia's export of education services in electrical engineering to Chinese (and other) students has supported the development of solar PV panel manufacturing plants in China for export to Australia and the rest of the world. Trade openness is thus vital to the energy transition.
Looking ahead, as countries move to 'smart energy' policies that depend on digital grids, Australia's expertise in information and communications technology, and data science will become an increasingly valuable tradable service. Trade has a direct role to play in the pursuit of environmental goals as a facilitator, not a weapon, that can benefit all countries involved.
*About the author: Ken Heydon is a visiting fellow at the London School of Economics. He is formerly an Australian trade official, Deputy Director-General of the Office of National Assessments and senior member of the OECD secretariat. His latest book is The Political Economy of International Trade: Putting Commerce in Context (Polity, 2019).
Source: This article was published by East Asia Forum
The article The Danger Of Weaponizing Trade For The Environment - Analysis appeared first on Eurasia Review.

Load-Date: November 30, 2020


End of Document




Forecasting Urbanization
Eurasia Review
May 9, 2020 Saturday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1071 words
Body


University of Delaware data scientist Jing Gao is fascinated by the ways that cities and towns grow over time. This concept is known as urbanization.
Take Chicago, Los Angeles and New York. All of these are cities, but they each grow differently, in terms of how the city's land areas expand. The same is true globally, from New Delhi, India, to Paris, France.
Gao, an assistant professor of geography and spatial sciences in UD's College of Earth, Ocean and Environment, and collaborator Brian O'Neill, a UD alumnus and professor from the University of Denver, have created a new global simulation model to predict how urban land will change over the next 100 years under different social and economic conditions.
The research leverages data science and machine learning to provide a first long-term look at how urbanization will unfold - decade by decade.
The researchers describe their simulations in a paper published in the journal Nature Communications.
Data science helps long-term forecasting
According to Gao, until recently it has been difficult to generate long-term, global forecasts of urban expansion. This is because while urbanization is a global trend, the way cities develop (buildings, roads, people, economics) can change over time. Additionally, this development can vary widely country to country, and even within different parts of the same country.
To understand how this change occurs, Gao and O'Neill used data science to analyze 15 global data sets depicting various aspects of urbanization, including a newly available global map series showing urban land change over the past 40 years based on satellite images of Earth. The global maps are accurate to within approximately 125 feet (38 meters) and provide a uniquely detailed look at past urban development that was not previously possible with this degree of specificity.
"Mining historical data revealed that there are three different urbanization styles: urbanized, steadily urbanizing and rapidly urbanizing," Gao said. "And countries evolve from rapidly urbanizing to steadily urbanizing to urbanized over time."
It should come as no surprise that the United States and most western European countries are already urbanized. India and China, which previously experienced rapid development, have now transitioned to steadily urbanizing. Rapidly urbanizing countries at present include many countries in Africa.
And here's the data science part. Understanding these broad styles is not enough to capture - globally - how urbanization is playing out on the ground at a local scale.
To do this, the researchers divided the world into 375 small regions and ran a unique model for each region simultaneously, then pieced results from all models together to develop a global map. This information can shed light on how our cities may change and reveal potential impacts of urbanization that can inform local to global urban planners and policymakers.
The research team's projections show that the total amount of urban areas on Earth can grow anywhere from 1.8 to 5.9-fold by 2100. On average, if past urbanization trends continue, the world will build approximately 618,000 square miles (1.6 million square kilometers) of new urban areas globally over the century. This is an area roughly 4.5 times the size of Germany, or, more than 225 million football fields.
How this urban expansion occurs, however, largely depends on societal trends in the years to come. This includes trends in economic growth, population change and lifestyle habits, and what level of consideration is given to how our habits affect the environment.
For both developed and developing countries, for example, countries in Europe and Southeast Asia, urban expansion is expected to roughly triple if society favors materialistic and fossil-fuel driven development instead of adopting a sustainability mindset.
In the U.S., the least urban expansion occurs if people are focused on sustainability, such as green development and environmental awareness. In this case, urban land is expected to grow by 1.3 times by 2100. But if people favor highly materialistic development over the same timeframe, with high consumption of fossil fuels and a material-driven society, sprawl-like urban expansion is expected, with close to four times the amount of expansion the U.S. had at the beginning of the century.
The U.S. already is among the countries with the largest amount of developed land, so four-fold growth in urban expansion is a lot.
"This is where our projections can inform policy and planning," said Gao. "These projections can help researchers and analysts understand how large-scale changes that occur over a long time period, such as climate change, may affect local urban areas."
Most individuals do not realize how changes to the landscape, such as buildings and roads, may affect their lives. In Delaware, for example, second homes being built near the coast often come at the cost of agricultural farmland. While these developments may increase an area's economic prosperity, they can have other unintended consequences, such as increased potential exposure to coastal flooding and sea level rise.
And, no matter what socio-economic scenario was selected, the simulations show that most countries will become urbanized by the end of the century.
One interesting finding from the work is that although prevailing thought is that urbanization is primarily happening in the developing world, Gao said this may not be the case.
"If you look at the data collected over the past 40 years, the absolute amount of new urban land construction in the developed world is comparable to the developing world," she said. "However, the changes seem faster in the developing world because there currently is much less developed land there, so the rate of change appears greater."
This begs the question: as developing countries in Africa continue to grow, will they ever catch up to or surpass developed countries like the United States in terms of urbanized land?
"According to today's definition, Africa is expected to become urbanized by 2100," said Gao. "But even if it continues developing at a very fast rate relative to the rest of the world throughout the century, it won't catch up to developed countries like the U.S. because the difference at the outset is large and the developed world still keeps expanding its urban area."
The post Forecasting Urbanization appeared first on Eurasia Review.

Load-Date: May 10, 2020


End of Document




Irish And UK Research Helps To Unravel Secrets Behind Game Of Thrones
Eurasia Review
November 10, 2020 Tuesday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 679 words
Body


A researcher at University of Limerick in Ireland has played a key role in examining some of the secrets behind Game of Thrones.
What are the secrets behind one of the most successful fantasy series of all time? How has a story as complex as the one in George R.R. Martin's novels enthralled the world and how does it compare to other narratives?
Researchers from five universities across the UK and Ireland - including UL's Dr Padraig MacCarron - came together to unravel 'A Song of Ice and Fire', the books on which the TV series is based.
In a paper that has just been published by theProceedings of the National Academy of Sciences of the USA, the team of physicists, mathematicians and psychologists from Coventry, Warwick, Limerick, Cambridge and Oxford universities used data science and network theory to analyse the acclaimed book series by George R.R. Martin.
The study shows the way the interactions between the characters are arranged is similar to how humans maintain relationships and interact in the real world. Moreover, although important characters are famously killed off at random as the story is told, the underlying chronology is not at all so unpredictable, the research shows.
The team found that, despite over 2,000 named characters in 'A Song of Ice and Fire' and over 41,000 interactions between them, at chapter-by-chapter level these numbers average out to match what we can handle in real life. Even the most predominant characters - those who tell the story - average out to have only 150 others to keep track of. This is the same number that the average human brain has evolved to deal with.
While matching mathematical motifs might have been expected to lead to a rather narrow script, George R. R. Martin keeps the tale bubbling by making deaths appear random as the story unfolds. But, as the team show, when the chronological sequence is reconstructed the deaths are not random at all: rather, they reflect how common events are spread out for non-violent human activities in the real world.
"These books are known for unexpected twists, often in terms of the death of a major character, it is interesting to see how the author arranges the chapters in an order that makes this appear even more random than it would be if told chronologically," explained Dr Padraig MacCarron, a postdoctoral researcher at the Centre for Social Issues Research and Mathematics Applications Consortium for Science and Industry (MACSI) at UL.
"Social networks of the most connected characters, while seemingly extensive, mirrored the typical range of social networks that humans maintain. Furthermore, characters' social networks did not extend beyond the cognitive limit of social connections that humans are able to sustain.
"Although the time intervals between significant deaths in relation to the story's timeline may appear random, they are not told in chronological order. Re-arranging them in order of which they occur, they follow a pattern more commonly observed in reality," added Dr MacCarron.
'Game of Thrones' has invited all sorts of comparison to history and myth and the marriage of science and humanities in this paper opens new avenues to comparative literary studies. It shows, for example, that it is more akin to the Icelandic sagas than to mythological stories such as the Tain Bo Cuailnge or Beowulf. The trick in Game of Thrones, it seems, is to mix realism and unpredictability in a cognitively engaging manner.
"People largely make sense of the world through narratives, but we have no scientific understanding of what makes complex narratives relatable and comprehensible. The ideas underpinning this paper are steps towards answering this question," explained Professor Colm Connaughton, from the University of Warwick.
Fellow researcher Professor Robin Dunbar, from the University of Oxford, observed: "This study offers convincing evidence that good writers work very carefully within the psychological limits of the reader."
The article Irish And UK Research Helps To Unravel Secrets Behind Game Of Thrones appeared first on Eurasia Review.

Load-Date: November 11, 2020


End of Document




Artificial Intelligence Can Predict Students' Educational Outcomes Based On Tweets
Eurasia Review
October 24, 2020 Saturday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 2100 words
Body


Ivan Smirnov, Leading Research Fellow of the Laboratory of Computational Social Sciences at the Institute of Education of HSE University, has created a computer model that can distinguish high academic achievers from lower ones based on their social media posts. The prediction model uses a mathematical textual analysis that registers users' vocabulary (its range and the semantic fields from which concepts are taken), characters and symbols, post length, and word length.
Every word has its own rating (a kind of IQ). Scientific and cultural topics, English words, and words and posts that are longer in length rank highly and serve as indicators of good academic performance. An abundance of emojis, words or whole phrases written in capital letters, and vocabulary related to horoscopes, driving, and military service indicate lower grades in school. At the same time, posts can be quite short--even tweets are quite informative. The study was supported by a grant from the Russian Science Foundation (RSF), and an article detailing the study's results was published in EPJ Data Science.
Smirnov's study used a representative sample of data from HSE University's longitudinal cohort panel study, 'Educational and Career Trajectories' (TrEC). The study traces the career paths of 4,400 students in 42 Russian regions from high schools participating in PISA (the Programme for International Students Assessment). The study data also includes data about the students' VK accounts (3,483 of the student participants consented to provide this information).
'Since this kind of data, in combination with digital traces, is difficult to obtain, it is almost never used,' Smirnov says. Meanwhile, this kind of dataset allows you to develop a reliable model that can be applied to other settings. And the results can be extrapolated to all other students--high school students and middle school students.
Posts from publicly viewable VK pages were used as a training sample--this included a total of 130,575 posts from 2,468 subjects who took the PISA test in 2012. The test allowed the researcher to assess a student's academic aptitude as well as their ability to apply their knowledge in practice. The study included only publicly visible VK posts from consenting participants.
When developing and testing the model from the PISA test, only students' reading scores were used an indicator of academic aptitude, although there are three tests in total: reading, mathematics, and science. PISA defines reading literacy as 'understanding, using, reflecting on and engaging with written texts in order to achieve one's goals, to develop one's knowledge and potential, and to participate in society.' The exam has six proficiency levels. Students who score a 2 are considered to meet only the basic, minimum level, while those who score a 5 or 6 are considered to be strong students.
In the study, unsupervised machine learning with word vector representations was performed on VK post corpus (totaling 1.9 billion words, with 2.5 million unique words). It was combined with a simpler supervised machine learning model that was trained in individual positions and taught to predict PISA scores.
'We represented each post as a 300-dimensional vector by averaging over vector representations of all its constituent words,' Smirnov writes. 'These post representations were used to train a linear regression model to predict the PISA scores of the posts' authors.'
By 'predict', the researcher does not refer to future forecasting, but rather the correlation between the calculated results and the real scores students earned on the PISA exam, as well as their USE scores (which are publicly available online in aggregated form--i.e., average scores per school). In the preliminary phase, the model learned how to predict the PISA data. In the final model, the calculations were checked against the USE results of high school graduates and university entrants.
The final model was supposed to be able to reliably recognize whether a strong student or a weak student had written a particular social media post, or in other words, differentiate the subjects according to their academic performance. After the training period, the model was able to distinguish posts written by students who scored highly or poorly on PISA (levels 5-6 and levels 0-1) with an accuracy of 93.7%. As for the comparability of PISA and the USE, although these two tests differ, studies have shown that students' scores for the two tests strongly correlate with each other.
'The model was trained using PISA data, and we looked at the correlation between the predicted and the real PISA scores (which are available in the TrEC study),' Smirnov explains. 'With the USE things gets more complicated: since the model does not know anything about the unified exams, it predicted the PISA scores as before. But if we assume that the USE and PISA measure the same thing -- academic performance -- then the higher the predicted PISA results are, the higher the USE results should be.' And the fact that the model learned to predict one thing and can predict another is quite interesting in itself, Smirnov notes.
However, this also needed to be verified, so the model was then applied to 914 Russian high schools (located in St. Petersburg, Samara and Tomsk; this set included almost 39,000 users who created 1.1 million posts) and one hundred of Russia's largest universities (115,800 people; 6.5 million posts) to measure the academic performance of students at these institutions.
It turned out that 'predicted academic performance is closely related to USE scores,' says Smirnov. 'The correlation coefficient is between 0.49 and 0.6. And in the case of universities, when the predicted academic performance and USE scores of applicants were compared (the information is available in HSE's ongoing University Admissions Quality Monitoring study), then the results also demonstrated a strong connection. The correlation coefficient is 0.83, which is significantly higher than for high schools, because there is more data.'
But can the model be applied to other social media sites? 'I checked what would happen if, instead of posts on VK, we gave the model tweets written by the same users,' Smirnov says. 'It turned out that the quality of the model does not significantly decrease.' But since a sufficient number of twitter accounts were available only for the university dataset (2,836), the analysis was performed only on this set.
It is important that the model worked successfully on datasets of different social media sites, such as VK and Twitter, thereby proving that is can be effective in different contexts. This means that it can be applied widely. In addition, the model can be used to predict very different characteristics, from student academic performance to income or depression.
Smirnov's study used a representative sample of data from HSE University's longitudinal cohort panel study, 'Educational and Career Trajectories' (TrEC). The study traces the career paths of 4,400 students in 42 Russian regions from high schools participating in PISA (the Programme for International Students Assessment). The study data also includes data about the students' VK accounts (3,483 of the student participants consented to provide this information).
'Since this kind of data, in combination with digital traces, is difficult to obtain, it is almost never used,' Smirnov says. Meanwhile, this kind of dataset allows you to develop a reliable model that can be applied to other settings. And the results can be extrapolated to all other students--high school students and middle school students.
Posts from publicly viewable VK pages were used as a training sample--this included a total of 130,575 posts from 2,468 subjects who took the PISA test in 2012. The test allowed the researcher to assess a student's academic aptitude as well as their ability to apply their knowledge in practice. The study included only publicly visible VK posts from consenting participants.
It is important that the scores on the standardized PISA and USE tests were used as an academic aptitude metric. This gives a more objective picture than assessment mechanisms that are school-specific (such as grades).
When developing and testing the model from the PISA test, only students' reading scores were used an indicator of academic aptitude, although there are three tests in total: reading, mathematics, and science. PISA defines reading literacy as 'understanding, using, reflecting on and engaging with written texts in order to achieve one's goals, to develop one's knowledge and potential, and to participate in society.' The exam has six proficiency levels. Students who score a 2 are considered to meet only the basic, minimum level, while those who score a 5 or 6 are considered to be strong students.
In the study, unsupervised machine learning with word vector representations was performed on VK post corpus (totaling 1.9 billion words, with 2.5 million unique words). It was combined with a simpler supervised machine learning model that was trained in individual positions and taught to predict PISA scores.
Word vector representations, or word embedding, is a numeric vector of a fixed size that describes some features of a word or their sequence. Embedding is often used for automated word processing. In Smirnov's research, the fastText system was used since it is particularly conducive to working with Russian-language text.
'We represented each post as a 300-dimensional vector by averaging over vector representations of all its constituent words,' Smirnov writes. 'These post representations were used to train a linear regression model to predict the PISA scores of the posts' authors.'
By 'predict', the researcher does not refer to future forecasting, but rather the correlation between the calculated results and the real scores students earned on the PISA exam, as well as their USE scores (which are publicly available online in aggregated form--i.e., average scores per school). In the preliminary phase, the model learned how to predict the PISA data. In the final model, the calculations were checked against the USE results of high school graduates and university entrants.
Results
First, Smirnov highlighted the general textual features of posts in relation to the academic performance of their authors (Fig. 1). The use of capitalized words (-0.08), emojis (-0.06), and exclamations (-0.04) were found to be negatively correlated with academic performance. The use of the Latin characters, average post and word length, vocabulary size, and entropy of users' texts on the other hand, were found to positively correlate with academic performance (from 0.07 to 0.16, respectively).
It was also confirmed that students with different levels of academic performance have different vocabulary ranges. Smirnov explored the resulting model by selecting 400 words with the highest and lowest scores that appear at least 5 times in the training corpus. Thematic clusters were identified and visualized (Fig. 2).
The clusters with the highest scores (in orange) include:
English words (above, saying, yours, must);Words related to literature (Bradbury, Fahrenheit, Orwell, Huxley, Faulkner, Nabokov, Brodsky, Camus, Mann);Concepts related to reading (read, publish, book, volume);Terms and names related to physics (Universe, quantum, theory, Einstein, Newton, Hawking);Words related to thought processes (thinking, memorizing).
Clusters with low scores (in green) include misspelled words, names of popular computer games, concepts related to military service (army, oath, etc.), horoscope terms (Aries, Sagittarius), and words related to driving and car accidents (collision, traffic police, wheels, tuning).
Smirnov calculated the coefficients for all 2.5 million words of the vector model and made them available for further study. Interestingly, even words that are rarely found in a training dataset can predict academic performance. For example, even if the name 'Newt' (as in the Harry Potter character, Newt Scamander) never appears in the training dataset, the model might assign a higher rating to posts that contain it. This will happen if the model learns that words from novel series are used by high-achieving students, and, through unsupervised learning, 'intuit' that that the name 'Newt' belongs to this category (that is, the word is closely situated to other concepts from Harry Potter in the vector space).
The article Artificial Intelligence Can Predict Students' Educational Outcomes Based On Tweets appeared first on Eurasia Review.

Load-Date: October 24, 2020


End of Document




Knowing The Model You Can Trust: The Key To Better Decision-Making
Eurasia Review
October 25, 2020 Sunday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 367 words
Body


As much of Europe is engulfed by a second wave of Covid-19, and track and trace struggles to meet demand, modelling support tools are being increasingly used by policymakers to make key decisions. Most notably, models have been used to predict the Covid-19 R0 rate - the average rate of secondary infections from a single infection, which has formed the basis for many lockdown decisions across the UK.
Models can represent the most effective tool for identifying interventions that can balance the risks of widespread infection and help assess socio-economic disruption until an effective treatment is established. However, not all models are equal, and differences in model predictions during the Covid-19 pandemic have caused confusion and suspicion.
A recent paper 'Three questions to ask before using model outputs for decision support' published inNature Communicationsaims to help decision makers choose the best available model for the problem at hand. The paper proposes three screening questions that can help critically evaluate models with respect to their purpose, organisation, and evidence, and enable more secure use of models for key decisions by policy makers.
One of the authors of the paper, Dr Alice Johnston, Lecturer in Environmental Data Science at Cranfield University, said: "From Covid-19 to the stock market, models are increasingly used by policymakers to support their decisions.
"However, different models are based on different assumptions and so can produce conflicting results, even when they represent the same system. Models used early on in the Covid-19 pandemic were a prime example of this, which led to confusion over which models to trust to support the decision-making process. This really highlights the need for clear communication of a model's context, so that policymakers have confidence in which models to trust.
"We propose that before engaging with a model, policymakers ask themselves three screening questions focusing on the model's purpose, organisation and evidence base, with the aim of bringing greater clarity to the decision-making process."
The article Knowing The Model You Can Trust: The Key To Better Decision-Making appeared first on Eurasia Review.

Load-Date: October 26, 2020


End of Document




Spain Using Mobile Phone Data To Study Efficacy Of Lockdown On Spread Of COVID-19
Eurasia Review
April 14, 2020 Tuesday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 855 words
Body


A new CSIC [National Scientific Research Council] project uses computer science and data science techniques to observe how the lockdown measures taken to halt the spread of the disease COVID-19 are proving effective. The results will be key to improving social distancing strategies taken in future outbreaks of this disease and of others.
To carry out this research, a multi-disciplinary team with experts in computer science, demographics, physics and movement studies are analyzing anonymous and high resolution big data obtained from telephone operators and map servers. These data explain how mobility patterns and social contact have changed since the start of the lockdown.
How to lift the lockdown and when
Once all the data is gathered, the team simulates different scenarios and strategies for social distancing and helps with decision-making. The results are key both for deciding whether a stricter lockdown should be activated and to plan for the safe and effective lifting of the lockdown.
"We hope that the results serve to better understand the effects of the lockdown on the spread of the disease, but also help in decision-making related to the lifting of the measures, to see whether or not it is better to end the lockdown gradually," said Frederic Bartumeus.
"To achieve this goal the project includes several phases that are being carried out in parallel," said José Javier Ramasco. "Firstly, mobility is characterized, which is being coordinated by the IFISC based on the contribution from various data platforms: information, for example, from online social media and mobility patterns captured from mobile phone records. In this latter case, the data are collected by the operators and companies that are taking part in the project, which provide the research team with aggregated travel flows between different areas" specifies the researcher.
In no case is individual information accessed.
A second aspect is the change in conduct of people due to the perception of risk. The CEAB and IEGD are carrying out surveys and implementing mobile phone applications to quantify these changes, trying to estimate the adherence to personal protection measures by people and the changes in the amount and quality of personal contact.
"This information is crucial for understanding the contagion process," said  José Javier Ramasco.
Lastly, all these data form part of the computational models being developed by the IFISC and IFCA to study the different scenarios to exit the crisis.
"The lockdown has been widespread and relatively sudden, but to avoid new outbreaks it is necessary to use simulators capable of assessing scenarios with different rhythms to return to normality, both by sector and by geographic area," José Javier Ramasco said.
Epidemiology in the future
The project uses artificial intelligence tools and data science, and integrates big data in real time on human mobility, geo-localized surveys and computational models. This is a new way of undertaking epidemiology which combines computational epidemiology, digital demography and human mobility models.
"The study will take into account such important aspects as the spatial distribution of the population, their age structure, and the distribution and characteristics of social health centers (hospitals, local health centers, and care homes for the elderly). We can see how the contention measures have changed the mobility and conduct of people," said José Javier Ramasco.
The information and models to be developed during this research study will be made available to the public for their future use following an open data model under FAIR (Findable, Accessible, Interoperable, Reusable) principles.
A second long-term goal is to establish the basis for a computational epidemiology network in Spain, as in other countries, and a series of interoperable analytical tools based on epidemiological theories, data science and artificial intelligence, to report decisions to be taken in future situations of epidemiological crisis which, as the scientists say, is something that "has already happened on several occasions since 2009 and is likely to be recurrent in our globalized and interconnected world".
The project, pre-financed by the CSIC, thanks to the donation received from AENA, is coordinated by the scientists José Javier Ramasco, from the Institute of Complex Physics System (Spanish acronym: IFISC, a joint CSIC and University of Balearic Islands centre) and Frederic Bartumeus, from the Blanes Advanced Studies Centre (Spanish acronym: CEAB-CSIC) and the CREAF [Centre for Ecological Research and Forestry Applications]. It also involves the participation of teams from the Institute for Economy, Geography and Demography (Spanish acronym: IEGD-CSIC), from the Institute for Physics of Cantabria (Spanish acronym: IFCA-CSIC), from the National Biotechnology Centre (Spanish acronym: CNB-CSIC), as well as scientists from Pompeu Fabra University and the National Epidemiology Centre- Carlos III Health Institute (ISCIII).
The post Spain Using Mobile Phone Data To Study Efficacy Of Lockdown On Spread Of COVID-19 appeared first on Eurasia Review.

Load-Date: April 14, 2020


End of Document




Climate Signals Detected In Global Weather
Eurasia Review
January 2, 2020 Thursday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 765 words
Body


In October this year, weather researchers in Utah measured the lowest temperature ever recorded in the month of October in the US (excluding Alaska): -37.1°C. The previous low-temperature record for October was -35°C, and people wondered what had happened to climate change.
Until now, climate researchers have responded that climate is not the same thing as weather. Climate is what we expect in the long term, whereas weather is what we get in the short term - and since local weather conditions are highly variable, it can be very cold in one location for a short time despite long-term global warming. In short, the variability of local weather masks long-term trends in global climate.
A paradigm shift
Now, however, a group led by ETH professor Reto Knutti has conducted a new analysis of temperature measurements and models. The scientists concluded that the weather-is-not-climate paradigm is no longer applicable in that form. According to the researchers, the climate signal - that is, the long-term warming trend - can actually be discerned in daily weather data, such as surface air temperature and humidity, provided that global spatial patterns are taken into account.
In plain English, this means that - despite global warming - there may well be a record low temperature in October in the US. If it is simultaneously warmer than average in other regions, however, this deviation is almost completely eliminated. "Uncovering the climate change signal in daily weather conditions calls for a global perspective, not a regional one," says Sebastian Sippel, a postdoc working in Knutti's research group and lead author of a study recently published inNature Climate Change.
Statistical learning techniques extract climate change signature
In order to detect the climate signal in daily weather records, Sippel and his colleagues used statistical learning techniques to combine simulations with climate models and data from measuring stations. Statistical learning techniques can extract a "fingerprint" of climate change from the combination of temperatures of various regions and the ratio of expected warming and variability. By systematically evaluating the model simulations, they can identify the climate fingerprint in the global measurement data on any single day since spring 2012.
A comparison of the variability of local and global daily mean temperatures shows why the global perspective is important. Whereas locally measured daily mean temperatures can fluctuate widely (even after the seasonal cycle is removed), global daily mean values show a very narrow range.
If the distribution of global daily mean values from 1951 to 1980 are then compared with those from 2009 to 2018, the two distributions (bell curves) barely overlap. The climate signal is thus prominent in the global values but obscured in the local values, since the distribution of daily mean values overlaps quite considerably in the two periods.
Application to the hydrological cycle
The findings could have broad implications for climate science. "Weather at the global level carries important information about climate," says Knutti. "This information could, for example, be used for further studies that quantify changes in the probability of extreme weather events, such as regional cold spells. These studies are based on model calculations, and our approach could then provide a global context of the climate change fingerprint in observations made during regional cold spells of this kind. This gives rise to new opportunities for the communication of regional weather events against the backdrop of global warming."
The study stems from a collaboration between ETH researchers and the Swiss Data Science Center (SDSC), which ETH Zurich operates jointly with its sister university EPFL. "The current study underlines how useful data science methods are in clarifying environmental questions, and the SDSC is of great use in this," says Knutti.
Data science methods not only allow researchers to demonstrate the strength of the human "fingerprint", they also show where in the world climate change is particularly clear and recognisable at an early stage. This is very important in the hydrological cycle, where there are very large natural fluctuations from day to day and year to year.
"In future, we should therefore be able to pick out human-induced patterns and trends in other more complex measurement parameters, such as precipitation, that are hard to detect using traditional statistics," says the ETH professor.
The post Climate Signals Detected In Global Weather appeared first on Eurasia Review.

Load-Date: January 2, 2020


End of Document




Experts To Create Predictive Tool To Tackle Hate Crime In Los Angeles
Eurasia Review
September 22, 2016 Thursday


Copyright 2016 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 672 words
Body


Cardiff University team awarded over $800,000 by the US Department of Justice to develop real-time predictions of hate crime using Twitter
Experts from Cardiff University are developing a statistical tool that uses social media to make real-time predictions of where hate crimes may occur.
The team, from the University's Social Data Science Lab, will be using Los Angeles County as a test bed for their study, thanks to over $800,000 in funding from the US Department of Justice.
It is the first time that social media has been used in the United States to create predictive policing models of hate crime.
Over the next three years, the team will be closely scrutinizing data taken from Twitter and cross-referencing this with reported hate crimes in Los Angeles to develop markers, or signatures, which could indicate if, and where, a hate crime is likely to take place at a certain point in time, and then enable police officers to intervene.
The term hate crime is used to describe a prejudice-motivated crime, often violent, which occurs when a perpetrator targets a victim because of his or her affiliation to a social group, such as their sex, ethnicity, disability or religion.
According to the US Bureau of Justice Statistics (BJS), in 2012 an estimated 293,800 nonfatal violent and property hate crime victimizations occurred in the United States.
UK official data shows that there were 52,528 hate crimes recorded by the police in England and Wales in 2014/15, an increase of 18 per cent compared with 2013/14.
Previous research from the Social Data Science Lab has already shown that Twitter data can be used to identify hot spots, such as certain states or cities, where hate speech has occurred but where hate crime has not been reported. One example is an area when recent immigrants may be unlikely to report crime due to fear of deportation.
Professor Matt Williams, from the University's School of Social Science, said, "Developing a better understanding of hateful sentiments online and their relationship with crime on the streets could push law enforcement to better identify, report and address hate crimes that are occurring offline.
"The insights provided by our work will help US localities to design policies to address specific hate crime issues unique to their jurisdiction and allow service providers to tailor their services to the needs of victims, especially if those victims are members of an emerging category of hate crime targets."
The Los Angeles Police Department has a history of incorporating progressive and forward-thinking methods into their policing, having previously used mathematical models to predict other areas of crime, which have been shown to successfully lower crime rates.
The huge volumes of data that social media now generates has provided researchers with large swathes of information that can be used to identify emerging patterns and trends in a number of areas across society, including crime.
Dr Pete Burnap, from the University's School of Computer Science and Informatics, said, "This is the first study in the United States to use social media data in predictive policing models of hate crime. Predictive policing is a proactive law enforcement model that has become more common partially due to the advent of advanced analytics such as data mining and machine-learning methods.
"New analytic approaches and the ability to process very large data sets have increased the accuracy of predictive models over traditional crime analysis methods and this project will evaluate if police departments can leverage these new data and techniques to reduce hate crimes."
Cardiff University's Social Data Science Lab forms part of the Data Innovation Research Institute and has been involved in research grants amounting to more than £6 million. The Social Data Science Lab brings together social, computer, political, health, statistical and mathematical scientists to study the methodological, theoretical, empirical and technical dimensions of New Forms of Data in social and policy contexts.

Load-Date: September 22, 2016


End of Document




Police Stop Fewer Black Drivers At Night When 'Veil Of Darkness' Obscures Their Race
Eurasia Review
May 6, 2020 Wednesday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 883 words
Body


The largest-ever study of alleged racial profiling during traffic stops has found that blacks, who are pulled over more frequently than whites by day, are much less likely to be stopped after sunset, when "a veil of darkness" masks their race.
That is one of several examples of systematic bias that emerged from a five-year study that analyzed 95 million traffic stop records, filed by officers with 21 state patrol agencies and 35 municipal police forces from 2011 to 2018.
The Stanford-led study also found that when drivers were pulled over, officers searched the cars of blacks and Hispanics more often than whites. The researchers also examined a subset of data from Washington and Colorado, two states that legalized marijuana, and found that while this change resulted in fewer searches overall, and thus fewer searches of blacks and Hispanics, minorities were still more likely than whites to have their cars searched after a pull-over.
"Our results indicate that police stops and search decisions suffer from persistent racial bias, and point to the value of policy interventions to mitigate these disparities," the researchers write in Nature Human Behaviour.
The paper culminates a five-year collaboration between Stanford's Cheryl Phillips, a journalism lecturer whose graduate students obtained the raw data through public records requests, and Sharad Goel, a professor of management science and engineering whose computer science team organized and analyzed the data.
Goel and his collaborators, which included Ravi Shroff, a professor of applied statistics at New York University, spent years culling through the data, eliminating records that were incomplete or from the wrong time periods, to create the 95 million-record database that was the basis for their analysis. "There is no way to overstate the difficulty of that task," Goel said.
Creating that database enabled the team to find the statistical evidence that a "veil of darkness" partially immunized blacks against traffic stops. That term and idea has been around since 2006 when it was used in a study that compared the race of 8,000 drivers in Oakland, California, who were stopped at any time of day or night over a six month period. But the findings from that study were inconclusive because the sample was too small to prove a link between the darkness of the sky and the race of the stopped drivers.
The Stanford team decided to repeat the analysis using the much larger dataset that they had gathered. First, they narrowed the range of variables they had to analyze by choosing a specific time of day - around 7 p.m. - when the probable causes for a stop were more or less constant. Next, they took advantage of the fact that, in the months before and after daylight saving time each year, the sky gets a little darker or lighter, day by day. Because they had such a massive database, the researchers were able to find 113,000 traffic stops, from all of the locations in their database, that occurred on those days, before or after clocks sprang forward or fell back, when the sky was growing darker or lighter at around 7 p.m. local time.
This dataset provided a statistically valid sample with two important variables - the race of the driver being stopped, and the darkness of the sky at around 7 p.m. The analysis left no doubt that the darker it got, the less likely it became that a black driver would be stopped. The reverse was true when the sky was lighter.
More than any single finding, the collaboration's most lasting impact may be from the Stanford Open Policing Project, which the researchers started to make their data available to investigative and data-savvy reporters, and to hold workshops to help reporters learn how to use the data to do local stories.
For example, the researchers helped reporters at the Seattle-based non-profit news organization, Investigate West, understand the patterns in the data for stories showing bias in police searches of Native Americans. That reporting prompted the Washington State Patrol to review its practices and boost officer training. Similarly, the researchers helped reporters at the Los Angeles Times analyze data that showed how police searched minority drivers far more often than whites. It resulted in a story that was part of a larger investigative series that prompted changes in Los Angeles Police Department practices.
"All told we've trained about 200 journalists, which is one of the unique things about this project," Phillips said.
Goel and Phillips plan to continue collaborating through a project called Big Local News that will explore how data science can shed light on public issues, such as civil asset forfeitures - instances in which law enforcement is authorized to seize and sell property associated with a crime. Gathering and analyzing records of when and where such seizures occur, to whom, and how such property is disposed will help shed light on how this practice is being used. Big Local News is also working on collaborative efforts to standardize information from police disciplinary cases.
"These projects demonstrate the power of combining data science with journalism to tell important stories," Goel said.
The post Police Stop Fewer Black Drivers At Night When 'Veil Of Darkness' Obscures Their Race appeared first on Eurasia Review.

Load-Date: May 6, 2020


End of Document




Model Beats Wall Street Analysts In Forecasting Business Financials
Eurasia Review
December 20, 2019 Friday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1237 words
Body


Knowing a company's true sales can help determine its value. Investors, for instance, often employ financial analysts to predict a company's upcoming earnings using various public data, computational tools, and their own intuition. Now MIT researchers have developed an automated model that significantly outperforms humans in predicting business sales using very limited, "noisy" data.
In finance, there's growing interest in using imprecise but frequently generated consumer data - called "alternative data" - to help predict a company's earnings for trading and investment purposes. Alternative data can comprise credit card purchases, location data from smartphones, or even satellite images showing how many cars are parked in a retailer's lot. Combining alternative data with more traditional but infrequent ground-truth financial data - such as quarterly earnings, press releases, and stock prices - can paint a clearer picture of a company's financial health on even a daily or weekly basis.
But, so far, it's been very difficult to get accurate, frequent estimates using alternative data. In a paper published this week in the Proceedings of ACM Sigmetrics Conference, the researchers describe a model for forecasting financials that uses only anonymized weekly credit card transactions and three-month earning reports.
Tasked with predicting quarterly earnings of more than 30 companies, the model outperformed the combined estimates of expert Wall Street analysts on 57 percent of predictions. Notably, the analysts had access to any available private or public data and other machine-learning models, while the researchers' model used a very small dataset of the two data types.
"Alternative data are these weird, proxy signals to help track the underlying financials of a company," says first author Michael Fleder, a postdoc in the Laboratory for Information and Decision Systems (LIDS). "We asked, 'Can you combine these noisy signals with quarterly numbers to estimate the true financials of a company at high frequencies?' Turns out the answer is yes."
The model could give an edge to investors, traders, or companies looking to frequently compare their sales with competitors. Beyond finance, the model could help social and political scientists, for example, to study aggregated, anonymous data on public behavior. "It'll be useful for anyone who wants to figure out what people are doing," Fleder says.
Joining Fleder on the paper is EECS Professor Devavrat Shah, who is the director of MIT's Statistics and Data Science Center, a member of the Laboratory for Information and Decision Systems, a principal investigator for the MIT Institute for Foundations of Data Science, and an adjunct professor at the Tata Institute of Fundamental Research.
Tackling the "small data" problem
For better or worse, a lot of consumer data is up for sale. Retailers, for instance, can buy credit card transactions or location data to see how many people are shopping at a competitor. Advertisers can use the data to see how their advertisements are impacting sales. But getting those answers still primarily relies on humans. No machine-learning model has been able to adequately crunch the numbers.
Counterintuitively, the problem is actually lack of data. Each financial input, such as a quarterly report or weekly credit card total, is only one number. Quarterly reports over two years total only eight data points. Credit card data for, say, every week over the same period is only roughly another 100 "noisy" data points, meaning they contain potentially uninterpretable information.
"We have a 'small data' problem," Fleder says. "You only get a tiny slice of what people are spending and you have to extrapolate and infer what's really going on from that fraction of data."
For their work, the researchers obtained consumer credit card transactions - at typically weekly and biweekly intervals - and quarterly reports for 34 retailers from 2015 to 2018 from a hedge fund. Across all companies, they gathered 306 quarters-worth of data in total.
Computing daily sales is fairly simple in concept. The model assumes a company's daily sales remain similar, only slightly decreasing or increasing from one day to the next. Mathematically, that means sales values for consecutive days are multiplied by some constant value plus some statistical noise value - which captures some of the inherent randomness in a company's sales. Tomorrow's sales, for instance, equal today's sales multiplied by, say, 0.998 or 1.01, plus the estimated number for noise.
If given accurate model parameters for the daily constant and noise level, a standard inference algorithm can calculate that equation to output an accurate forecast of daily sales. But the trick is calculating those parameters.
Untangling the numbers
That's where quarterly reports and probability techniques come in handy. In a simple world, a quarterly report could be divided by, say, 90 days to calculate the daily sales (implying sales are roughly constant day-to-day). In reality, sales vary from day to day. Also, including alternative data to help understand how sales vary over a quarter complicates matters: Apart from being noisy, purchased credit card data always consist of some indeterminate fraction of the total sales. All that makes it very difficult to know how exactly the credit card totals factor into the overall sales estimate.
"That requires a bit of untangling the numbers," Fleder says. "If we observe 1 percent of a company's weekly sales through credit card transactions, how do we know it's 1 percent? And, if the credit card data is noisy, how do you know how noisy it is? We don't have access to the ground truth for daily or weekly sales totals. But the quarterly aggregates help us reason about those totals."
To do so, the researchers use a variation of the standard inference algorithm, called Kalman filtering or Belief Propagation, which has been used in various technologies from space shuttles to smartphone GPS. Kalman filtering uses data measurements observed over time, containing noise inaccuracies, to generate a probability distribution for unknown variables over a designated timeframe. In the researchers' work, that means estimating the possible sales of a single day.
To train the model, the technique first breaks down quarterly sales into a set number of measured days, say 90 - allowing sales to vary day-to-day. Then, it matches the observed, noisy credit card data to unknown daily sales. Using the quarterly numbers and some extrapolation, it estimates the fraction of total sales the credit card data likely represents. Then, it calculates each day's fraction of observed sales, noise level, and an error estimate for how well it made its predictions.
The inference algorithm plugs all those values into the formula to predict daily sales totals. Then, it can sum those totals to get weekly, monthly, or quarterly numbers. Across all 34 companies, the model beat a consensus benchmark - which combines estimates of Wall Street analysts - on 57.2 percent of 306 quarterly predictions.
Next, the researchers are designing the model to analyze a combination of credit card transactions and other alternative data, such as location information. "This isn't all we can do. This is just a natural starting point," Fleder says.
The post Model Beats Wall Street Analysts In Forecasting Business Financials appeared first on Eurasia Review.

Load-Date: December 20, 2019


End of Document




March Madness Bracket Analysis Shows Picking Final Four First Leads To Better Brackets
Eurasia Review
March 5, 2020 Thursday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 619 words
Body


Data science researchers at the University of Illinois have some March Madness advice based on new research: Pick top-seeded teams as the Final Four in your March Madness bracket and work backward and forward from there. If you are going to submit multiple brackets-as you can in the ESPN, CBS Sports and Yahoo Challenges-starting with the Final Four is still a good strategy, but make sure you also diversify your brackets as much as possible.
A paper describing the research behind this advice is published in the American Statistical Association's (ASA)Journal of Quantitative Analysis in Sports(JQAS) by Sheldon H. Jacobson (computer science faculty), Ian Ludden (computer science graduate student), Arash Khatibi (former graduate student) and Douglas M. King (industrial and enterprise systems engineering faculty).
"If you can only pick one bracket, then leaning heavily on the top seeds makes sense," said Jacobson. "However, all bracket challenges allow you to submit multiple entries. A person does not need all of their brackets to score well; just one will do." Jacobson's research on basketball brackets over the past decade has focused entirely on seeds, not teams, making his body of seed-centered work distinct.
Given there are 2^63 possible brackets, which is more than 9 quintillion (9 x 10^18) combinations, picking a bracket with all 63 games correct is highly unlikely, even if you can submit multiple brackets. So, Jacobson suggests focusing on your Final Four teams first, and then building backward and forward from those games. "Once you pick a set of Final Four teams, 12 additional game outcomes become fixed, effectively reducing the number of games that you must pick," Jacobson said. "Our research suggests that anything that can be done to reduce the uncertainty in your picks, while simultaneously expanding the diversity of your pool, will give you a step up in having a good scoring bracket amongst your set of brackets." More information can be found on Jacobson's Bracket Odds website athttp://bracketodds.cs.illinois.edu/pool.html.
"For the 2016 through 2019 tournaments, our models produce many brackets that would have placed in the top 100 of the ESPN bracket challenge." Ludden said. "Our models that start by picking the Elite Eight or Final Four teams perform especially well, perhaps because they balance the two main risks: incorrect picks in the first two rounds, which may propagate through the tournament, and incorrect teams in the later rounds, where each game is worth more points."
Jacobson's seed-centered research has been integrated into the Bracket Odds website. Launched in 2012, the website-labeled as a University of Illinois STEM Learning Laboratory-draws together graduate and undergraduate students to apply data science methods to the tournament. The site has attracted more than 650,000 hits since its inception, providing insights and information for those interested in the mathematics of March Madness.
The website offers a smorgasbord of data analytics for people following the tournament. For example, one of the website calculators gives the probability of all number-one seeds reaching the Final Four to be 0.0155, or around once every 64 tournaments. Meanwhile, the probability of a Final Four comprised of only No. 16 seeds-the lowest-seeded teams in the tournament-is so small that it has a frequency of happening once every 13 trillion tournaments. (For perspective, if an entire tournament was played once every second, the lowest-seeded teams would only meet in the Final Four approximately once every 433,000 years.)
The post March Madness Bracket Analysis Shows Picking Final Four First Leads To Better Brackets appeared first on Eurasia Review.

Load-Date: March 5, 2020


End of Document




Influencing Electoral Outcomes: The Ugly Face Of Facebook - Analysis
Eurasia Review
March 27, 2018 Tuesday


Copyright 2018 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1574 words
Body


By Munish Sharma*
Free and fair elections are the backbone of a democratic system of governance, and they are often celebrated as the "festival of democracy". Election campaigns of political parties and candidates employ a wide variety of strategies and tactics to influence voters. The digital era has added a whole new flavour, be it the eye-catching colossal digital campaigns or instances of foreign governments interfering in the electoral process. Last year, the Presidential elections in both the US and France were controversial due to hacking incidents and data leaks at the campaigns of the Democratic National Committee (DNC) and En Marche, respectively. In general, cyber means of intervention appear to be becoming an inevitable part of the electoral process. The Cambridge Analytica incident proves that India is no exception to this trend. During the next general elections in 2019, the Election Commission of India has an uphill task to thwart both external interference and the abuse of social media platforms to influence voter behaviour.
While there is a long history of external interference in elections both through covert and overt means, digital platforms add a new dimension. News and online content over digital platforms can spread at lightning speed, without paying heed to the credibility or authenticity of the source. Moreover, social media platforms generate vast amounts of data related to the socio-economic conditions, purchasing behaviour, interests, hobbies, and political inclinations or orientations of the users. These details are captured and treasured for commercial purposes. Business analytics feed on this data to generate business intelligence and derive monetary benefits for informed decision making. Present day electoral campaigns are also data driven and they are well-funded to let the campaigners harness data for their own political advantage. Data analytics tools can harvest data from user profiles and sift through the trove to support research, augment targeted campaigns and help political parties in assessing and evaluating their performance. These have been quite effective in targeting swing voters and behaviour forecasting.
As the popularity of social media platforms hits new heights, Facebook and Twitter in particular have been under the scanner of both intelligence agencies and election watchdogs. With close to 2.2 billion active users (by the end of 2017), Facebook alone sits on a stockpile of data which could be used to drive election campaigns towards any preferred outcome. Data in itself is worthless, but data science and the corresponding analytical tools turn it into a goldmine for both businesses and political strategists in the digital age.
Cambridge Analytica, the London-based political consultancy firm presently under the scanner, has an eight-year-old association with Indian elections. It undertook an in-depth electorate analysis for the Bihar Assembly Election in 2010 and, as per the case study details on its website, "the client (political party) achieved a landslide victory, with over 90 percent of total seats targeted by Cambridge Analytica being won."1 This was carried out through Ovleno Business Intelligence, which is an Indian affiliate of Cambridge Analytica's parent firm Strategic Communications Laboratories. The firm had hit media headlines for its association with Donald Trump's election campaign, which it has referred to as "A Full-Scale Data-Driven Digital Campaign". Bringing together the expertise of data scientists, researchers, strategists and content writers in three integrated teams (research, data science, and digital marketing), Cambridge Analytica's campaign helped Trump win the elections.2 The above case studies, mentioned in the Cambridge Analytica website, are prime examples of the vital role data science has begun to play especially in devising techniques to change voter behaviour in the targeted population or audience.
Facebook has played a central role in this entire episode. In a statement, Facebook has accepted that in 2015 a research app for psychologists with the name "thisisyourdigitallife", developed by a psychology professor at Cambridge University, was used for commercial purposes by Cambridge Analytica and other firms in violation of its platform policies. The app, meant for personality prediction, had around 270,000 downloads. Users revealed content related to their likes, preferences, and their own social circles according to their privacy settings.3 The access to Facebook content, in technical terms, was legitimate and through proper channels but the information was passed on to third parties likes Cambridge Analytica and Eunoia Technologies, which exploited it for commercial gains. However, Cambridge Analytica has outright denied allegations of using Facebook data as part of the services rendered to the Trump presidential campaign and while working on the Brexit referendum in the UK.4
As of January 2018, with 250 million users, India is the largest user-base for Facebook. It is also an important tool for the government to take forward its flagship programmes to the wider populace. Facebook is one of the top contenders for partnering with the government's societal development and digital inclusion plans. The Election Commission of India had also partnered with Facebook in 2017, launching a nationwide voter registration campaign.5 Indian users, paying little regard to the privacy terms and condition of social media platforms, uninhibitedly share images, pictures and other content, and are extremely vulnerable to the tools, techniques and campaigns devised for influencing both commercial and political behaviour. Against this backdrop, the government's concerns have been raised by Cambridge Analytica's alleged mining of data from the profiles of 50 million US Facebook users without their consent.6 If such an incident were to occur in India, it would constitute a serious violation of the IT Act. Not just in India, Cambridge Analytica is also at loggerheads with the Electoral Commission in the UK over its alleged role in the BREXIT vote and in Europe for violating EU privacy laws in collusion with Facebook.
Although Facebook has tendered an assurance of data security on its platform for the upcoming elections in India (2019) and Brazil (October 2018), the incident has caused severe damage to its reputation even as a development partner for governments in digital inclusion or other societal benefits plans. As the stakes in elections go up, political parties are unlikely to shy away from leveraging the technical expertise of data analytical firms like Cambridge Analytica fed with expansive data sets harvested from prominent social media platforms.
Data is being extensively harvested and harnessed for commercial purposes, targeted marketing campaigns and to influence consumer choices. It is ethically and legally controversial when information derived without the consent of the users or through dubious means is leveraged to influence political choices. Flourishing in the void of effective legal and regulatory regimes, such incidents seriously undermine the trust of people in the democratic process. To an extent, users understanding the perils of sharing unwanted details or content on social media platforms and aware of their privacy settings is pertinent for containing such instances of abuse. For India, as a functioning democracy, the Cambridge Analytica episode highlights the need to expedite the process of developing a data protection framework and probably amend the IT Act in accordance with the changing realities of cyberspace. The earlier this is realised, the better it would be for the healthy functioning of our democratic systems and processes.
Views expressed are of the author and do not necessarily reflect the views of the IDSA or of the Government of India.
About the author:

*Munish Sharma is Consultant at the Institute for Defence Studies and Analyses, New Delhi.
Source:

This article was published by IDSA.
Notes:
1. "Case Studies - India", Cambridge Analytica Political, available at https://ca-political.com /casestudies/casestudyindia, accessed 23 March, 2018.
2. "Case Studies  Donald J. Trump for President", Cambridge Analytica Political, available at            https://ca-political.com /casestudies, accessed 23 March, 2018.
3. Paul Grewal, "Suspending Cambridge Analytica and SCL Group from Facebook", Facebook Newsroom, March 16, 2018, available at            https://newsroom.fb.com /news/2018/03/suspending-cambridge-analytica/, accessed 25 March, 2018.
4. Cambridge Analytica, "Cambridge Analytica responds to false allegations in the media", available at            https://ca-commercial.com /news/cambridge-analytica-responds-false-allegations-media, accessed 25 March, 2018.
5. "Election Commission of India partners with Facebook to launch first nationwide voter registration reminder", Facebook, June 27, 2017, available at            https://www.facebook.com /notes/facebook/election-commission-of-india-partners-with-facebook-to-launch-first-nationwide-v/1672618862758040/, accessed 23 March, 2018.
6. Prashant Jha, "In eye of Facebook 'data breach' storm, Cambridge Analytica in talks with Cong, BJP for 2019", Hindustan Times, March 19, 2018, available at            https://www.hindustantimes.com /india-news/suspended-by-facebook-over-data-breach-cambridge-analytica-in-talks-with-congress-bjp-for-2019/story-g7J9rodV24lgCK7Xqu5eaO.html, accessed 23 March, 2018.

Load-Date: March 27, 2018


End of Document




How Climate Change Affects Crops In India
Eurasia Review
June 18, 2019 Tuesday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 602 words
Body


Kyle Davis is an environmental data scientist whose research seeks to
increase food supplies in developing countries. He combines techniques 
from environmental science and data science to understand patterns in 
the global food system and develop strategies that make food-supply 
chains more nutritious and sustainable.
Since joining the Data Science Institute as a postdoctoral fellow in  September 2018, Davis has co-authored four papers, all of which detail  how developing countries can sustainably improve their crop production.  For his latest study, he focuses on India, home to 1.3 billion people,  where he led a team that studied the effects of climate on five major  crops: finger millet, maize, pearl millet, sorghum and rice.
These crops  make up the vast majority of grain production during the  June-to-September monsoon season - India's main growing period - with  rice contributing three-quarters of the grain supply for the season.  Taken together, the five grains are essential for meeting India's  nutritional needs.
And in a paper published in Environmental Research Letters,  Davis found that the yields from grains such as millet, sorghum, and  maize are more resilient to extreme weather; their yields vary  significantly less due to year-to-year changes in climate and generally  experience smaller declines during droughts. But yields from rice,  India's main crop, experience larger declines during extreme weather  conditions.
"By relying more and more on a single crop - rice - India's  food supply is potentially vulnerable to the effects of varying  climate," said Davis, the lead author on the paper, "Sensitivity of  Grain Yields to Historical Climate Sensitivity in India," which has four  co-authors, all of whom collaborated on the research.
"Expanding the area planted with these four alternative grains can 
reduce variations in Indian grain production caused by extreme climate, 
especially in the many places where their yields are comparable to 
rice," Davis added. "Doing so will mean that the food supply for the 
country's massive and growing population is less in jeopardy during 
times of drought or extreme weather."
Temperatures and rainfall amounts in India vary from year to year 
and influence the amount of crops that farmers can produce. And with 
episodes of extreme climate such as droughts and storms becoming more 
frequent, it's essential to find ways to protect India's crop production
from these shocks, according to Davis.
The authors combined historical data on crop yields, temperature,  and rainfall. Data on the yields of each crop came from state  agricultural ministries across India and covered 46 years (1966-2011)  and 593 of India's 707 districts.
The authors also used modelled data on  temperature (from the University of East Anglia's Climate Research  Unit) and precipitation (derived from a network of rain gauges  maintained by the Indian Meteorological Department). Using these climate  variables as predictors of yield, they then employed a linear mixed  effects modelling approach - similar to a multiple regression - to  estimate whether there was a significant relationship between  year-to-year variations in climate and crop yields.
"This study shows that diversifying the crops that a country grows 
can be an effective way to adapt its food-production systems to the 
growing influence of climate change," said Davis. "And it adds to the 
evidence that increasing the production of alternative grains in India 
can offer benefits for improving nutrition, for saving water, and for 
reducing energy demand and greenhouse gas emissions from agriculture."

Load-Date: June 18, 2019


End of Document




Facebook, Cambridge Analytica And Surveillance Capitalism - OpEd
Eurasia Review
March 23, 2018 Friday


Copyright 2018 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1429 words
Body


Whether it creeps into politics, marketing, or simple profiling, the nature of surveillance as totality has been affirmed by certain events this decade.  The Edward Snowden disclosures of 2013 demonstrated the complicity and collusion between Silicon Valley and the technological stewards of the national security state.
It took the election of Donald J. Trump in 2016 to move the issue of social media profiling, sharing and targeting of information, to another level.  Not only could companies such as Facebook monetise their user base; those details could, in turn, be plundered, mined and exploited for political purpose.
As a social phenomenon, Facebook could not help but become a juggernaut inimical to the private sphere it has so comprehensively colonised.  "Facebook in particular," claimed WikiLeaks' Julian Assange in May 2011, "is the most appalling spy machine that has ever been invented." It furnished "the world's most comprehensive database about people, their relationships, their names, their addresses, their locations, their communications with each other, and their relatives, all sitting within the United States, all accessible to US intelligence."
Now, the unsurprising role played by Cambridge Analytica with its Facebook accessory to politicise and monetise data reveals the tenuous ground notions of privacy rest upon.  Outrage and uproar has been registered, much of it to do with a simple fact: data was used to manipulate, massage and deliver a result to Trump  or so goes the presumption.  An instructive lesson here would be to run the counter-factual: had Hillary Clinton won, would this seething discontent be quite so enthusiastic?
Be that as it may, the spoliations of Cambridge Analytica are embedded in a broader undertaking: the evisceration of privacy, and the generation of user profiles gathered through modern humanity's most remarkable surveillance machine.  The clincher here is the link with Facebook, though the company insists that it "received data from a contractor, which we deleted after Facebook told us the contractor had breached their terms of service."
Both Facebook and Cambridge Analytica have attempted to isolate and distance that particular contractor, a certain Aleksandr Kogan, the Cambridge University researcher whose personality quiz app "thisisyourdigitallife" farmed the personal data of some 50 million users who were then micro-targeted for reasons of political advertising.
The sinister genius behind this was the ballooning from the initial downloads  some 270,000 people  who exchanged personal data on their friends including their "likes" for personality predictions.  A broader data set of profiles were thereby created and quarried.
Kogan claims to have been approached by Cambridge Analytica, rather than the other way around, regarding "terms of usage of Facebook data".  He was also reassured that the scheme was legal, being "commercial" in nature and typical of the way "tens of thousands of apps" were using social media data.  But it took Cambridge Analytica's whistleblower, Christopher Wylie, to reveal that data obtained via Kogan's app was, in fact, used for micro-targeting the US electorate in breach of privacy protocols.
Mark Zuckerberg's response has entailed vigorous hand washing.  In 2015, he claims that Facebook had learned that Cambridge Analytica shared data from Kogan's app.  "It is against our policies for developers to share data without other people's consent, so we immediately banned Kogan's app from our platform". Certifications were duly provided that such data had been deleted, though the crew at Facebook evidently took these at unverified face value.  Not so, as matters transpired, leading to the claim that trust had not only been breached between Facebook, Kogan and Cambridge Analytica, but with the users themselves.
Facebook, for its part, has been modestly contrite.  "We have a responsibility to protect your data," went Zuckerberg in a statement, "and if we can't then we don't deserve to serve you."  His posted statement attempts to water down the fuss.  Data protections  most of them, at least  were already being put in place. He described the limitations placed on the accessing of user information by data apps connected to Facebook friends.
The networked sphere, as it is termed in with jargon-heavy fondness by some academics, has seen the accumulation of data all set and readied for the "information civilisation".  Google's chief economist Hal Varian has been singled out for special interest, keen on what he terms, in truly benign fashion, "computer-mediated transactions".  These entail "data extraction and analysis," various "new contractual forms" arising from "better monitoring", "personalisation and customisation" and "continuous experiments".
Such are the vagaries of the information age. As a user of such freely provided services, users are before a naked confessional, conceding and surrendering identities to third parties with Faustian ease.  This surrender has its invidious by products, supplying intelligence and security services accessible data.
Cambridge Analytica, for its part, sets itself up as an apotheosis of the information civilisation, a benevolent, professionally driven information hitman. "Data drives all we do," it boldly states to potential clients.  "Cambridge Analytica uses data to change audience behaviour."
This sounds rather different to the company's stance on Saturday, when it claimed that, "Advertising is not coercive; people are smarter than that."  With cold show insistence, it insisted that, "This isn't a spy movie."
Two services are provided suggesting that people are not, in the minds of its bewitchers, that intelligent: the arm of data-driven marketing designed to "improve your brand's marketing effectiveness by changing consumer behaviour" and that of "data-driven campaigns" where "greater influence" is attained through "knowing your electorate better".
On the latter, it is boastful, claiming to have supported over 100 campaigns across five continents. "Within the United States alone, we have played a pivotal role in winning presidential races as well as congressional and state elections."
CA has donned its combat fatigues to battle critics.  Its Board of Directors has suspended CEO Alexander Nix, claiming that "recent comments secretly recorded by Channel 4 and other allegations do not represent the values or operations of the firm and his suspension reflects the seriousness with which we view this violation."
The comments in question, caught in an undercover video, show Nix offering a range of services to the Channel 4 undercover reporter: Ukrainian sex workers posing as "honey-traps" a video evidencing corruption that might be uploaded to the Internet; and operations with former spies. "We can set up fake IDs and Web sites, we can be students doing research projects attached to a university; we can be tourists."
The company has also attempted to debunk a set of what it sees as flourishing myths.  It has not, for instance, been uncooperative with the UK's data regulator, the Information Commissioner's Office, having engaged it since February 2017.  It rejects notions that it peddles fake news. "Fake news is a serious concern for all of us in the marketing industry."  (Nix's cavalier advertising to prospective clients suggests otherwise.)
In other respects, Cambridge Analytica also rejected using Facebook data in its political models, despite having obtained that same data.  "We ran a standard political data science program with the same kind of political preference models used by other presidential campaigns."  Nor did it use personality profiles for the 2016 US Presidential election. Having only hopped on board in June, "we focused on the core elements of a core political data science program."
The company's weasel wording has certainly been extensive.  Nix has done much to meander, dodge and contradict.  On the one hand, he would like to take credit for the company's product  the swaying of a US election.  But in doing so, it did not use "psychographic" profiles.
Surveillance capitalism is the rope which binds the actors of this latest drama in the annals of privacy's demise.  There are discussions that political data mining designed to manipulate and sway elections be considered in the same way political donations are.  But in the US, where money and political information are oft confused as matters of freedom, movement on this will be slow.  The likes of Cambridge Analytica and similar information mercenaries will continue thriving.

Load-Date: March 23, 2018


End of Document




Surface Clean-Up Technology Won't Solve Ocean Plastic Problem
Eurasia Review
August 4, 2020 Tuesday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 717 words
Body


Clean-up devices that collect waste from the ocean surface won't solve the plastic pollution problem, a new study shows.
Researchers compared estimates of current and future plastic waste with the ability of floating clean-up devices to collect it - and found the impact of such devices was "very modest". However, river barriers could be more effective and - though they have no impact on plastic already in the oceans - they could reduce pollution "significantly" if used in tandem with surface clean-up technology.
The study - by the University of Exeter, the Leibniz Centre for Tropical Marine Research, the Leibniz Institute for Zoo and Wildlife Research, Jacobs University and Making Oceans Plastic Free - focusses on floating plastic, as sunk waste is difficult or impossible to remove depending on size and location.
The authors estimate that the amount of plastic reaching the ocean will peak in 2029, and surface plastic will hit more than 860,000 metric tonnes - more than double the current estimated 399,000 - by 2052 (when previous research suggested the rate of plastic pollution may finally reach zero).
"The important message of this paper is that we can't keep polluting the oceans and hoping that technology will tidy up the mess," said Dr Jesse F. Abrams, of the Global Systems Institute and the Institute for Data Science and Artificial Intelligence, both at the University of Exeter.
"Even if we could collect all the plastic in the oceans - which we can't - it's really difficult to recycle, especially if plastic fragments have floated for a long time and been degraded or bio-fouled.
"The other major solutions are to bury or burn it - but burying could contaminate the ground and burning leads to extra CO2 emissions to the atmosphere."
Private initiatives proposing to collect plastic from oceans and rivers have gained widespread attention recently.
One such scheme, called the Ocean Cleanup, aims to clean the "Pacific garbage patch" in the next 20 years using 600m floating barriers to collect plastic for recycling or incineration on land.
The new study analysed the impact of deploying 200 such devices, running without downtime for 130 years - from 2020 to 2150.
In this scenario, global floating plastic debris would be reduced by 44,900 metric tonnes - just over 5% of the estimated global total by the end of that period.
"The projected impact of both single and multiple clean up devices is very modest compared to the amount of plastic that is constantly entering the ocean," said Dr Sönke Hohn, of Leibniz Centre for Tropical Marine Research.
"These devices are also relatively expensive to make and maintain per unit of plastic removed."
As most plastic enters the oceans via rivers, the authors say a "complete halt" of such pollution entering the ocean using river barriers - especially in key polluting rivers - could prevent most of the pollution they otherwise predict over the next three decades.
However, due to the importance of large rivers for global shipping, such barriers are unlikely to be installed on a large scale.
Given the difficulty of recycling and the negative impacts of burying or burning plastic, the study says reducing disposal and increasing recycling rates are essential to tackle ocean pollution. "Plastic is an extremely versatile material with a wide range of consumer and industrial applications, but we need to look for more sustainable alternatives and rethink the way we produce, consume and dispose of plastic," said Professor Agostino Merico, of Leibniz Centre for Tropical Marine Research and Jacobs University.
Dr Roger Spranz, an author of the study, is a co-founder of non-profit organisation Making Oceans Plastic Free.
"We have developed expertise in changing behaviour to break plastic habits and stop plastic pollution at its source," Dr Spranz said.
"We are registered in Germany but the focus of our activities and collaborations is in Indonesia, the second-largest source of marine plastic pollution.
"Working with local partners, the implementation of our Tasini campaign in Indonesia has to date helped to prevent an estimated 20 million plastic bags and 50,000 plastic bottles from ending up in coastal areas and the ocean."
The article Surface Clean-Up Technology Won't Solve Ocean Plastic Problem appeared first on Eurasia Review.

Load-Date: August 5, 2020


End of Document




More Evidence Of Causal Link Between Air Pollution And Early Death
Eurasia Review
June 29, 2020 Monday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 579 words
Body


Strengthening U.S. air quality standards for fine particulate pollution to be in compliance with current World Health Association (WHO) guidelines could save more than 140,000 lives over the course of a decade, according to a new study from Harvard T.H. Chan School of Public Health.
The study, published June 26, 2020 inSciences Advances, provides the most comprehensive evidence to date of the causal link between long-term exposure to fine particulate (PM2.5) air pollution and premature death, according to the authors.
"Our new study included the largest-ever dataset of older Americans and used multiple analytical methods, including statistical methods for causal inference, to show that current U.S. standards for PM2.5 concentrations are not protective enough and should be lowered to ensure that vulnerable populations, such as the elderly, are safe," said doctoral student Xiao Wu, a co-author of the study.
The new research builds on a 2017 study that showed that long-term exposure to PM2.5 pollution and ozone, even at levels below current U.S. air quality standards, increases the risk of premature death among the elderly in the U.S.
For the new study, researchers looked at 16 years' worth of data from 68.5 million Medicare enrollees--97% of Americans over the age of 65--adjusting for factors such as body mass index, smoking, ethnicity, income, and education. They matched participants' zip codes with air pollution data gathered from locations across the U.S. In estimating daily levels of PM2.5 air pollution for each zip code, the researchers also took into account satellite data, land-use information, weather variables, and other factors. They used two traditional statistical approaches as well as three state-of-the-art approaches aimed at teasing out cause and effect.
Results were consistent across all five different types of analyses, offering what authors called "the most robust and reproducible evidence to date" on the causal link between exposure to PM2.5 and mortality among Medicare enrollees--even at levels below the current U.S. air quality standard of 12 ?g/m3 (12 micrograms per cubic meter) per year.
The authors found that an annual decrease of 10 ?g/m3 in PM2.5 pollution would lead to a 6%-7% decrease in mortality risk. Based on that finding, they estimated that if the U.S. lowered its annual PM2.5 standard to 10 ?g/m3--the WHO annual guideline--143,257 lives would be saved in one decade.
The authors included additional analyses focused on causation, which address criticisms that traditional analytical methods are not sufficient to inform revisions of national air quality standards. The new analyses enabled the researchers, in effect, to mimic a randomized study--considered the gold standard in assessing causality--thereby strengthening the finding of a link between air pollution and early death.
"The Environmental Protection Agency has proposed retaining current national air quality standards. But, as our new analysis shows, the current standards aren't protective enough, and strengthening them could save thousands of lives. With the public comment period for the EPA proposal ending on June 29, we hope our results can inform policymakers' decisions about potentially updating the standards," said co-author Francesca Dominici, Clarence James Gamble Professor of Biostatistics, Population, and Data Science.
The article More Evidence Of Causal Link Between Air Pollution And Early Death appeared first on Eurasia Review.

Load-Date: June 29, 2020


End of Document




There's No End In Sight To The Zombie Economy  OpEd
Eurasia Review
June 2, 2020 Tuesday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 988 words
Body


By Andrew Moran*
The United States was waiting for the zombie apocalypse. The country was given a coronapocalypse instead. But could the two events merge and provide the nation with a dangerous economic trend? Corporate America's worst-kept secret had been the swelling number of zombies kept on life support and hidden away during the boom phase of the business cycle. Now that the coronavirus pandemic has exposed the fault lines underneath theeconomy, the zombification may accelerate due to a toxic concoction of Federal Reserve stimulus and congressional relief. Will zombies leave their graves, searching for freshly created US dollars and feasting on the carcass of the ailing marketplace?
The Walking Dead
Azombiecompany is a business that requires perpetual bailouts to keep its doors open, or it is a deeply indebted firm that can only repay the interest on its debt. The zombification has been eating away at Japan and China, and now it is gradually infecting the US economy. What's worse is that American zombie businesses employ about 2 million people, according to new Arbor Data Science figures.
Workers employed by zombie companies are found in many different industries. Arbor's research found that the top five sectors by employee headcount were:
Industrial conglomerates: 233,000Hardware, storage, and peripherals: 193,000Energy equipment and services: 185,000Hotels, restaurants, and leisure: 153,000Software: 142,000
But could the fragility of the market cause a huge number of workers to file for unemployment benefits? It might seem counterintuitive, but it has become a lot easier for these businesses to be resuscitated.
One institution has been supplying these walkers with brain nourishment: the Federal Reserve.
Feeding the Undead
Zombies have been finding it easier to borrow for a combination of reasons. The first is that interest rates are historically low, so it can be less difficult to repay the interest on the debt and use the cost savings to keep the lights on. The second is that the US central bank has taken unprecedented action by acquiringcorporate debtthrough the secondary exchange-traded fund (ETF) market.
Put simply, if it were not for accommodative monetary policy, these firms would have otherwise shut down by now. Once again, the Eccles Building is refusing to allow the invisible hand to rein in the excess for fear a liquidity crisis, credit crunch, and every other fancy way of saying we are in a dilly of a pickle.
But it is the entire market that is taking advantage of a desperate Fed. US companies are borrowing at the fastest pace in history year-to-date, issuing more than $1 trillion in new bonds, according to Bank of America Global. This is about double the number from 2019 during the same period. This eyebrow-raising number highlights two important facts in this environment: Companies are borrowing more cheaply than anybody would have anticipated last year, and investors are being paid so little to fund operations in an uncertain market.
MarketWatch alluded to AutoNation as a struggling company that recently borrowed $500 million from the bond market. The national chain of car dealerships posted a $232.3 million net loss in the first quarter, but it still attracted investor interest due to its 4.4 percent yield. Under present conditions, traders know that their investment is insured, because the central bank could just swoop in and rescue a troubled asset to avoid a powder keg from going off.
And it is not just Fed chair Jerome Powell going on a bond-buying spree. There is still obviously a demand for all kinds of bondsinvestment grade and junk statusand this is fueling the rise of moribund companies that are gorging on debt. The undead can roam the streets for several more years if this is thede factomonetary policy.
Zombification
In Tokyo and Beijing, the typical walking dead are the banks. In the postcoronavirus economy, it is evident that the next generation of zombies will be cruise lines, retailers, and airlines. In a truly free market, these industries would eliminate a large number of companies, but because of cheap money and an accommodative Fed, that is not going to happen. Many of these businesses are also benefiting from creditors waiving or loosening previous debts, allowing some of the world's largest organizations to reach noteworthy agreements with lenders.
This is terrible news for Main Street, because now capital is being misallocated and transferred to unproductive enterprises. Businesseslarge or smallwith high growth prospects may not see their visions realized, because the money is going to prop up the Marriotts and Vail Resorts of the world.
With the death of small businesses potentially nigh, the entrepreneurial spirit may wither away before it even has a chance to be zombified, because it cannot access as much liquidity as the big boys.
Money Printer Go Brrr
The ramifications of these whatever-it-takes and money printer go brrr policies will only be felt in a few years, when the Fed chooses to tighten up and remove the training wheels. We have seen what happens when the Fed scales back its quantitative easing efforts: triple-digit losses on the stock market and ballooning debt-servicing payments. If this ever happens, a tidal wave of debt defaults and bankruptcies will swallow the US. Should the central bank refrain from embarking upon a prescription of tightening, a new type of economy will be born: the anti-productivity economy, comparable to Japan in the 1990s and China today. You could even make a movie out of it: Big Trouble in Little Tokyo: Dawn of the American Dead.
About the author: Andrew Moran is the Economics Correspondent at LibertyNation.com and is the author of The War on Cash. You can find more of his work at AndrewMoran.net.
Source: This article was originally published by LibertyNation.
The post There's No End In Sight To The Zombie Economy - OpEd appeared first on Eurasia Review.

Load-Date: June 2, 2020


End of Document




Limits To Strategic Foresight: Try Wisdom Of The Crowds  Analysis
Eurasia Review
May 21, 2020 Thursday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1292 words
Body


Even for a nation with Singapore's foresight capability, the full-range of COVID-19's consequences could not have been foreseen. Would not now be the ideal time to revisit key tenets of Singapore's foresight enterprise  and by implication our national security framework  and perhaps limiting the impact of future strategic surprise?
By Shashi Jayakumar and Adrian W J Kuah*
Could Singapore, with its strategic foresight capability, have predicted COVID-19? It is reasonable to ask this, since Singapore is admired globally for its strategic foresight capability. Two milestones stand out in the development of this capability.
The first occurred in the mid-1980s, when the Government experimented with using long-term scenarios in defence planning and not long after rolled it out for broader use across the different ministries. This led to the practice of "National Scenarios" that forms, at least partly, the basis for long-term planning. The second was a major review which in 2004 led to the Strategic Framework for Singapore's National Security.
Swan, Elephant, or Something Else?
Deputy Prime Minister and Coordinating Minister for Security and Defence Dr Tony Tan's 2005Ministerial Statementon the Framework observed that "the most important recommendation" was to "set up a new Coordinating Structure in the centre of Governmentin the Prime Minister's Office."
Dr Tan, in highlighting the importance of investing in "imaginative solutions to intractable problems", also announced the establishment of a Risk Assessment and Horizon Scanning capability intended "to help us anticipate and deal with shocks to our system."
The Risk Assessment and Horizon Scanning Programme Office was set up within NSCS, alongside the other strategic foresight apparatus of the Government such as the Centre for Strategic Futures in the Prime Minister's Office.
Returning to the question posed above: was COVID-19 inherently unpredictable? Was it a "black swan" that would have defied the best foresight capability? Or was COVID-19 a "black elephant", the sort of catastrophic problem identified in advance but ultimately ignored, sometimes wilfully?
But maybe there is a third possibility ? that catastrophic surprise is context specific: it is not simply that it is unforeseen and unforeseeable, but seems that way because of the limits imposed by one's perspective and experiences.
Limits to Whole-of-Government Approach?
Notwithstanding the lapse when it came to assessing the magnitude of risk to foreign worker dormitories, agencies have not been unprepared. They have for years been anticipating, and preparing for, disruptions to food security (through source diversification) and infectious diseases outbreaks (the opening of the National Centre for Infectious Diseases in 2019 being a case in point).
Still, COVID-19 should give us pause: there is the opportunity to use this crisis, and do deep thinking on the our strategic foresight enterprise, and cognitive diversity within.
A horizon scanning system, for example, combines data science and data analytics with the craft and intuition of the analyst, the scenario planner or the futurist, so that we can mitigate the failure of imagination and the lack of communication that allows people to feign ignorance and thereafter act surprised.
A well-designed horizon scanning system is underpinned by Ashby's Law of Requisite Variety, encapsulated in the aphorism, "it takes complexity to defeat complexity." To better able to intuit catastrophic surprises, it helps if we look in different directions, have different mental models, and are constantly and courageously challenging each other's interpretations of what is going on.
So, could a horizon scanning system have foreseen COVID-19? Alas, probably not, and certainly not in its painful detail. But COVID-19 has shown that the best whole-of-government efforts must be enhanced. To be sure, centralising strategic foresight and horizon scanning capability within the Government, albeit with frequent consultations with outsiders, certainly makes for greater efficiency.
Wisdom of the Crowds: Look Beyond Government
But it also creates a single point of failure. To prevent this, we need to inject the requisite diversity of viewpoints and expertise necessary to detect and tackle emerging threats. Needless to say, this more distributed approach will have to trade off efficiency for robustness (and messiness).
If complexity is required to defeat complexity, then the diversity of expertise, knowledge and insights that can help Singapore deal with these emerging threats may well come as much from outside of Government  from the usual quarters such as universities and think tanks, as well as unexpected ones such as NGOs and loving critics  as from within.
In his 2004 "The Wisdom of Crowds", James Surowiecki suggested that, under certain conditions, a sufficiently diverse and independent group could produce more accurate forecasts and guesses than so-called experts.
To mitigate that "single point of failure", the strategic foresight enterprise should be less of a "Government-hub and different spokes" model, and more of a decentralised network with diverse and independent nodes simultaneously complementing and countering each other in making sense of the world. This idea has been put into practice by various corporations and government agencies, in the form of hackathons, crowdsourcing for ideas, and prediction markets.
There is a practical consideration. No matter how enlightened a government or organisation is, there is a limit to inconvenient albeit useful insights it is willing to accept, much less embrace, especially if surfaced internally. Moreover, an internal group established to counter groupthink can itself on occasion be susceptible to groupthink.
By building capabilities outside of the Government or drawing on pre-existing ones, we can shift the cognitive odds in our favour even though the foresight enterprise remains a fallible one.
National Security: Time for Review
A distributed, extra-Government horizon scanning process provides a platform for multiple interpretations of ambiguous weak signals to occur. Incompatible interpretations of the same signal, far from causing consternation, are instructive: it tells you that it does not conform to any previous pattern, and we should pay it even more attention, instead of trying to force a convergence of views.
This is where culture comes into play: we should further develop a culture that takes on board potentially useful albeit unpalatable insights surfaced by outsiders.
We are now at a point where national security and strategic foresight need to encompass and embrace that emerging range of uncomfortable novelties: not just pandemics, but food security, climate change, and grey zone operations such as disinformation. Some 15 years ago, Singapore's national security enterprise placed resilience and whole-of-government coordination front and centre, and augmented its foresight capabilities with horizon scanning.
Given how the world has changed because of COVID-19 (though not only because of it), another review may be in order, one that expands the national security discourse from one that is whole-of-government to whole-of-society, in order to tap the wisdom of the crowds.
*Shashi Jayakumar is Head, Centre of Excellence for National Security (CENS) and Executive Coordinator for Future Issues and Technology (FIT) cluster at the S. Rajaratnam School of International Studies (RSIS), Nanyang Technological University (NTU), Singapore. Adrian W J Kuah is Director, Futures Office at the National University of Singapore. This is part of an RSIS Series.
The post Limits To Strategic Foresight: Try Wisdom Of The Crowds - Analysis appeared first on Eurasia Review.

Load-Date: May 21, 2020


End of Document




Baseball Illustrates Economics With Each Game  OpEd
Eurasia Review
December 28, 2019 Saturday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1417 words
Body


By Andrew Moran*
Legendarybaseballmanager Tommy Lasorda wrote, "No matter how good you are, you're going to lose one-third of your games. No matter how bad you are you're going to win one-third of your games. It's the other third that makes the difference." Baseball is a game of failure; you are going to groundout, flyout, and strikeout more than you are going to get a base hit or a walk. Are you batting .289? You are having a good season.
Baseball is a unique sport because nothing else can replicate it. The thinking man's game is also a great mechanism to learn abouteconomics. Let's explore some of the various economic principles that Major League Baseball (MLB) teaches us.
Creative Destruction
In economics, creative destruction, also known as Schumpeter's gale, is the process of ending conventional practices to make room for innovation. You will find creative destruction in every industry, whether it is transportation or finance. It is even ubiquitous in The Show.
Baseball is in a constant state of evolution. The greatest sport on the planet routinely innovates thanks to the ingenuity and creativity of players, managers, and front offices. From the way pitchers throw from the stretch to the way the defense is positioned on the diamond, ballclubs are always searching for a competitive edge, and the only way to achieve this is by employing unique strategies and tactics.
The best example of creative destruction in the MLB is the infield shift.Although the defensive shift has been around for a century, this realignment oddity has become the norm in the last decade (thanks, Joe Maddon!). You will see third basemen playing on the other side of second base to prevent left-handed batters from placing the ball into the gap between fielders. Also, thanks to advanced analytics, teams are further maneuvering players to spots where the hitters  left- and right-handed  are more likely to place their balls.
You are starting to witness, on occasion, the outfield doing the shift. It has become so prevalent due to its efficacy that there has been a discussion about banning the measure. Until then, the shift will be used and refined by franchises across the league.It might not be as exciting as seeing a bullet into the gap in shallow right-field, but the shift is effective.
Subjective Value
Thesubjective theoryof value is the concept that a good's or a service's worth is not innate but rather determined by consumers based on how much they want or need the object. This economic principle can be found in every component of professional baseball. Everyone is participating in it, too: the owners, the players, and even the fans.
Many people will say that it is ridiculous to pay one athlete $324 million over nine years. But that is exactly what the New York Yankees did when they signed Gerrit Cole. The Boston Red Sox thought it was worth signing David Price to a $217 million contract. The Los Angeles Angels opened the checkbook and inked an aging Albert Pujols to a 10-year, $240 million deal.
This is a lot of money to spend on just one guy, especially when you have nine players on the field and 25 men on the main roster. Owners who sign these players have a couple of things in mind when handing out such lucrative contracts, including marketing dollars and championships. If your team has a superstar, then the idea is that fans are more likely to see the team. If that player helps the club win a championship, then paying a single person $27 million a year was worth the cost.
Fans make similar decisions but on a much smaller scale. When they visit Fenway Park (Red Sox) or Citi Field (New York Mets), they are paying $30, $50, or $200 for tickets, proving that they value the tickets more than the cash. Plus, these fans will pay premium prices for hot dogs and beer, showing, once again, that they prefer to consume ballpark food and drinks than to keep their hard-earned money.
To non-baseball fans, It might seem like a waste of money and time. However, baseball aficionados might feel the same way aboutStar Warsnerd culture and grown men buying fake lightsabers and pretending to be Luke Skywalker at Comic-Con. It is all about subjective value.
Self-Regulating Markets
You may have heard about the Houston Astros' sign-stealing controversy from the 2017 season and postseason. If you have not, let's just say that it consisted of overhead cameras, trashcan banging, and whistling. While many teams and players suspected foul play on the part of the 2017 World Series champions, nothing was ever proven and little national attention was generated. The MLB has initiated an exhaustive investigation and is expected to release the results of the probe in early 2020.
Sign-stealing is common in baseball, just not on the alleged level of the Astros. A runner on second base, for example, can try to determine what the pitcher is going to throw and then convey to the hitter what type of pitch he can expect. A pitcher might have a tell, revealing to batters what he is going to toss from the mound. Teams are, rightfully, so paranoid about sign-stealing that they have taken matters into their own hands.
If you watch a random MLB broadcast, you might notice the back catcher delivering multiple signs tothe pitcher. The purpose of this act is to ensure that the opposing squad cannot quickly and accurately determine the signal if the coming pitch is high cheese with some hair on it or salad (consult your nearest Ecktionary for definitions).
Rather than wait for rule changes or bans, some teams are attempting to eliminate sign-stealing with complex signs and unique communication. Should clubs fail to correctly decipher what signs are being shown, theywillinevitably shut down operations and try to win games by just being the better team through tactics and athleticism. This is how markets regulate themselves without any state interventions.
Labor Negotiations
How did you land your job? Chances are, you worked on an arrangement with the hiring manager that satisfied both parties.Liberty Nationrecently reported on the $800 million in contracts that agent Scott Boras helped negotiate during the 2019 winter meetings in San Diego, CA. For all the insults thrown his way, he is an effective agent who gets the job done by generating top dollar for his clients. Ask Cole, ask Anthony Rendon, and ask Stephen Strasburg.
In baseball, there is no salary cap, meaning teams can spend any amount they wish on a player of their choosing. Depending on the state of the market and the teams' needs, elite athletes usually have the advantage. They typically demand a few things: dollar-amount, length of the contract, and average annual value (AAV). A team may offer a five-year deal worth $143 million, but a player may request a six-year deal valued at $146 million.
To get what they want, athletes will have the best people around them negotiating and ironing out the terms and conditions of a lucrative contract. General managers may attempt to sweeten the pot by throwing in incentives: If a player finishes in the top 10 of MVP voting each season, then he will earn an extra $2 million a year  or a pitcher may need three consecutive seasons of 200 or more innings to receive a $4 million bonus and a club option.
Sometimes it works and sometimes it doesn't. The point is all about labor relations and how employers and employees agree on a working partnership.
As American As Apple Pie
Today, every team's front office is filled with employees holding degrees in economics, mathematics, or data science. Baseball was always a game of strategy, but now this has morphed into intricate and obscure measurements, such as xFIP, BABIP, and REW  all Sabermetrics that would trigger headaches for non-baseball fans and non-STEM majors. Key performance indicators are integral to the sport, but baseball is also a fantastic physical display of the various laws and principles of economics. In a world where this discipline is portrayed with great disdain, baseball can breathe some life into the science.
*About the author: Economics Correspondent at LibertyNation.com. Andrew has written extensively on economics, business, and political subjects for the last decade. He also writes about economics at Economic Collapse News and commodities at EarnForex.com. He is the author of "The War on Cash." You can learn more at AndrewMoran.net.
Source: This article was published by Liberty Nation
The post Baseball Illustrates Economics With Each Game - OpEd appeared first on Eurasia Review.

Load-Date: December 27, 2019


End of Document




More Than A Lifetime Away: World Faces 100-Year Wait For Gender Parity
Eurasia Review
December 17, 2019 Tuesday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1993 words
Body


The time it will take to close the gender gap narrowed to 99.5 years in 2019. While an improvement on 2018  when the gap was calculated to take 108 years to close  it still means parity between men and women across health, education, work and politics will take more than a lifetime to achieve. This is the finding of the World Economic Forum's Global Gender Gap Report 2020, published Monday.
According to the report, this year's improvement can largely be ascribed to a significant increase in the number of women in politics. The political gender gap will take 95 years to close, compared to 107 years last year. Worldwide in 2019, women now hold 25.2% of parliamentary lower-house seats and 21.2% of ministerial positions, compared to 24.1% and 19% respectively last year.
Politics, however, remains the area where least progress has been made to date. With Educational Attainment and Health and Survival much closer to parity on 96.1% and 95.7% respectively, the other major battlefield is economic participation. Here, the gap widened in 2019 to 57.8% closed from 58.1% closed in 2018. Looking simply at the progress that has been made since 2006 when the World Economic Forum first began measuring the gender gap, this economic gender gap will take 257 years to close, compared to 202 years last year.
Economic Gap Widening
The report attributes the economic gender gap to a number of factors. These include stubbornly low levels of women in managerial or leadership positions, wage stagnation, labour force participation and income. Women have been hit by a triple whammy: first, they are more highly represented in many of the roles that have been hit hardest by automation, for example, retail and white-collar clerical roles.
Second, not enough women are entering those professions  often but not exclusively technology-driven  where wage growth has been the most pronounced. As a result, women in work too often find themselves in middle-low wage categories that have been stagnant since the financial crisis 10 years ago.
Third, perennial factors such as lack of care infrastructure and lack of access to capital strongly limit women's workforce opportunities. Women spend at least twice as much time on care and voluntary work in every country where data is available, and lack of access to capital prevents women from pursuing entrepreneurial activity, another key driver of income.
"Supporting gender parity is critical to ensuring strong, cohesive and resilient societies around the world. For business, too, diversity will be an essential element to demonstrate that stakeholder capitalism is the guiding principle. This is why the World Economic Forum is working with business and government stakeholders to accelerate efforts to close the gender gap," said Klaus Schwab, Founder and Executive Chairman of the World Economic Forum.
Could the "Role Model Effect" close the gender gap?
One positive development is the possibility that a "role model effect" may be starting to have an impact in terms of leadership and possibly also wages. For example, in eight of the top 10 countries this year, high political empowerment corresponds with high numbers of women in senior roles. Comparing changes in political empowerment from 2006 to 2019 shows that improvements in political representation occurred simultaneously with improvements in women in senior roles in the labour market.
While this is a correlation, not a causation, in OECD countries, where women have been in leadership roles for relatively longer and social norms started to change earlier, role model effects could contribute to shaping labour market outcomes.
Gender Inequality in the Jobs of the Future
Possibly the greatest challenge preventing the economic gender gap from closing is women's under-representation in emerging roles. New analysis conducted in partnership with LinkedIn shows that women are, on average, heavily under-represented in most emerging professions. This gap is most pronounced across our "cloud computing" job cluster where only 12% of all professionals are women. The situation is hardly better in "engineering" (15%) and "Data and AI" (26%), however women do outnumber men in two fast-growing job clusters, "content production" and "people and culture".
According to our data, this reality presents leaders intent on addressing the gender gap in the future with two key challenges. The first and most obvious challenge is that more must be done to equip women with the skills to perform the most in-demand jobs. Indeed, there is an economic cost of not doing so as skills shortages in these professions hold back economic growth.
The second is possibly more complex. According to our data, even where women have the relevant in-demand skillset they are not always equally represented. In data science, for example, 31% of those with the relevant skillset are women even though only 25% of roles are held by women. Likewise, there is no gender gap in terms of skills when it comes to digital specialists, however only 41% of these jobs are performed by women.
These facts point to three key strategies that must be followed to hardwire gender equality into future workforces: to ensure women are equipped in the first place  either through skilling or reskilling  with disruptive technical skills; to follow-up by enhancing diverse hiring; and to create inclusive work cultures.
"Insights from LinkedIn's Economic Graph can help policymakers, business leaders, and educators understand and prepare for how women will be represented in the future workforce. Our data shows that meaningful action is needed to build the systems and talent pipelines required to close the gender gap in tech and ensure women have an equal role in building the future," said Allen Blue, Co-Founder and Vice-President, Product Strategy, LinkedIn.
What the Forum is Doing to Close the Gender Gap
The World Economic Forum's Platform for Shaping the Future of the New Economy and Society aims to close economic gender gaps through both in-country and global industry work. Through Closing the Gender Gap Accelerators, the Forum drives change by setting up action coalitions between relevant ministries and the largest employers in the country to increase female labour force participation, the number of women in leadership positions, closing wage gaps and preparing women for jobs of the future. Additionally, the global business commitment on Hardwiring Gender Parity in the Future of Work mobilizes businesses to commit to hiring 50% women for their five highest growth roles between now and 2022. Finally, the Forum has committed to at least double the current percentage of women participants at the Annual Meeting in Davos-Klosters, Switzerland, by 2030.
"To get to parity in the next decade instead of the next two centuries, we will need to mobilize resources, focus leadership attention and commit to targets across the public and private sectors. Business-as-usual will not close the gender gap  we must take action to achieve the virtuous cycle that parity creates in economies and societies," said Saadia Zahidi, Head of the Centre for the New Economy and Society and Member of the Managing Board, World Economic Forum.
The Global Gender Gap in 2020
Nordic countries continue to lead the way to gender parity. Iceland (87.7%) remains the world's most gender-equal country, followed by Norway (2nd, 84.2%), Finland (3rd, 83.2%) and Sweden (4th, 82.0%). Other economies in the top 10 include Nicaragua (5th, 80.4%), New Zealand (6th, 79.9%), Ireland (7th, 79.8%), Spain (8th, 79.5%), Rwanda (9th, 79.1%) and Germany (10th, 78.7%).
Among the countries that improve the most this year are Spain in Western Europe, Ethiopia in Africa, Mexico in Latin America, and Georgia in Eastern Europe and Central Asia. These countries all improved their positions in the ranking by more than 20 places, largely driven by improvements in the political empowerment dimension.
Western Europe is the best performing region for the 14th consecutive year. With an average score of 76.7% (out of 100), the region has now closed 77% of its gender gap, further improving from last edition. At the current pace, it will take 54 years to close the gap in Western Europe. The region is home to the four most gender-equal countries in the world, namely in order Iceland (87.7%), Norway (84.2%) and Finland (83.2%) and Sweden (82.0%), and one country (Spain, 8th) is among the most improved countries this year.
The North America region regroups the United States (72.4%, 53rd) and Canada (77.2%, 19th). Both countries' performances are stalling, especially in terms of economic participation and opportunity. At this rate it will take 151 years to close the gap.
The Eastern Europe and Central Asia region has closed 71.5% of its gender gap so far with a slight improvement since last year. To date the time to fully close its overall gender gap is estimated to be 107 years. The region has fully closed its educational gap and has improved women's political empowerment which however remains only closed at 15%. 21 of the 26 countries in this region have closed at least 70% and the top-ranked country, Latvia, 11th has closed 78.5% of its gap.
The Latin America and the Caribbean region has closed 72.1% of its gender gap so far, progressing 1 percentage points since last year. At this rate it will take 59 years to close the gender gap. The most noticeable improvement is on the Political empowerment dimension where the region closes its gap by 5 percentage points. Led my Nicaragua that has closed 80.4% of its gap (5th), 15 of the 24 countries covered by the report have improved their overall scores. Among the most improved countries, Mexico reduced its gender gap by 3.4 points on a year-over-year basis.
The Sub-Saharan Africa region has closed 68.0% of its gender gap so far. This result is a significant progress since last edition which leads to revise down the number of years it will take to close the gender gap, which is now estimated at 95 years. The region is home of one of the top-ten countries overall Rwanda (9th) while another 21 countries have improved their performances since last year, including Ethiopia (82nd) one of the best improved this year globally.
The East Asia and Pacific Region has closed 69% of the overall gender gap. If the region maintains the same rate of improvement as the 2006-2019 period, and given the current gap, it will take another 163 years to close the gender gap, the most time of any region. The region has improved on three of the four gender gap dimensions and has been the only region where political empowerment gap has widened (16% closed so far). The best performing country is New Zealand

6th, which has closed 79.9% of its gap. It is followed by the Philippines 16th with 78.1% closed and Lao PDR, 43rd with a score of 73.1%.
South Asia region has closed two thirds of its gender gap. The region's gender gap is the second largest despite a progress of 6 points over the past 14 years. If the rate of progress of the past 15 years was to continue it will take 71 years to close the region's gender gap. However, in contrast with the overall's performance, the region's Economic participation and opportunity gap widens this year. Bangladesh (50th) leads the region, while the second ranked country, Nepal, lags several positions behind (101th)
The Middle East and North Africa (MENA) region obtains the lowest score (61.1%) despite having narrowed its gap by 0.5 points since last year. Assuming the same rate of progress going forward it will take approximately 150 years to close the gender gap in the MENA region. The two most highly ranked countries in the region are Israel

(64th) with a closed gap to date of 71.8% and the United Arab Emirates (120th) with a score of 65.5%. 15 of the 19 countries in this region rank 130th or lower.
The post More Than A Lifetime Away: World Faces 100-Year Wait For Gender Parity appeared first on Eurasia Review.

Load-Date: December 16, 2019


End of Document


