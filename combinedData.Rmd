---
title: "Word Sentiment -- Belen"
author: "Belen Gomez Grimaldi"
date: "3/28/2021"
output:
  html_document:
    toc: TRUE
    theme: journal
    toc_float: TRUE
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, echo=FALSE, message=FALSE}
install.packages("dplyr")
library(dplyr)
install.packages("DT")
library(DT)
library(tidyverse)
library(tidytext)
library(ggwordcloud)
install.packages("gutenbergr") 
library(gutenbergr)
library(textdata)
install.packages("textreadr")
library("textreadr")
install.packages("striprtf")
library(striprtf)
save.image("tidytext.RData")

```

```{r, echo=FALSE, message=FALSE}
grep_clean <- function(data, column, listOfStrings) {
  for (str in listOfStrings) {
    data <- data[!grepl(str, column),]
  }
}
```


## Regional Articles Sentiment Analysis {.tabset}
### Northeast (New York) {.tabset}
```{r, echo=FALSE, message=FALSE, results=FALSE}
# Chunk to clean data

# Read in the articles from the .txt file
ny <- read_lines("ny.txt")

# Clean the data and make it usable
ny <- tibble(ny)

# Cut out the lines that contain TOC
ny <- tail(ny, -1098)

# Cut out any rows containing the following words, which indicate that those rows are not a part of any of the articles

grep_list <- list("Length", "Highlight", "Copyright", "Load-Date", "Section", "Byline", "PM EDT", "PM EST", "AM EST", "Graphic", "www")

for (str in grep_list) {
  ny <- ny[!grepl(str, ny$ny),]
}

exact_list <- list("", " ", "Body", "Link to Image", "End of Document", "Page of ")

for (str in exact_list) {
  ny <- ny[!(ny$ny==str),]
}


ny$ny <- as.character(ny$ny)

# Save the cleaned data for a tf-idf analysis later
clean_ny <- ny

```


```{r, echo=FALSE, message=FALSE}
# Create a new data frame with count of individual words
ny <- ny %>%
  unnest_tokens(word, ny)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)


# Do sentiment analyses for the NE region

ny_afinn <- ny %>%
  inner_join(get_sentiments("afinn"))

ny_nrc <- ny %>%
  inner_join(get_sentiments("nrc"))

ny_bing <- ny %>%
  inner_join(get_sentiments("bing"))
```

#### AFINN
```{r, echo=FALSE, message=FALSE}
# Plot the results
ggplot(data = ny_afinn, 
       aes(x=value)) +
  geom_histogram() +
  ggtitle("New York Sentiment Range") +
  theme_minimal()
```

#### NRC
```{r, echo=FALSE, message=FALSE}
# Plot the results
ggplot(ny_nrc, 
    aes(x = sentiment)) + 
    ggtitle("New York Sentiment Range") +
    geom_bar() +
    theme_minimal()
```

#### Bing
```{r, echo=FALSE, message=FALSE}
# Plot the results
ggplot(ny_bing, 
    aes(x = sentiment, fill=sentiment)) + 
    ggtitle("New York Sentiment Range") +
    geom_bar() +
    theme_minimal() +
    labs(x = 'Sentiment', y = 'Count') +
    scale_color_manual(values = c('red', 'green'))

```


### West Coast (California and Oregon) {.tabset}
```{r, echo=FALSE, message=FALSE, results=FALSE}
# Chunk for cleaning data

west <- read_lines("west.txt")
west <- tibble(west)

# Cut out the lines that contain TOC
west <- tail(west, -800)

# Cut out any rows containing the following words, which indicate that those rows are not a part of any of the articles

grep_list <- list("Length", "Highlight", "Copyright", "Load-Date", "Section", "Byline", "PM PST", "AM PST", "Graphic", "www")

for (str in grep_list) {
  west <- west[!grepl(str, west$west),]
}

exact_list <- list("", " ", "Body", "Link to Image", "End of Document", "Page of ", "Final Edition")

for (str in exact_list) {
  west <- west[!(west$west==str),]
}

west$west <- as.character(west$west)

# Save the cleaned data for later use
clean_west <- west
```

```{r, echo=FALSE, message=FALSE}
west <- west %>%
  unnest_tokens(word, west)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

# Sentiment analysis for West Coast

west_afinn <- west %>%
  inner_join(get_sentiments("afinn"))

west_nrc <- west %>%
  inner_join(get_sentiments("nrc"))

west_bing <- west %>%
  inner_join(get_sentiments("bing"))
```

#### AFINN
```{r, echo=FALSE, message=FALSE}
# Plot the results
ggplot(data = west_afinn, 
       aes(x=value)) +
  geom_histogram() +
  ggtitle("West Coast Sentiment Range") +
  theme_minimal()
```

#### NFC
```{r, echo=FALSE, message=FALSE}
# Plot the results

ggplot(west_nrc, 
    aes(x = sentiment)) + 
    ggtitle("West Coast Sentiment Range") +
    geom_bar() +
    theme_minimal()

```

#### Bing
```{r, echo=FALSE, message=FALSE}
# Plot the results

ggplot(west_bing, 
    aes(x = sentiment, fill=sentiment)) + 
    ggtitle("West Coast Sentiment Range") +
    geom_bar() +
    theme_minimal() +
    labs(x = 'Sentiment', y = 'Count') +
    scale_color_manual(values = c('red', 'green'))
```

### Midwest (Illinois) {.tabset}

```{r, echo=FALSE, message=FALSE, results=FALSE}

# Read in Illinois txt file
il <- read_lines('illinois_articles.txt')

# Convert to tibble for cleaning/analysis
il <- tibble(il)

# Data Cleaning

grep_list <- list("Length", "Highlight", "Copyright", "Load-Date", "Section", "Byline", "PM EDT", "PM EST", "AM EST", "Graphic", "www")

for (str in grep_list) {
  il <- il[!grepl(str, il$il),]
}

exact_list <- list("", " ", "Body", "Link to Image", "End of Document", "Page of ")

for (str in exact_list) {
  il <- il[!(il$il==str),]
}

#Converting to character
il$il <- as.character(il$il)
clean_il <- il
```

```{r, echo=FALSE, message=FALSE}
# Data preparation chunk

# Preparing data for analysis
il_counts <- il %>% unnest_tokens(word, il) %>%
  anti_join(stop_words) %>%
  count(word, sort=TRUE)

# il_counts
il_counts$word <- as.factor(il_counts$word)

```

#### Afinn Analysis
```{r, echo=FALSE, message=FALSE}

# Sentiment Analysis Chunk

# afinn 

il_sentiment_afinn <- il_counts %>%
  inner_join(get_sentiments("afinn"))

# sentiment range plot
ggplot(data = il_sentiment_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("Illinois Sentiment Range")+
  theme_bw() + 
  labs(x = 'Sentiment Value', y = 'Count', title='Illinois Sentiment Range (afinn)') +
  stat_bin(bins=19)
```

#### NRC Analysis
```{r, echo=FALSE, message=FALSE}
# nrc 

il_sentiment_nrc <- il_counts %>% 
  inner_join(get_sentiments("nrc"))

# sentiment range plot
ggplot(data = il_sentiment_nrc, 
       aes(x=sentiment)
        )+
  geom_bar()+
  ggtitle("Illinois Sentiment Range")+
  theme_bw() + 
  labs(x = 'Sentiment', y = 'Count', title='Illinois Sentiment Range') +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.95, hjust=1))
```

#### Bing Analysis
```{r, echo=FALSE, message=FALSE}
# bing

il_sentiment_bing <- il_counts %>% 
  inner_join(get_sentiments("bing"))

# sentiment range plot
ggplot(data = il_sentiment_bing, 
       aes(x=sentiment, fill=sentiment)
        )+
  geom_bar()+
  ggtitle("Illinois Sentiment Range")+
  theme_bw() + 
  labs(x = 'Sentiment', y = 'Count', title='Illinois Sentiment Range') +
  scale_color_manual(values = c('red', 'green'))

```

### South (Florida) {.tabset}
```{r, echo=FALSE, message=FALSE, result=FALSE}

florida <- read_lines("FloridaDataScience.txt")

florida <- tibble(florida)

florida$florida <- as.character(florida$florida)

clean_fl <- florida

```


``` {r, echo=FALSE, message=FALSE}
florida <- florida %>%
  unnest_tokens(word, florida)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

# Sentiment Analysis

florida_afinn <- florida %>%
  inner_join(get_sentiments("afinn"))

florida_nrc <- florida %>%
  inner_join(get_sentiments("nrc"))

florida_bing <- florida %>%
  inner_join(get_sentiments("bing"))

```

#### AFINN Analysis
```{r, echo=FALSE, message=FALSE}
ggplot(data = florida_afinn, 
       aes(x=value)) +
  geom_histogram() +
  ggtitle("Florida Sentiment Range") +
  theme_minimal()

```

#### NRC Analysis
```{r, echo=FALSE, message=FALSE}

ggplot(florida_nrc, 
    aes(x = sentiment)) + 
    ggtitle("Florida Sentiment Range") +
    geom_bar() +
    theme_minimal()


```

#### Bing Analysis
```{r, echo=FALSE, message=FALSE}
ggplot(florida_bing, 
    aes(x = sentiment, fill=sentiment)) + 
    ggtitle("Florida Sentiment Range") +
    geom_bar() +
    theme_minimal() +
    labs(x = 'Sentiment', y = 'Count') +
    scale_color_manual(values = c('red', 'green'))
```

## Comparing Relative Word Frequencies

```{r, echo=FALSE, message=FALSE}
data_prep <- function(x,y,z){
  i <- as_tibble(t(x))
  ii <- unite(i,"text",y:z,remove = TRUE,sep = "")
}

ny_bag <- data_prep(clean_ny,'V1','V2937')

il_bag <- data_prep(clean_il,'V1','V3913')

fl_bag <- data_prep(clean_fl,'V1','V6372')

west_bag <- data_prep(clean_west,'V1','V3108')

region <- c("North East","Midwest","South", "West")


tf_idf_text <- tibble(region,text=t(tibble(ny_bag,il_bag,fl_bag,west_bag,.name_repair = "universal")))

# class(tf_idf_text)

word_count <- tf_idf_text %>%
  unnest_tokens(word, text) %>%
  count(region, word, sort = TRUE)

total_words <- word_count %>% 
  group_by(region) %>% 
  summarize(total = sum(n))

region_words <- left_join(word_count, total_words)

region_words <- region_words %>%
  bind_tf_idf(word, region, n)

region_words

```
