---
title: "Word Sentiment Analysis for Data Science"
author: "Belen Gomez Grimaldi, Kay Mattern, and Amanda Rein"
date: "3/30/2021"
output:
  html_document:
    toc: TRUE
    theme: journal
    toc_float: TRUE
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, echo=FALSE, message=FALSE}
#install.packages("dplyr")
library(dplyr)
#install.packages("DT")
library(DT)
library(tidyverse)
library(tidytext)
library(ggwordcloud)
library(textdata)
#install.packages("textreadr")
library("textreadr")
#install.packages("striprtf")
library(striprtf)
save.image("tidytext.RData")

```

```{r, echo=FALSE, message=FALSE}
grep_clean <- function(data, column, listOfStrings) {
  for (str in listOfStrings) {
    data <- data[!grepl(str, column),]
  }
}
```

## Introduction
In order to get our data, we used LexusNexus website to find different news articles relating to Data Science. For the purpose of analysis, we decided to break up the data collection by region: North East, Midwest, West Coast, Mid-Atlantic, and the South. We ensured that we got 100 articles from each region, usually picking out one or two states to represent the region.

After downloading the data, we had to convert the files into a workable format. For us, that meant converting the files into .txt files, which made the files much easier to read in. After reading in this information, we quickly realized we needed to clean up much of the file. First, we removed all of the rows that contained the table of contents. Next, we removed any rows that were empty or contained just a space. Lastly, we removed any rows that related to page number or the title or formatting of the article. While we were not able to perfectly clean up the data, we definitely were able to remove a lot of the unnecessary lines.

## Regional Articles Sentiment Analysis {.tabset}

After cleaning up the data for each region, we chose to run independent sentiment analyses on each region. We chose to run AFINN, NRC, and Bing sentiment analyses on each region in order to get the most analysis possible to best understand what is going on in each region.

The results of the AFINN analysis showed a similar spread for all of the regions, which indicates that there is a similar range of strength of positive and negative words being used in the articles. Similarly, for the NRC analysis, many of the same emotions seemed to be expressed in all regions. The main difference, it seems, is that in the West Coast and South, there seems to be more negative words being used than in the other areas. On the other hand, the Midwest and North East seem to be the regions using the most positive words. Finally, the Bing analysis showed some differences among regions. While the Mid-Atlantic, South, and West regions used more negative words than positive (which seems to agree with the NRC analysis), the Midwest seems to use many more positive words than negative. This, again, agrees with the NRC analysis. The Northeast seems to be the most balanced, using similar amounts of positive and negative words.

### Northeast (NY) {.tabset}
```{r, echo=FALSE, message=FALSE, results=FALSE}
# Chunk to clean data

# Read in the articles from the .txt file
ny <- read_lines("ny.txt")

# Clean the data and make it usable
ny <- tibble(ny)

# Cut out the lines that contain TOC
ny <- tail(ny, -1098)

# Cut out any rows containing the following words, which indicate that those rows are not a part of any of the articles

grep_list <- list("Length", "Highlight", "Copyright", "Load-Date", "Section", "Byline", "PM EDT", "PM EST", "AM EST", "Graphic", "www")

for (str in grep_list) {
  ny <- ny[!grepl(str, ny$ny),]
}

exact_list <- list("", " ", "Body", "Link to Image", "End of Document", "Page of ")

for (str in exact_list) {
  ny <- ny[!(ny$ny==str),]
}


ny$ny <- as.character(ny$ny)

# Save the cleaned data for a tf-idf analysis later
clean_ny <- ny

```


```{r, echo=FALSE, message=FALSE}
# Create a new data frame with count of individual words
ny <- ny %>%
  unnest_tokens(word, ny)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)


# Do sentiment analyses for the NE region

ny_afinn <- ny %>%
  inner_join(get_sentiments("afinn"))

ny_nrc <- ny %>%
  inner_join(get_sentiments("nrc"))

ny_bing <- ny %>%
  inner_join(get_sentiments("bing"))
```

#### AFINN
```{r, echo=FALSE, message=FALSE}
# Plot the results
ggplot(data = ny_afinn, 
       aes(x=value)) +
  geom_histogram() +
  ggtitle("Northeast Sentiment Range") +
  theme_minimal()
```

#### NRC
```{r, echo=FALSE, message=FALSE}
# Plot the results
ggplot(ny_nrc, 
    aes(x = sentiment)) + 
    ggtitle("Northeast Sentiment Range") +
    geom_bar() +
    theme_minimal()
```

#### Bing
```{r, echo=FALSE, message=FALSE}
# Plot the results
ggplot(ny_bing, 
    aes(x = sentiment, fill=sentiment)) + 
    ggtitle("Northeast Sentiment Range") +
    geom_bar() +
    theme_minimal() +
    labs(x = 'Sentiment', y = 'Count') +
    scale_color_manual(values = c('red', 'green'))

```


### West Coast (CA & OR) {.tabset}
```{r, echo=FALSE, message=FALSE, results=FALSE}
# Chunk for cleaning data

west <- read_lines("west.txt")
west <- tibble(west)

# Cut out the lines that contain TOC
west <- tail(west, -800)

# Cut out any rows containing the following words, which indicate that those rows are not a part of any of the articles

grep_list <- list("Length", "Highlight", "Copyright", "Load-Date", "Section", "Byline", "PM PST", "AM PST", "Graphic", "www")

for (str in grep_list) {
  west <- west[!grepl(str, west$west),]
}

exact_list <- list("", " ", "Body", "Link to Image", "End of Document", "Page of ", "Final Edition")

for (str in exact_list) {
  west <- west[!(west$west==str),]
}

west$west <- as.character(west$west)

# Save the cleaned data for later use
clean_west <- west
```

```{r, echo=FALSE, message=FALSE}
west <- west %>%
  unnest_tokens(word, west)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

# Sentiment analysis for West Coast

west_afinn <- west %>%
  inner_join(get_sentiments("afinn"))

west_nrc <- west %>%
  inner_join(get_sentiments("nrc"))

west_bing <- west %>%
  inner_join(get_sentiments("bing"))
```

#### AFINN
```{r, echo=FALSE, message=FALSE}
# Plot the results
ggplot(data = west_afinn, 
       aes(x=value)) +
  geom_histogram() +
  ggtitle("West Coast Sentiment Range") +
  theme_minimal()
```

#### NRC
```{r, echo=FALSE, message=FALSE}
# Plot the results

ggplot(west_nrc, 
    aes(x = sentiment)) + 
    ggtitle("West Coast Sentiment Range") +
    geom_bar() +
    theme_minimal()

```

#### Bing
```{r, echo=FALSE, message=FALSE}
# Plot the results

ggplot(west_bing, 
    aes(x = sentiment, fill=sentiment)) + 
    ggtitle("West Coast Sentiment Range") +
    geom_bar() +
    theme_minimal() +
    labs(x = 'Sentiment', y = 'Count') +
    scale_color_manual(values = c('red', 'green'))
```

### Midwest (IL) {.tabset}

```{r, echo=FALSE, message=FALSE, results=FALSE}

# Read in Illinois txt file
il <- read_lines('illinois_articles.txt')

# Convert to tibble for cleaning/analysis
il <- tibble(il)

# Data Cleaning

grep_list <- list("Length", "Highlight", "Copyright", "Load-Date", "Section", "Byline", "PM EDT", "PM EST", "AM EST", "Graphic", "www")

for (str in grep_list) {
  il <- il[!grepl(str, il$il),]
}

exact_list <- list("", " ", "Body", "Link to Image", "End of Document", "Page of ")

for (str in exact_list) {
  il <- il[!(il$il==str),]
}

#Converting to character
il$il <- as.character(il$il)
clean_il <- il
```

```{r, echo=FALSE, message=FALSE}
# Data preparation chunk

# Preparing data for analysis
il_counts <- il %>% unnest_tokens(word, il) %>%
  anti_join(stop_words) %>%
  count(word, sort=TRUE)

# il_counts
il_counts$word <- as.factor(il_counts$word)

```

#### AFINN
```{r, echo=FALSE, message=FALSE}

# Sentiment Analysis Chunk

# afinn 
il_sentiment_afinn <- il_counts %>%
  inner_join(get_sentiments("afinn"))

# sentiment range plot
ggplot(data = il_sentiment_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("Midwest Sentiment Range")+
  theme_minimal() + 
  labs(x = 'Sentiment Value', y = 'Count', title='Midwest Sentiment Range (afinn)') +
  stat_bin(bins=19)
```

#### NRC
```{r, echo=FALSE, message=FALSE}
# nrc 
il_sentiment_nrc <- il_counts %>% 
  inner_join(get_sentiments("nrc"))

# sentiment range plot
ggplot(data = il_sentiment_nrc, 
       aes(x=sentiment)
        )+
  geom_bar()+
  ggtitle("Midwest Sentiment Range")+
  theme_minimal() + 
  labs(x = 'Sentiment', y = 'Count', title='Midwest Sentiment Range')
```

#### Bing
```{r, echo=FALSE, message=FALSE}
# bing
il_sentiment_bing <- il_counts %>% 
  inner_join(get_sentiments("bing"))

# sentiment range plot
ggplot(data = il_sentiment_bing, 
       aes(x=sentiment, fill=sentiment)
        )+
  geom_bar()+
  ggtitle("Midwest Sentiment Range")+
  theme_minimal() + 
  labs(x = 'Sentiment', y = 'Count', title='Midwest Sentiment Range') +
  scale_color_manual(values = c('red', 'green'))

```

### South (FL) {.tabset}
```{r, echo=FALSE, message=FALSE, result=FALSE}

# Read in Florida data
florida <- read_lines("FloridaDataScience.txt")

florida <- tibble(florida)

# Clean up the Florida data by excluding rows without relevant info
florida <- tail(florida, -1177)

grep_list <- list("Length", "Content Type", "Narrowed by", "News", "Highlight", "Copyright", "Load-Date", "Section", "Byline", "PM EDT", "PM EST", "AM EST", "Graphic", "www", "Location by Publication", "Client/Matter", "Search", "See image link", "All Rights Reserved", "MORE DETAILS", "Sources", "Photo")

for (str in grep_list) {
  florida <- florida[!grepl(str, florida$florida),]
}

exact_list <- list("", " ", "Body", "Link to Image", "End of Document", "Page of ")

for (str in exact_list) {
  florida <- florida[!(florida$florida==str),]
}

florida$florida <- as.character(florida$florida)

clean_fl <- florida
```


``` {r, echo=FALSE, message=FALSE, result=FALSE}
florida <- florida %>%
  unnest_tokens(word, florida)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

# Sentiment Analysis

florida_afinn <- florida %>%
  inner_join(get_sentiments("afinn"))

florida_nrc <- florida %>%
  inner_join(get_sentiments("nrc"))

florida_bing <- florida %>%
  inner_join(get_sentiments("bing"))

```

#### AFINN
```{r, echo=FALSE, message=FALSE}
ggplot(data = florida_afinn, 
       aes(x=value)) +
  geom_histogram() +
  ggtitle("South Sentiment Range") +
  theme_minimal()

```

#### NRC
```{r, echo=FALSE, message=FALSE}

ggplot(florida_nrc, 
    aes(x = sentiment)) + 
    ggtitle("South Sentiment Range") +
    geom_bar() +
    theme_minimal()


```

#### Bing
```{r, echo=FALSE, message=FALSE}
ggplot(florida_bing, 
    aes(x = sentiment, fill=sentiment)) + 
    ggtitle("South Sentiment Range") +
    geom_bar() +
    theme_minimal() +
    labs(x = 'Sentiment', y = 'Count') +
    scale_color_manual(values = c('red', 'green'))
```


### Mid-Atlantic (D.C.) {.tabset}
```{r, echo=FALSE, message=FALSE}
dc <- read_lines("DCDataScience.txt")

dc <- tibble(dc)

dc$dc <- as.character(dc$dc)

dc <- tail(dc, -2257)

grep_list <- list("Length", "Content Type", "Narrowed by", "News", "Highlight", "Copyright", "Load-Date", "Section", "Byline", "PM EDT", "PM EST", "AM EST", "Graphic", "www", "Location by Publication", "Client/Matter", "Search", "See image link", "All Rights Reserved", "MORE DETAILS", "Sources", "Photo")

for (str in grep_list) {
  dc <- dc[!grepl(str, dc$dc),]
}

exact_list <- list("", " ", "Body", "Link to Image", "End of Document", "Page of ")

for (str in exact_list) {
  dc <- dc[!(dc$dc==str),]
}

clean_dc <- dc

```


```{r, echo=FALSE, message=FALSE}
# Sentiment Anaylsis Section for DC
dc <- dc %>%
  unnest_tokens(word, dc)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

dc_afinn <- dc %>%
  inner_join(get_sentiments("afinn"))

dc_nrc <- dc %>%
  inner_join(get_sentiments("nrc"))

dc_bing <- dc %>%
  inner_join(get_sentiments("bing"))

```

#### AFINN
```{r, echo=FALSE, message=FALSE}
ggplot(data = dc_afinn, 
       aes(x=value)) +
  geom_histogram() +
  ggtitle("Mid-Atlantic Sentiment Range") +
  theme_minimal()
```

#### NRC
```{r, echo=FALSE, message=FALSE}
ggplot(dc_nrc, 
    aes(x = sentiment)) + 
    ggtitle("Mid-Atlantic Sentiment Range") +
    geom_bar() +
    theme_minimal()
```

#### Bing
```{r, echo=FALSE, message=FALSE}
ggplot(dc_bing, 
    aes(x = sentiment, fill=sentiment)) + 
    ggtitle("Mid-Atlantic Sentiment Range") +
    geom_bar() +
    theme_minimal() +
    labs(x = 'Sentiment', y = 'Count') +
    scale_color_manual(values = c('red', 'green'))

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#tf_idf for all of the articles

data_prep <- function(x,y,z){
  i <- as_tibble(t(x))
  ii <- unite(i,"text",y:z,remove = TRUE,sep = "")
}

ny_bag <- data_prep(clean_ny,'V1','V2937')

il_bag <- data_prep(clean_il,'V1','V3913')

fl_bag <- data_prep(clean_fl,'V1','V3186')

west_bag <- data_prep(clean_west,'V1','V3108')

dc_bag <- data_prep(clean_dc,'V1','V819')

region <- c("North East","Midwest","South", "West", "Mid-Atlantic")


tf_idf_text <- tibble(region,text=t(tibble(ny_bag,il_bag,fl_bag,west_bag,dc_bag,.name_repair = "universal")))


word_count <- tf_idf_text %>%
  unnest_tokens(word, text) %>%
  count(region, word, sort = TRUE)

total_words <- word_count %>% 
  group_by(region) %>% 
  summarize(total = sum(n))

region_words <- left_join(word_count, total_words)

region_words <- region_words %>%
  bind_tf_idf(word, region, n)

```

## Comparison Plots by Word Frequency
To compare the most frequent words for the articles in each region, we generated bar graphs for each region. On the x-axis, we plotted the tf-idf value for the top words for each region. To choose the top words, we grouped by region and used slice_max() to select the top 15 tf-idf values. As evident in the plot, the most common words were vastly different for each region. These differences in words may indicate the prevalence of data science applications to different industries and issues around the nation. For example, the words "rats", "rat", "trash", and "burrows" are all identified in data science articles in the Mid-Atlantic. This may indicate that data science is being applied to solve rat infestations or trash problems in this area. Overall, this graphic is useful in identifying data science applications and references across the United States.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

region_words <- region_words[order(-region_words$tf_idf),]

# Graph for top words by region
region_words %>% 
  group_by(region) %>% 
  slice_max(tf_idf, n = 15) %>% 
  ungroup() %>% 
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill=region)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~region, ncol = 3, scales = "free") +
  labs(x = "tf-idf", y = NULL, title='Most Common Words by Region') +
  theme_minimal()
```

We also wanted to compare the differences in sentiment between the five regions. To do so, we generated histograms to show the count of positive and negative words for each region (using Bing sentiment analysis). It's clear that data science articles in the South and West regions have a stronger negative connotation than elsewhere in the US. Additionally, the Midwest is the only region where the count of positive words outnubmbers the count of negative words. Comparing word sentiment can help when identifying the overall view of data science in regions across the country. Based on this graphic, the data science articles being published tend to contain more negative words, however this may be a result of the application of data science to complex and challenging environmental, political and economic issues across the country.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# Graph for sentiment by region

dc_imp_bing <- filter(region_words, region == "Mid-Atlantic", tf_idf > 5.0e-07)

dc_imp_bing <- dc_imp_bing %>%
  inner_join(get_sentiments("bing"))

fl_imp_bing <- filter(region_words, region == "South", tf_idf > 5.0e-07)

fl_imp_bing <- fl_imp_bing %>%
  inner_join(get_sentiments("bing"))

il_imp_bing <- filter(region_words, region == "Midwest", tf_idf > 5.e-07)

il_imp_bing <- il_imp_bing %>%
  inner_join(get_sentiments("bing"))

ny_imp_bing <- filter(region_words, region == "North East", tf_idf > 5.e-07)

ny_imp_bing <- ny_imp_bing %>%
  inner_join(get_sentiments("bing"))

west_imp_bing <- filter(region_words, region == "West", tf_idf > 5.e-07)

west_imp_bing <- west_imp_bing %>%
  inner_join(get_sentiments("bing"))

bing_regions = rbind(dc_imp_bing, fl_imp_bing, il_imp_bing, ny_imp_bing, west_imp_bing)
bing_regions$region = c(replicate(nrow(dc_imp_bing), "Mid-Atlantic"), replicate(nrow(fl_imp_bing), "South"), replicate(nrow(il_imp_bing), "Midwest"),
replicate(nrow(ny_imp_bing), "North East"),
replicate(nrow(west_imp_bing), "West"))

ggplot(bing_regions, aes(x=sentiment, fill=sentiment)) +
    geom_bar() +
    facet_wrap(~region) +
    labs(x='Sentiment (Bing)', y='Count', title='Sentiment Range (Bing) by Region') +
   theme_minimal() +
   theme(axis.text.x=element_blank())

```

## Conclusion
Our resulting sentiment analysis gave us a lot of insight into how the field of data science is being reported about in different regions of the United States. We were able to draw interesting conclusions from our graphs, for example discovering that the only region with a net positive sentiment on the topic of data science was the Midwest. We also noticed some strange things from our graphs, for example how the comparison plots by word frequency showed us such seemingly-unrelated groups of most common words per region. The majority of the most common words by region were not even directly related to data science, making us question the usefulness of comparison plots by word frequency. 

In terms of next steps, there are many things that could be improved upon from our analysis in order to increase its accuracy. For starters, we only gathered 100 news articles from each region; with more data spanning a longer period of time, we would be able to see how the conversation around data science has evolved over time, as well as have more certainty that we are seeing the whole picture. Along these lines, the analysis would have been more representative if we were careful to select from the major news sources in each place. We may have accidentally captured biases from one or two particular news sources if those were far more represented than others. Additionally, our regions were not very representative; the Northeast only included New York, the West Coast was just California and Oregon, the Midwest was only Illinois, the South only consisted of Florida, and the Mid-Atlantic was just DC. We would have liked to sample from every state in each region, in order to ensure that we were comparing regional sentiment properly. Besides diversifying our data set, we would have liked to create more graphs from different sentiment analyses. We did use the afinn, bing, and nrc lexicons, but there is also a “loughran” lexicon with the get_sentiments function that we could have utilized. If we could have somehow combined the results from all four lexicons into one graph, we would have been able to find far more information in just one place. Regardless of the time limitations that prevented us from going further with this project, we feel that we have successfully completed a sentiment analysis and gained some valuable insight on publications on data science per region in America.
